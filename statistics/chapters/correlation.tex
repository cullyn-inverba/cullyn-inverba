\chapter{Correlation}

\section{Correlation Fundamentals}
\begin{itemize}
  \item \ddd{Correlation (dependence)}: a statistical relationship between two random variables or bivariate data.
    \begin{itemize}
      \item Correlations can indicate a predictive relationship that can be exploited. 
      \item Presence of correlation is not sufficient to infer a causal relationship, i.e., \emph{correlation does not imply causation}.
    \end{itemize}
  \item \ddd{Covariance \(\cov(x,y)\)}: a measure of the joint variability of two random variables, i.e.,
  \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \cov(\xxx{x},\yyy{y}) =  E[\xxx{(x-\bar{X})}\yyy{(y-\bar{Y})}]
  \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item Equivocally, using discrete random variables:
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \cov(\xxx{x},\yyy{y})= \frac{1}{n-1}\sum_{i = 1}^{n}(\xxx{x_i-\bar{X}})(\yyy{y_i-\bar{Y}})
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \item If linearity is assumed, then can be simplified to \hyperref[Section: Sampling]{\ulink{expected values}} of their product minus the product of their expected values, i.e.,
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \cov(\xxx{x},\yyy{y}) = E[\xxx{x}\yyy{y}] - \xxx{\bar{X}}\yyy{\bar{Y}}
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \end{itemize}
  \item \ddd{Correlation coefficient \([\bbb{-1},\rrr{1}]\)}: a numerical measure of some type of correlation. 
    \begin{itemize}
      \item Correlation is the \hyperref[Section: Introduction to Normalization]{\ulink{normalized (dimensionless)}} representation of covariance.
      \item Often, correlation refers to linear relationships via Pearson correlation, however, there are several measures of correlation based on data types. 
        \begin{itemize}
          \item Not all correlations will be covered, but the most common will be, i.e., the \hyperref[Subsection: Pearson Correlation]{\dlink{Pearson}}, \hyperref[Subsection: Spearman Correlation]{\dlink{Spearman}}, and \hyperref[Subsection: Kendall Correlation]{\dlink{Kendall}} correlations. 
        \end{itemize}
    \end{itemize}
  \item \ddd{Covariance matrix \(K_{\bm{xx}}\)}: a square matrix giving the covariance between each pair of elements of a given random vector, i.e.,
  \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  K_{\bm{xx}} = \cov[x,x] = E[(x-\bar{X})(x-\bar{X})^T] = E[xx^T] - \bar{X}\bar{X}^T
  \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
    \begin{itemize}
      \item \minor{Disclaimer: linear algebra references will not be explained, as long as it was previously covered in \href{https://github.com/cullyn-inverba/notes}{\underline{my notes}} on linear algebra.}
      \item Any covariance is symmetric, positive semi-definite, and its main diagonal contains variances (i.e., the covariance of each element with itself).
    \end{itemize}
  
  \subsection{Pearson Correlation}
  \begin{itemize}
    \item \ddd{Pearson correlation coefficient \(\rho\)}: a measure of \emph{linear correlation} between two sets of data; simply the covariance divided by their standard deviations, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \rho_{\xxx{x},\yyy{y}} = \frac{\cov(\xxx{x},\yyy{y})}{\sigma_\xxx{x}\sigma\yyy{y}}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item \(\sigma_\xxx{x},~\sigma\yyy{y}\): the standard deviation of \(\xxx{x}~\text{and}~\yyy{y}\)
    \end{itemize}
    \item \ddd{Sample Pearson correlation coefficient \(r\)}: application of Pearson correlation to discrete random sample, assuming paired data:
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    r_{\xxx{x},\yyy{y}} = \frac{\sum_{i = 1}^{n}(\xxx{x_i-\bar{x}})(\yyy{y_i-\bar{y}})}
    {\sqrt{\sum_{i = 1}^{n}(\xxx{x_i-\bar{x}})^2}\sqrt{\sum_{i = 1}^{n}(\yyy{y_i-\bar{y}})}}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item The Pearson correlation generally does not completely characterize relationships, as it only emphasizes the strength of linear relationships.
      \begin{itemize}
        \item In particular, if the \ddd{conditional mean \(E(Y~|~X)\)} of \(Y\) given \(X\) is not linear in \(X\), then the Pearson correlation will fail to characterize the relationship.
        \item E.g., Anscombe's quartet demonstrates this problem, show how outliers or non-linear relationships can heavily impact the \emph{linear correlation}: 
        \begin{center}
          \hspace{-32pt}\Image{0.6\columnwidth}{chapters/images/quartet.png}
        \end{center}
        \begin{itemize}
          \item All have the same mean, variance, correlation, and regression line, but obviously have different distributions.
        \end{itemize}
        \item The conclusion is that Pearson correlation can only fully characterize relationships between variables drawn from \emph{multivariate normal distributions}.
          \begin{itemize}
            \item Note: in practice, many distributions can be accurately calculated from normal distributions (not necessarily multivariate) if it has a finite covariance matrix. 
          \end{itemize}
      \end{itemize}
  \end{itemize}

  \item \ddd{Cosine Correlation \(\cos(\theta)\)}: a measure of similar between two non-zero vectors on an inner product space; represented using a dot product over magnitude, i.e.,
  \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \cos(\theta) = \frac{\bm{a}^T \bm{b}}{||\bm{a}||\,||\bm{b}||}= \frac{\sum_{i = 1}^{n}a_i b_i}{\sqrt{\sum_{i = 1}^{n}a^2_i}\sqrt{\sum_{i = 1}^{n}b^2_i}}
  \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item If the attribute vectors are normalized (e.g., \(\bm{a}-\bar{\bm{a}}\)), then the measure is equivalent to the Pearson correlation coefficient.
    \end{itemize}
  
\end{itemize}


\section{Rank Correlation}
\begin{itemize}
  \item \ddd{Rank correlation}: a series of statistical measures of \hyperref[Subsection: Data Types]{\ulink{ordinal}} association, i.e., the relationship between ranking of different ordinal values.
    \begin{itemize}
      \item \ddd{Rank correlation coefficient}: the degree of similarity between two rankings, whereby significance can be assessed.
    \end{itemize}
  
  \subsection{Spearman Correlation}
  \begin{itemize}
    \item \ddd{Spearman's \(\rho, r_s\)}: a \hyperref[Subsection: Parametric vs. Nonparametric]{\ulink{nonparametric}} measure of rank correlation between two variables via assessment of a \emph{monotonic relationship}.
      \begin{itemize}
        \item \ddd{Monotonic function}: a function between ordered sets that preserves or reverses the given order, 
        \item I.e., the Pearson correlation tests for linear relationships, while Spearman's correlation tests for monotonic relationships (linear or not). 
        \item Can be applied for both continuous and discrete ordinal variables. 
      \end{itemize}
    \item Spearman's \(\rho \) is simply defined as the Pearson correlation coefficient between rank variables \(rv_x, rv_y\), i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    r_s = \rho_{\xxx{rv_x},\yyy{rv_y}} = \frac{\cov(\xxx{rv_x},\yyy{rv_y})}{\sigma_{\xxx{rv_x},\yyy{rv_y}}}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \end{itemize}

  \subsection{Determining Significance}
  \begin{itemize}
    \item Determining significance of correlations can be done in a number of ways, using permutation testing, the Fisher Z-transformation, or the \(t\)-distribution under the \(H_\nil\).
    \begin{itemize}
      \item \ddd{Fisher \(z\)-transformation \(F(r)\)}: a method of transforming uniformly distributed data (\(-1 < r < 1\)) into an approximately normal distribution, i.e.,
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      F(r) = \frac{1}{2}\ln\left(\frac{1+r}{1-r}\right) = \arctan(r)
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \begin{itemize}
        \item This creates a new distribution where values near \bbb{-1} and \rrr{+1} are stretched out, creating the tails of the normal distribution, as \bbb{-1} and \rrr{+1} represent perfect monotonic relationships (typically outliers). 
        \item Significance (\hyperref[Subsection: Z-Score Standardization]{\ulink{\(z\)-score}}) can then be tested when accounting for sample size \(n\):
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        z = \sqrt{\frac{n-3}{1.06}}F(r)
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \end{itemize}
    \end{itemize}
    \item One can use a \(t\)-distribution with \(n-2\) degrees of freedom under the \(H_\nil\), based on strength of correlation \(r\) and data points \(n\), i.e.,
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      t_{n-2} = \frac{r\sqrt{n-2}}{1-r^2}        
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \end{itemize}
  
  \subsection{Kendall Correlation}
  \begin{itemize}
    \item \ddd{Kendall's \(\tau\) coefficient}: a measure of ordinal data, wherein the ranks do not have meaningful relationships between levels.
    \item Kendall \(\tau \) will be high when observations have relatively similar rank between two variables, and low when relatively not similar, compared to the rest of the data.
    \item \ddd{Concordant}: a pair of observations (\(x_1,y_1\)), (\(x_2,y_2\)) wherein the \emph{signs} of both elements are \rrr{greater than}, \bright{equal to}, or \bbb{less than} the \emph{corresponding elements} of the \emph{other pair}, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \sgn(x_2-x_1) = \sgn(y_2-y_1)
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item \ddd{Discordant}: not concordant or inverse signs, i.e., one pair contains a higher value of \(x\) then the other pair contains a higher value of \(y\): 
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \sgn(x_2-x_1) = -\sgn(y_2-y_1)
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item \ddd{Tau-a}: the strength of associations of the cross tabulations, with no adjustments for ties, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \tau_a = \frac{n_c - n_d}{n_b}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item \(n_c\to\) number of concordant pairs; \(n_d\to\) number of discordant pairs.
      \item \(n_b = \binom{n}{2} = \frac{n(n-1)}{2}\to\) the binomial coefficient for numbers of ways to choose two items from \(n\) items.
    \end{itemize}
    \item \ddd{Tau-b}: an adjustment to \(\tau \) that takes in account ties, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \tau_b = \frac{n_c - n_d}{\sqrt{(n_b-n_1)(n_b-n_2)}}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item \(n_1 = \sum_{i} t_i(t_i -1)/2\), 
        \begin{itemize}
          \item \(t_i\to\) number of tied values in the \(i^{th}\) group of ties for the first quantity.
        \end{itemize}
      \item \(n_2 = \sum_{j} u_j(u_j -1)/2\)
      \begin{itemize}
        \item \(u_j\to\) number of tied values in the \(j^{th}\) group of ties for the second quantity.
      \end{itemize}
    \end{itemize}
    \item \minor{Disclaimer: Tau-c and significance tests for Kendall's \(\tau \) not covered, as of now.}
  \end{itemize}
  
\end{itemize}

\section{Partial Correlation}
\begin{itemize}
  \item[]

  \subsection{Confounding}
  \begin{itemize}
    \item 
  \end{itemize}

  \subsection{Subgroup Paradox}
  \begin{itemize}
    \item 
  \end{itemize}
  
\end{itemize}
