\chapter{Hypothesis Testing}

\section{Hypothesis Testing Fundamentals}
\begin{itemize}
  \item Reviewing dependent and independent variables (parameters):
    \begin{itemize}
      \item \ddd{Dependent variable \(y\)}: the variable you are trying to explain; the \yyy{output} of a function.
      \item \ddd{Independent variables \(x_n\)}: the variables that potentially explain the dependent variable; the \xxx{input(s)} to a function.
      \item Often the assumptions about the relationship can effect what is assumed to be the independent and dependent variables; interpretations can be difficult.
    \end{itemize}
  \item \ddd{Models}: a simplified system made of the composition of concepts which are used to help know, understand, or simulate a subject the model represents.
    \begin{itemize}
      \item \ddd{Residual (error) \(\epsilon\)}: the degree that features not explained by variables that make up the composition of models. 
      \item Residuals should be small (\emph{accurate}), but models should also be simple (\emph{useful}); finding the balance between these two goals is a major part of statistics/science.
    \end{itemize}
  \item \ddd{(Alternative; effect) hypothesis \(H_a\)}: a proposed explanation for a phenomenon;a falsifiable claim that requires verification, typically from experimental data, and that allows for predictions about future observations.
    \begin{itemize}
      \item Most formal hypotheses connect concepts by specifying the expected relationships between propositions, leading to expected differences.
      \item Hypothesis testing is used to develop better theories via the rejection of previous theories; most progress in science is the result of hypothesis testing.
      \item A \emph{strong hypothesis} is:
      \begin{multicols}{2}
        \begin{itemize}
          \item \emph{Falsifiable}---ideally testable, makes a criticizable prediction. 
          \item \emph{Parsimonious}---limits excessive entities; application of ``Occam's razor.''
          \item \emph{Scoped}---clear, specific, applicable; a statement, not a question.
          \item \emph{Fruitful}---may explain further phenomena, aids in understanding.
        \end{itemize}
      \end{multicols}
    \end{itemize}

  \item \ddd{Null hypothesis \(H_\nil\)}: the default hypothesis that a quantity to be measure is zero. 
    \begin{itemize}
      \item Typically, a quantity being measure is the difference between two situations, thus support for the alternative hypothesis is gained via \emph{rejection of the null hypothesis}.
      \item Testing the null hypothesis is a central task in hypothesis testing and the modern practice of science; weak evidence fails to reject the null hypothesis.
      \item Criteria for excluding the null hypothesis will be covered in more depth when discussing \hyperref[Chapter: Confidence Intervals]{\dlink{confidence intervals}}.
    \end{itemize}
  
  \subsection{Basis of Inferential Statistics}
  \begin{itemize}
    \item Essentially, the basis of inferential statistics relies on the \emph{comparison} between \hyperref[Section: Sampling]{\ulink{sample distributions}} under the null and alternative hypotheses.
    \item In most cases, \hyperref[Subsection: Population vs. Sample Data]{\ulink{population data}} is not attainable, instead, use of the \hyperref[Subsection: Law of Large Numbers and Central Limit Theorem]{\ulink{central limit theorem}} allows for the \hyperref[Section: Sampling]{\ulink{expected value}} to be found via use of repeated sampling.
      \begin{itemize}
        \item \ddd{\(H_\nil\) distribution}: the distribution created due to \hyperref[Section: Sampling]{\ulink{sampling variability}} under the null hypothesis, i.e., the differences between the expected mean value and sampled mean value, centered around \nil.
          \begin{itemize}
            \item Results from a formula based on assumptions, \hyperref[Subsection: Degrees of Freedom]{\dlink{degrees of freedom}}, and type/nature of particular tests being performed.
          \end{itemize}
        \item \ddd{\(H_a\) distribution}: the distribution of differences due to the alternative hypothesis, rejection of the null hypothesis is likely to occur if observations reflect this distribution and not the \(H_\nil\) distribution.
          \begin{itemize}
            \item Results from empirical observations, gathered data and \hyperref[Subsection: Sampling Methods]{\ulink{sampling methods}}.
          \end{itemize}
      \end{itemize}
    \item Quantifying the differences between the \(H_\nil\) and \(H_a\) requires normalization, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \frac{\text{\ttt{Difference of centers}}}
    {\text{\fff{Widths of distributions}}} = 
    \frac{\text{\ttt{Central Tendency}}}
    {\text{\fff{Dispersion}}} = 
    \frac{\text{\ttt{Signal}}}
    {\text{\fff{Noise}}}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item The investigation of the ratio between \ttt{signal}-to-\fff{noise} is essentially all of inferential statistics; fitting data into workable frameworks contains the majority of the work.
  \end{itemize}
    
  \subsection{P-Value}
  \begin{itemize}
    \item \ddd{\textit{p}-value}: the \hyperref[Section: Probability Fundamentals]{\ulink{probability}} of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypotheses is correct, i.e.,
      \begin{itemize}
        \item How likely is the \(H_a\) value to occur if \(H_\nil\) is correct?
        \item What is the probability of observing a parameter estimate of \(H_a\) or larger, given there is no true effect?
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \emph{P(H_a~|~H_\nil)}
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \item \ttt{Small \(p\)-value} \to~outcome is \fff{very unlikely} to occur under the \fff{null hypothesis}.
      \end{itemize}
    \item \ddd{Significance level \(\alpha\)}: the somewhat arbitrary threshold whereby a study would reject the null hypothesis, typically \(\alpha \leq 0.05, 0.01,~\text{or}~0.001\)
    \item \ddd{Statistically significant}: when \(p \leq \alpha\); significance can have \hyperref[Subsection: Interpretations of Significance]{\dlink{other interpretations}}.
    \item Either side of a distribution is unlikely; \emph{two-tailed} distributions need to \emph{split \(\alpha\)}.
      \begin{itemize}
        \item Hypotheses should aim to be one-tailed, but this is often not feasible.
      \end{itemize}
    \item \(p\)-values are often misinterpreted, sometimes even intentionally abused, and an important topic in metascience.
    \item Common misinterpretations of \(p\)-values:
      \begin{itemize}
        \item \false{Incorrect}: 
          \begin{itemize}
            \item ``\textit{My \(p\)-value is 0.02, so the effect is present for 2\% of the population.}''
            \item ``\textit{My \(p\)-value is 0.02, so there is a 90\% chance that my sample statistic equals the population parameter.}''
            \item ``\textit{My p-value is smaller than the threshold, therefore the effect is real.}''
          \end{itemize}
        \item \true{Correct}: 
        \begin{itemize}
          \item ``My \(p\)-value is 0.02, therefore there is a 2\% chance that there is no effect and my sample statistic was due to sampling variability, noise, small sample size, and/or systematic bias.''
        \end{itemize}
      \end{itemize}
    \item Recall that the \hyperref[Subsection: Z-Score Standardization]{\ulink{z-score}} is a dimensionless measure of \hyperref[Subsection: Measures of Dispersion]{\ulink{standard deviations \(\sigma_x\)}} from the mean; the relation between \(p\)- and z-values can be useful to memorize.
    \item Given a Gaussian distribution, z-proportion (above/below) values are:
      \begin{itemize}
        \item 68.3\% of the data are within \(\sigma_1\leftrightarrow z = \pm 1=0.683\) 
        \item 95.5\% of the data are within \(\sigma_2\leftrightarrow z = \pm 2 = 0.955\)
        \item 99.7\% of the data are within \(\sigma_3\leftrightarrow z = \pm 3 = 0.997\)
      \end{itemize}
    \item Common \(p\)-values parings with standard deviations:
    \vspace{-6pt}
    \begin{multicols}{2}
      \begin{itemize}
        \item One-tailed \(\downarrow\)
        \item \(p=0.05 \leftrightarrow z = 1.64\)
        \item \(p=0.01 \leftrightarrow z = 2.32\)
        \item \(p=0.001 \leftrightarrow z = 3.09\)
        \item \(\downarrow\) Two-tailed \(\downarrow\)
        \item \(p=0.05 \leftrightarrow z = 1.96\)
        \item \(p=0.01 \leftrightarrow z = 2.58\)
        \item \(p=0.001 \leftrightarrow z = 3.29\)
      \end{itemize}
    \end{multicols}
  \end{itemize}
  
  \subsection{Degrees of Freedom}
  \begin{itemize}
    \item \ddd{Degrees of freedom (d.f. \(\nu\))}: the number of values in the final calculation of a statistic that are free to vary.
      \begin{itemize}
        \item I.e., the minimum number of independent coordinates that can specify the position of the system completely.
      \end{itemize}
    \item Degrees of freedom determine the shape of \(H_\nil\) distributions (often the width).
    \item Higher degrees of freedom generally indicate more \hyperref[Chapter: Statistical Power and Sample Sizes]{\dlink{power to reject}} the \(H_\nil\).
    \item Can be useful metric for quickly determining relevant accuracy and understanding of experimental designs.
    \item Generally, \emph{\(\nu = n - k\)}; with \(n\) data points and \(k\) parameters.
  \end{itemize}

  \subsection{Statistical Errors}
  \begin{itemize}
    \item Yes
    
    \Image{0.8\columnwidth}{chapters/images/errors.png}
      
  \end{itemize}

\end{itemize}

\section{Testing Properties}
\begin{itemize}
  \item []

  \subsection{Parametric vs. Nonparametric}
  \begin{itemize}
    \item 
  \end{itemize}

  \subsection{Multiple Comparisons Problem}
  \begin{itemize}
    \item 
  \end{itemize}
  
  \subsection{Interpretations of Significance}
  \begin{itemize}
    \item 
  \end{itemize}
  
\end{itemize}

\section{Cross Validation}
\begin{itemize}
  \item []

  \subsection{P-Value vs. Model Accuracy}
  \begin{itemize}
    \item 
  \end{itemize}
  
\end{itemize}

\section{T-Tests}
\begin{itemize}
  \item []
  
  \subsection{Assumptions and Uses of T-Tests}
  \begin{itemize}
    \item 
  \end{itemize}

  \subsection{One-Sample T-Test}
  \begin{itemize}
    \item 
  \end{itemize}
  
  \subsection{Two-Sample T-Test}
  \begin{itemize}
    \item 
  \end{itemize}

  \subsection{Nonparametric T-Tests}
  \begin{itemize}
    \item 
  \end{itemize}
  
  \subsection{Permutation Testing for T-Tests}
  \begin{itemize}
    \item 
  \end{itemize}
  
  
\end{itemize}
