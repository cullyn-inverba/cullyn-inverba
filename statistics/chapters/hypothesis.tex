\chapter{Hypothesis Testing}

\section{Hypothesis Testing Fundamentals}
\begin{itemize}
  \item Reviewing dependent and independent variables (parameters):
    \begin{itemize}
      \item \ddd{Dependent variable \(y\)}: the variable you are trying to explain; the \yyy{output} of a function.
      \item \ddd{Independent variables \(x_n\)}: the variables that potentially explain the dependent variable; the \xxx{input(s)} to a function.
      \item Often the assumptions about the relationship can effect what is assumed to be the independent and dependent variables; interpretations can be difficult.
    \end{itemize}
  \item \ddd{Models}: a simplified system made of the composition of concepts which are used to help know, understand, or simulate a subject the model represents.
    \begin{itemize}
      \item \ddd{Residual (error) \(\epsilon\)}: the degree that features not explained by variables that make up the composition of models. 
      \item Residuals should be small (\emph{accurate}), but models should also be simple (\emph{useful}); finding the balance between these two goals is a major part of statistics/science.
    \end{itemize}
  \item \ddd{(Alternative; effect) hypothesis \(H_a\)}: a proposed explanation for a phenomenon;a falsifiable claim that requires verification, typically from experimental data, and that allows for predictions about future observations.
    \begin{itemize}
      \item Most formal hypotheses connect concepts by specifying the expected relationships between propositions, leading to expected differences.
      \item Hypothesis testing is used to develop better theories via the rejection of previous theories; most progress in science is the result of hypothesis testing.
      \item A \emph{strong hypothesis} is:
      \begin{multicols}{2}
        \begin{itemize}
          \item \emph{Falsifiable}---ideally testable, makes a criticizable prediction. 
          \item \emph{Parsimonious}---limits excessive entities; application of ``Occam's razor.''
          \item \emph{Scoped}---clear, specific, applicable; a statement, not a question.
          \item \emph{Fruitful}---may explain further phenomena, aids in understanding.
        \end{itemize}
      \end{multicols}
    \end{itemize}

  \item \ddd{Null hypothesis \(H_\nil\)}: the default hypothesis that a quantity to be measure is zero. 
    \begin{itemize}
      \item Typically, a quantity being measure is the difference between two situations, thus support for the alternative hypothesis is gained via \emph{rejection of the null hypothesis}.
      \item Testing the null hypothesis is a central task in hypothesis testing and the modern practice of science; weak evidence fails to reject the null hypothesis.
      \item Criteria for excluding the null hypothesis will be covered in more depth when discussing \hyperref[Section: Confidence Intervals]{\dlink{confidence intervals}}.
    \end{itemize}
  
  \subsection{Basis of Inferential Statistics}
  \begin{itemize}
    \item Essentially, the basis of inferential statistics relies on the \emph{comparison} between \hyperref[Section: Sampling]{\ulink{sample distributions}} under the null and alternative hypotheses.
    \item In most cases, \hyperref[Subsection: Population vs. Sample Data]{\ulink{population data}} is not attainable, instead, use of the \hyperref[Subsection: Law of Large Numbers and Central Limit Theorem]{\ulink{central limit theorem}} allows for the \hyperref[Section: Sampling]{\ulink{expected value}} to be found via use of repeated sampling.
      \begin{itemize}
        \item \ddd{\(H_\nil\) distribution}: the distribution created due to \hyperref[Section: Sampling]{\ulink{sampling variability}} under the null hypothesis, i.e., the differences between the expected mean value and sampled mean value, centered around \nil.
          \begin{itemize}
            \item Results from a formula based on assumptions, \hyperref[Subsection: Degrees of Freedom]{\dlink{degrees of freedom}}, and type/nature of particular tests being performed.
          \end{itemize}
        \item \ddd{\(H_a\) distribution}: the distribution of differences due to the alternative hypothesis, rejection of the null hypothesis is likely to occur if observations reflect this distribution and not the \(H_\nil\) distribution.
          \begin{itemize}
            \item Results from empirical observations, gathered data and \hyperref[Subsection: Sampling Methods]{\ulink{sampling methods}}.
          \end{itemize}
      \end{itemize}
    \item Quantifying the differences between the \(H_\nil\) and \(H_a\) requires normalization, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \frac{\text{\ttt{Difference of centers}}}
    {\text{\fff{Widths of distributions}}} = 
    \frac{\text{\ttt{Central Tendency}}}
    {\text{\fff{Dispersion}}} = 
    \frac{\text{\ttt{Signal}}}
    {\text{\fff{Noise}}}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item The investigation of the ratio between \ttt{signal}-to-\fff{noise} is essentially all of inferential statistics; fitting data into workable frameworks contains the majority of the work.
  \end{itemize}
    
  \subsection{P-Value}
  \begin{itemize}
    \item \ddd{\textit{p}-value}: the \hyperref[Section: Probability Fundamentals]{\ulink{probability}} of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypotheses is correct, i.e.,
      \begin{itemize}
        \item How likely is the \(H_a\) value to occur if \(H_\nil\) is correct?
        \item What is the probability of observing a parameter estimate of \(H_a\) or larger, given there is no true effect?
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \emph{\cp{a}}
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \item \ttt{Small \(p\)-value} \to~outcome is \ttt{very unlikely} to occur under the \ttt{null hypothesis}.
      \end{itemize}
    \item \ddd{Significance level \(\alpha\)}: the somewhat arbitrary threshold whereby a study would reject the null hypothesis, typically \(\alpha \leq 0.05, 0.01,~\text{or}~0.001\)
    \item \ddd{Statistically significant}: when \(p \leq \alpha\); significance can have \hyperref[Subsection: Interpretations of Significance]{\dlink{other interpretations}}.
    \item Either side of a distribution is unlikely; \emph{two-tailed} distributions need to \emph{split \(\alpha\)}.
      \begin{itemize}
        \item Hypotheses should aim to be one-tailed, but this is often not feasible.
      \end{itemize}
    \item \(p\)-values are often misinterpreted, sometimes even intentionally abused, and an important topic in metascience.
    \item Common misinterpretations of \(p\)-values:
      \begin{itemize}
        \item \false{Incorrect}: 
          \begin{itemize}
            \item ``\textit{My \(p\)-value is 0.02, so the effect is present for 2\% of the population.}''
            \item ``\textit{My \(p\)-value is 0.02, so there is a 90\% chance that my sample statistic equals the population parameter.}''
            \item ``\textit{My p-value is smaller than the threshold, therefore the effect is real.}''
          \end{itemize}
        \item \true{Correct}: 
        \begin{itemize}
          \item ``My \(p\)-value is 0.02, therefore there is a 2\% chance that there is no effect and my sample statistic was due to sampling variability, noise, small sample size, and/or systematic bias.''
        \end{itemize}
      \end{itemize}
    \item Recall that the \hyperref[Subsection: Z-Score Standardization]{\ulink{z-score}} is a dimensionless measure of \hyperref[Subsection: Measures of Dispersion]{\ulink{standard deviations \(\sigma_x\)}} from the mean; the relation between \(p\)- and z-values can be useful to memorize.
    \item Given a Gaussian distribution, z-proportion (above/below) values are:
      \begin{itemize}
        \item 68.3\% of the data are within \(\sigma_1\leftrightarrow z = \pm 1=0.683\) 
        \item 95.5\% of the data are within \(\sigma_2\leftrightarrow z = \pm 2 = 0.955\)
        \item 99.7\% of the data are within \(\sigma_3\leftrightarrow z = \pm 3 = 0.997\)
      \end{itemize}
    \item Common \(p\)-values parings with standard deviations:
    \vspace{-6pt}
    \begin{multicols}{2}
      \begin{itemize}
        \item One-tailed \(\downarrow\)
        \item \(p=0.05 \leftrightarrow z = 1.64\)
        \item \(p=0.01 \leftrightarrow z = 2.32\)
        \item \(p=0.001 \leftrightarrow z = 3.09\)
        \item \(\downarrow\) Two-tailed \(\downarrow\)
        \item \(p=0.05 \leftrightarrow z = 1.96\)
        \item \(p=0.01 \leftrightarrow z = 2.58\)
        \item \(p=0.001 \leftrightarrow z = 3.29\)
      \end{itemize}
    \end{multicols}
  \end{itemize}
  
  \subsection{Degrees of Freedom}
  \begin{itemize}
    \item \ddd{Degrees of freedom (d.f. \(\nu\))}: the number of values in the final calculation of a statistic that are free to vary.
      \begin{itemize}
        \item I.e., the minimum number of independent coordinates that can specify the position of the system completely.
      \end{itemize}
    \item Degrees of freedom determine the shape of \(H_\nil\) distributions (often the width).
    \item Higher degrees of freedom generally indicate more \hyperref[Chapter: Statistical Power and Sample Sizes]{\dlink{power to reject}} the \(H_\nil\).
    \item Can be useful metric for quickly determining relevant accuracy and understanding of experimental designs.
    \item Generally, \emph{\(\nu = n - k\)}; with \(n\) data points and \(k\) parameters.
  \end{itemize}

  \subsection{Statistical Errors}
  \begin{itemize}
    \item \ddd{\fff{False positive (type I error) \(p = \alpha\)}}: an \textbf{\fff{incorrect rejection}} of a \ttt{true \(H_\nil\)}.
      \begin{itemize}
        \item \ddd{\rrr{True positive \(p = 1-\beta\)}}: a \rrr{\textbf{correct rejection}} of a \fff{false \(H_\nil\)}.
      \end{itemize}
    \item \ddd{\XXX{False negative (type II error) \(p= \beta\)}}: an \textbf{\XXX{incorrect non-rejection}} of a \fff{false \(H_\nil\)}.
      \begin{itemize}
        \item \ddd{\bbb{True negative \(p = 1-\alpha\)}}: a \bbb{\textbf{correct non-rejection}} of a \ttt{true \(H_\nil\)}.
      \end{itemize}
    \item \ddd{``Overlap''}: the area shared between the \(H_\nil\) and the \(H_a\).
      \begin{itemize}
        \item Adjustments to the significance level \(\alpha \) can bias towards/away from either false negatives/positives, at the cost of increasing the other.
        \item Sometimes one error is more costly than the other, however, changing \(\alpha\) is a less than ideal way generally arbitrary way to minimize error.
      \end{itemize}
      \item The best way to minimize error is to minimize \ttt{signal}-to-\fff{noise}, i.e.,
      \begin{itemize}
        \item \ttt{Increase distance between} distributions (\ttt{bigger effects})
        \item \fff{Decrease the width} of the distributions (\fff{less variability}).
      \end{itemize}
    \begin{center}
      \Image{0.8\columnwidth}{chapters/images/errors.png}
    \end{center}
  \end{itemize}
  \subsection{Interpretations of Significance}
  \begin{itemize}
    \item \ddd{Statistical significance}: the probability of observing a test statistic of a certain magnitude given the \(H_\nil\) is true.
    \item \ddd{Theoretical significance}: a finding that is relevant for a theory or leads to a new experiment; not directly related to statistical significance. 
    \item \ddd{Clinical (practical, societal, educational)}: a finding is relevant for application in a particular field of interest. 
  \end{itemize}
\end{itemize}

\section{Testing Properties}
\begin{itemize}
  \item []

  \subsection{Parametric vs. Nonparametric}
  \begin{itemize}
    \item \ddd{Parametric statistics}: based on the assumptions wherein the sample data originates from a population that can be adequately modeled by a probability distribution with \OOO{a fixed set of parameters}.
    \item \ddd{Nonparametric statistics}: based on \bBb{relaxed assumptions} surrounding of parametric tests, e.g., underlying distribution less important, presence of outliers, or lower specificity of parameters.
    \item Generally, there is a nonparametric test related to each parametric test, with particular assumptions relaxed, e.g., 
    \bigskip
    \begin{table}[h]
      \centering
      \begin{tabular}{rl}
        \ddd{Parametric} &  
        \ddd{Nonparametric}  \\
        \hyperref[Subsection: One-Sample and Two-Sample T-Tests]{\dlink{1-sample \(t\)-test}} &
         \hyperref[Subsection: Nonparametric T-Tests]{\dlink{Wilcoxon sign-rank test}} \\
        \hyperref[Subsection: One-Sample and Two-Sample T-Tests]{\dlink{2-sample \(t\)-test}} &
         \hyperref[Subsection: Nonparametric T-Tests]{\dlink{Mann-Whitney U test}} \\
        \hyperref[Chapter: Correlation]{\dlink{Pearson correlation}} &
         \hyperref[Chapter: Correlation]{\dlink{Spearman correlation}} \\
        \hyperref[Chapter: Analysis of Variance (ANOVA)]{\dlink{ANOVA}} &
         \hyperref[Chapter: Analysis of Variance (ANOVA)]{\dlink{Kruskal-Wallis test}}\\
        \end{tabular}
    \end{table}
    \item Important applications of nonparametric statistics with no direct correlate involve \hyperref[Subsection: Primer: Permutation Testing]{\dlink{permutation testing}}, \hyperref[Subsection: Primer: Cross-Validation]{\dlink{cross-validation}}, and \hyperref[Subsection: Primer: Bootstrapping]{\dlink{bootstrapping}}.
    \item \true{\!Advantages} and \false{\!limitations (sometimes)} of \ddd{parametric statistics}:
    \begin{multicols}{2}
      \begin{itemize}
        \item \true{\! Standard, widely used}
        \item \true{\! Computationally efficient/simple}
        \item \true{\! Analytically proven}
        \item \false{\! Based on assumptions}
        \item \false{\! Assumptions can be hard to test}
        \item \false{\! Violations can be inscrutable}
      \end{itemize}
    \end{multicols}
    \item \true{\!Advantages} and \false{\!limitations (sometimes)} of \ddd{nonparametric statistics}:
    \begin{multicols}{2}
      \begin{itemize}
        \item \true{\! ``No'' assumptions necessary}
        \item \true{\! Appropriate for non-numeric data}
        \item \true{\! Appropriate for small sample sizes}
        \item \false{\! Can be ``block box'' algorithms}
        \item \false{\! Can be inefficient/slow}
        \item \false{\! Results can vary each run}
      \end{itemize}
    \end{multicols}
    \item In general, use:
      \begin{itemize}
        \item \OOO{Parametric} methods when \OOO{possible}.
        \item \bBb{Nonparametric} methods when \bBb{necessary}.
      \end{itemize} 
  \end{itemize}

  \subsection{Multiple Comparisons Problem}
  \begin{itemize}
    \item \ddd{Multiplicity (multiple comparison problem)}: the increase of erroneous inferences when comparing a set of statistical inferences simultaneously, or when inferring a subset of parameters based on the observed values.
      \begin{itemize}
        \item As more attributes are compared, the more likely it becomes that observed outcome is due to sampling error, as probabilities are additive.
        \item E.g., despite all the alternative hypotheses have a statistical significant value individually (5\%), together they provide a high rate of \hyperref[Subsection: Statistical Errors]{\ulink{type I errors \(\alpha \)}}, i.e.,
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \cp{1} + \cp{2} + \cp{3} = \fff{0.15} = \fff{\alpha}
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \item The above is just a comparing against the \(H_\nil\), the problem becomes much worse when including pairwise comparisons between all \(H_a\) in the set (\fff{15\%\,\to\,30\% \(\alpha \)}).
      \end{itemize}
    \item Common conceptualizations of multiplicity problem can be done via descriptions of errors rates, e.g.,
      \begin{itemize}
        \item \ddd{Family-wise error rate (FWER) \(\tilde{\alpha}\)}: the probability of making \emph{at least one} \fff{false positive} when performing multiple hypotheses tests, i.e.,
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \tilde{\alpha} = 1 - (1-\fff{\alpha}_i)^m \leftrightarrow p(\fff{\alpha} \geq 1)
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \begin{itemize}
          \item \(i =\text{per comparison}\), \(m=\text{total hypotheses tested}\)
        \end{itemize}
        \item \ddd{False discovery rate (FDR) \(E[Q]\)}: the \emph{expected} proportion \(Q\) of \fff{false positives} relative to total number of \rrr{true positives \(1-\beta\)}, i.e.,
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        E[Q] = \frac{\fff{\alpha}}{(\fff{\alpha}+\rrr{(1-\XXX{\beta})})}\qquad \XXX{\beta=\text{false negative}}
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \end{itemize}
    \item Each conceptualization can have a variety of relevant controlling procedures that are used to correct for multiplicity issues, e.g,
      \begin{itemize}
        \item \ddd{Bonferroni correction}: a conservative method, free of dependence and distributional assumptions, wherein the \fff{false positive rate per comparison} is simply divided by the total number of hypotheses \(m\) tested, i.e.,
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \fff{\alpha}_i = \frac{\fff{\alpha}}{m} \leftrightarrow \text{reject \(H_i\) if} \leq \frac{\fff{\alpha}}{m} 
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \item \ddd{Šidák correction}: slightly more powerful than Bonferroni, but with small gain and potential to fail when tests are negatively dependent; found via solving the FWER equation, i.e.,
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \fff{\alpha}_i = 1 - (1-\alpha)^{1/m}
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \item Controlling procedures for false discovery rate not described, I'm not sure relevance as of now---might revisit later.
      \end{itemize}
  \end{itemize}

  \subsection{Primer: Cross-Validation}
    \begin{itemize}
    \item \ddd{Cross-validation}: a set of model validation techniques for assessing how well statistical analysis will generalize via parcelization of given data. 
      \begin{itemize}
        \item Mainly used to estimate how accurate a predictive model might be in practice for \hyperref[Subsection: Data Types]{\ulink{nominal and ordinal data}} (discrete is also possible).
        \item \ddd{Training set (known data)}: the portion of given data that a predictive model is used to train on. 
        \item \ddd{Testing set (unknown data)}: the portion of data set aside to later estimate accuracy of the trained model.
        \item Falls under the category of \ddd{``resampling''} methods, which also includes \hyperref[Subsection: Primer: Permutation Testing]{\dlink{permutation testing}} and \hyperref[Subsection: Primer: Bootstrapping]{\dlink{bootstrapping}}.
      \end{itemize}
    \item Cross-validation is used on models with one or more unknown parameters, wherein a dataset is used to fit the data to the parameter via optimization.
      \begin{itemize}
        \item \ddd{Optimization}: selection of the best element, with regard to some criterion, from some set of available alternatives.
        \item \ddd{Overfitting}: when analysis \emph{corresponds too closely} to a particular dataset, leading to poor predictive performance
        \item \ddd{Underfitting}: when analysis \emph{fails to capture} the underlying structure of the data, leading to poor predictive performance.
      \end{itemize}
    \item Cross-validation is of greater importance when dealing with \hyperref[Chapter: Regression]{\dlink{regression}} and \hyperref[Section: Confidence Intervals]{\dlink{confidence intervals}}.
      \begin{itemize}
        \item In-depth discussion will occur later, including distinctions between \hyperref[Subsection: Exhaustive vs. Non-Exhaustive]{\dlink{exhausting~and~non-exhaustive}} methods. 
        \item In most methods, multiple rounds of cross-validation are performed using different partitions, with the results being combined over the rounds.
      \end{itemize}
  \end{itemize}

  \subsection{P-Value vs. Classification Accuracy}
  \bigskip
  \begin{table}[h]
    \centering
    \begin{tabular}{rl}
      \ddd{P-Value} & \ddd{Accuracy}  \\
      Tests of probability of sample & Model outcome vs.\ observed outcome  \\
      Parameter based scoring  & Individual parameters uncertain \\
      Analytical solutions, theoretical & Empirically informed, inconsistent \\
      Works for most model/variable types & Restricted by model/variable type   \\
      Sensitive to extreme sample sizes & Robust to sample sizes \\
      \end{tabular}
  \end{table}
  
\end{itemize}

\section{T-Tests}
\begin{itemize}
  \item \ddd{Student's \(t\)-test}: a test statistic that follow a \hyperref[Subsection: Primer: Probability Distributions]{\ulink{student's t-distribution}}, i.e., a test for relatively small sample sizes with unknown variance. 
    \begin{itemize}
      \item Common \(t\)-tests include the \hyperref[Subsection: One-Sample and Two-Sample T-Tests]{\dlink{one-sample}} and \hyperref[Subsection: One-Sample and Two-Sample T-Tests]{\dlink{two-sample}} tests, often called student's \(t\)-test or simply, \(t\)-tests.
        \begin{itemize}
          \item Note: usage student's \(t\)-test implies the variances are assumed near equal.
        \end{itemize}
      \item Fundamentally, \(t\)-tests are often used to determine if the means of two sets of data are significantly different from each other (when \(p < t\)).
    \end{itemize}
  \item In general, most \(t\)-tests adopt the form based on the \ttt{signal}-to-\fff{noise} ratio, i.e., 
  \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  t_k = \frac{\ttt{Z}}{\fff{s}} = \frac{\ttt{\bar{x}-\bar{y}}}{\fff{\sigma/\sqrt{n}}}
  \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
    \item \(\ttt{Z}\): difference in means; sensitive to \(H_a\), increasing in magnitude if \(H_a\) is not wrong.
    \item \(\fff{s}\): scaling factor, of the standard deviations \(\sigma \) of the sample distribution. 
    \item \(n\): number of samples. \(k\): degrees of freedom.
    \item Thus, increasing the \(t\)-statistic can be done via increasing group differences, reducing variances, or increasing sample size.
  \end{itemize}

  \subsection{One-Sample and Two-Sample T-Tests}
  \begin{itemize}
    \item \ddd{One-sample \(t\)-test}: a single test aimed at determining whether a \emph{single set} of numbers could have been drawn from a distribution with a specified mean, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    t_{n-1} = \frac{\bar{x}-\mu_\nil}{\sigma/\sqrt{n}}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \begin{itemize}
        \item \(\bar{x}\): sample mean. \(\mu_{\mathsmaller\nil}\): specified mean of \(H_{\mathsmaller\nil}\). \(\sigma\): sample standard deviation. 
        \item \(k = n-1\), as the mean is the only unknown value.
      \end{itemize}
    \item Assumptions for one-sample \(t\)-test:
      \begin{itemize}
        \item Data are numeric, ideally interval or ratio (discrete can work, sometimes).
        \item Data are independent of each other and randomly drawn from the population.
        \item The parent population (\(H_\nil\)) does not need to be normally distributed, but the \(\bar{x}\) is assumed to be (approximately) normally distributed.
      \end{itemize}
    \item \ddd{Two-sample \(t\)-test}: an extension of the one-sample \(t\)-test, whereby \emph{two sets} of numbers could have been drawn form the same distribution.
    \begin{itemize}
      \item The numerator stays the same, but the denominator can change based on group pairing, size, and variance.
      \item \ddd{Paired or unpaired}: whether two groups of data are drawn form the same population, e.g.,
        \begin{itemize}
          \item Paired: same individuals sampled, overtime.
          \item Unpaired: different populations sampled overtime.
        \end{itemize}
      \item \ddd{Equal or unequal variance}: whether two groups have roughly equal variance.
      \item \ddd{Equal or unequal sample size}: whether the groups have the same number of values, only applied to unpaired groups.
      \item Exact algebraic definitions of each particular case will not be discussed; selection of relevant \(t\)-test depends on various combination of above factors and can easily be done in practice using various code libraries.
    \end{itemize}
  \end{itemize}

  \subsection{Nonparametric T-Tests}
  \begin{itemize}
    \item \ddd{Wilcoxon signed-rank test}: a \hyperref[Subsection: Parametric vs. Nonparametric]{\ulink{nonparametric}} variation of the one-sample or two-sample (paired) \(t\)-test.
      \begin{itemize}
        \item Mainly used when the data are assumed to be \emph{not normally distributed}; done via \emph{testing of medians} rather than means.
        \item Generally speaking, the test applies the following algorithm:
          \begin{itemize}
            \item Remove equal pairs.
            \item Rank-transform the differences, i.e., \(r= \operatorname{rank}(|x-y|)\)
            \item Sum ranks where \(x>y\).
            \item Convert to a \(z\)-score, which is normally distributed under the \(H_\nil\), allowing for \hyperref[Subsection: Degrees of Freedom]{\ulink{conversion to a p-value}}.
          \end{itemize}
      \end{itemize}
      \item Note: the actual process is not covered here, again, when to use tests like these are the important factor here.
      \item \ddd{Mann-Whitney U test (Wilcoxon rank-sum test)}: an alternative to the independent two-sample \(t\)-test, wherein the groups do \emph{not} need to have \emph{equal sample sizes}.
        \begin{itemize}
          \item The general algorithm:
            \begin{itemize}
              \item Note the samples sizes, specifically, determine dataset with \bbb{fewer} points \(\bbb{x_f}, \bbb{n_f}\) and dataset with \rrr{more} points \(\rrr{x_m}, \rrr{n_m}\).
              \item Pool data and compute rank, i.e., \(rank(\{\bbb{x_f},\rrr{x_m}\})\)
              \item Compute U score, i.e., \(U = \sum_{i = 1}^{\bbb{n_f}}r_i\)
              \item Convert to a \(z\)-score, \hyperref[Subsection: Degrees of Freedom]{\ulink{and thus}}, a p-value.
            \end{itemize}
        \end{itemize}
  \end{itemize}
  
  \subsection{Primer: Permutation Testing}
  \begin{itemize}
    \item \ddd{Permutation (randomization) test}: a test of statistical significance wherein the \emph{\(H_\nil\)~distribution} is obtained via calculation of \emph{all possible values} of the \emph{test statistic} under \emph{all possible rearrangements} of the observed data points.
      \begin{itemize}
        \item I.e., methods of treatments to the subjects of an experimental design is analysis of that design---if the labels are exchangeable under the \(H_\nil\), then results should to yield equal significance.
        \item Falls under the category of \ddd{``resampling''} methods, which also includes \hyperref[Subsection: Primer: Cross-Validation]{\ulink{cross-validation}} and \hyperref[Subsection: Primer: Bootstrapping]{\dlink{bootstrapping}}.
      \end{itemize}
    \item Permutation tests are mainly used to provide a p-value, generally done via the following methods:
      \begin{itemize}
        \item \ddd{Z-score approach}: simply the difference between observed value and the expected value of the \(H_\nil\) divided by the standard deviation of the \(H_\nil\), i.e.,
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        Z = \frac{obs - E[H_\nil]}{std[H_\nil]}
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \begin{itemize}
          \item \hyperref[Subsection: Degrees of Freedom]{\ulink{Conversion}} to p-value is then easily done. 
          \item The observed value is not contained within the \(H_\nil\), thus conversion to a \(z\)-score is often done case-by-case.
          \item Only works for approximately Gaussian \(H_\nil\) distributions.
        \end{itemize}
        \item \ddd{Counts approach}: proportion of times that the \(H_\nil\) was greater than observed value to the number of permutations ran, i.e.,
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        p_c = \frac{\sum(H_\nil > obs)}{N_{H_\nil}}
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \begin{itemize}
          \item Generally appropriate, distribution shape not as significant.
          \item Gives p-value directly, must be mindful of tail. 
          \item Can be more arbitrary than one would like.
        \end{itemize}
      \end{itemize}
      \item Again, permutation tests are a subset of nonparametric tests meant for unbalanced designs, potentially with mixtures of data types.
      \item Permutation testing can be computationally expensive, as it is in large part useful thanks to the exploitation of the \hyperref[Subsection: Law of Large Numbers and Central Limit Theorem]{\ulink{central limit theorem}}. 
  \end{itemize}
  
\end{itemize}

\section{Confidence Intervals}
\begin{itemize}
  \item \ddd{Confidence intervals (CI)}: the probability that an \emph{unknown population parameter \(\theta\)} falls \emph{within a range} of values in \emph{repeated samples}, i.e.,
  \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  p(L < \theta < U) = \gamma
  \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
    \item \ddd{Confidence level \(\gamma\)}: a somewhat arbitrary number between 0--1.
      \begin{itemize}
        \item Similar to significance levels, but instead it represents the consistency of the sampling of parameter in question, rather than the legitimacy of the \(H_\nil\).
        \item Typical values range from 0.95 to 0.99.
      \end{itemize}
  \end{itemize}
  \item Confidence intervals are influenced by the sample size and variance, with \emph{larger~sample sizes} and \emph{smaller variances} leading to shorter confidence intervals.
  \item Analytic method for computing confidence intervals:
  \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  CI = \bar{x}\pm t^*(k)\frac{\sigma}{\sqrt{n}}
  \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
    \item \(\bar{x}\): sample mean, \(\sigma\): sample standard deviation, \(n\): sample size
    \item \(t^*\): \(t\)-value with \(k\) degrees of freedom; \(t\)-value associated with one tail of confidence interval, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    t^*(k) = \operatorname{tinv}\hspace{-3pt}\left(\frac{1-\gamma}{2},~ n-1\right)
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item Note: \(\sigma\) is assumed to be an appropriate \hyperref[Chapter: Analysis of Variance (ANOVA)]{\dlink{measure of variability}}.
    \end{itemize}
    
  \end{itemize}

  \subsection{Primer: Bootstrapping}
  \begin{itemize}
    \item \ddd{Bootstrapping}: an empirical method for estimating measures of accuracy, via use of random sampling with replacement, of a given dataset.
      \begin{itemize}
        \item Falls under the category of methods termed \ddd{``resampling''}, which also includes \hyperref[Subsection: Primer: Cross-Validation]{\ulink{cross-validation}} and \hyperref[Subsection: Primer: Permutation Testing]{\ulink{permutation testing}}. 
        \item Treats the sample data as pseudo-population data, with each resample being a new pseudo-sample; leads to estimation of any measure of accuracy.
      \end{itemize}
    \item \true{\!Advantages} and \false{\!limitations (sometimes)} of bootstrapping:
    \begin{multicols}{2}
      \begin{itemize}
        \item \true{Works for any kind of parameter}.
        \item \true{Potentially useful for limited datasets}.
        \item \true{Not based on assumptions of normality}
        \item \false{Can be inconsistent}
        \item \false{Dependent on quality of representative sample}
        \item \false{Can be time/computationally intensive}.
      \end{itemize}
    \end{multicols}
  \end{itemize}

  \subsection{Confidence Intervals: Misconceptions}
  \begin{itemize}
    \item \false{Incorrect}:
      \begin{itemize}
        \item \textit{``I am 95\% confident that the population mean \fff{is the sample mean}''}
        \item \textit{``I am 95\% confident that the population mean \fff{is within the CI} in my dataset.''}
        \item \textit{``\fff{95\% of the data} are \fff{between} the upper and lower bounds of the CI.''}
        \item \textit{``CIs for \fff{two parameters overlap}, therefore, they \fff{cannot be significantly different}''}
      \end{itemize}
    \item \true{Correct}:
      \begin{itemize}
        \item \textit{``95\% of confidence intervals in \ttt{repeated} samples will \ttt{contain} the true population mean.''}
        \item The confidence interval is \ttt{not based on raw data}---it's based on descriptive statistics of the sample data.
        \item The confidence interval refers to the \ttt{estimate} of a parameter, \ttt{not the relationship between} parameters.
      \end{itemize}
  \end{itemize}
  
  
\end{itemize}
