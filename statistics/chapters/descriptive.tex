\chapter{Descriptive Statistics}

\section{Descriptive Statistics Fundamentals}
\begin{itemize}
  \item[]

  \subsection{Descriptive vs. Inferential Statistics}
  \begin{itemize}
    \item \ddd{Descriptive statistics}: the processes of using and analyzing summary statistics that quantitatively describes or summarizes features of a collection of information.
      \begin{itemize}
        \item Methods/measures of descriptive statistics:
          \begin{itemize}
            \item \hyperref[Subsection: Primer: Probability Distributions]{\dlink{Distribution shape}}
            \item \hyperref[Subsection: Measures of Central Tendency]{\dlink{Mean, median, mode}}
            \item \hyperref[Subsection: Measures of Dispersion]{\dlink{Variance}}
            \item \hyperref[Subsection: Statistical Moments]{\dlink{Kurtosis, skew}}
          \end{itemize}
        \item No relation to population.
        \item No generalization to other data sets.
        \item Concerned only with properties of observed data.
      \end{itemize}
    \item \ddd{Inferential statistics}: the process data analysis to deduce properties of an underlying probability distribution.
      \begin{itemize}
        \item Methods/measures of inferential statistics:
          \begin{itemize}
            \item \hyperref[Chapter: Probability Theory]{\dlink{Probability theory}}
            \item \hyperref[Chapter: Hypothesis Testing]{\dlink{Hypothesis testing}}
            \item \hyperref[Section: Confidence Intervals]{\dlink{Confidence intervals}}
            \item And essentially all of applied statistics.
          \end{itemize}
        \item Assumes that the observed data set is sampled from a larger population. 
        \item Entire purpose is to generalize/relate features to other data sets.
      \end{itemize}
  \end{itemize}

  \subsection{Accuracy, Precision, Resolution}
  \begin{itemize}
    \item \ddd{Accuracy}: the relationship between the measurement and the actual truth.
      \begin{itemize}
        \item Inversely related to bias; colloquially interchangeable with accuracy. 
      \end{itemize}
    \item \ddd{Precision}: the certainty of each measurement.
      \begin{itemize}
        \item Inversely related to \hyperref[Subsection: Measures of Dispersion]{\dlink{variance}}
      \end{itemize}
    \item \ddd{Resolution}: the number of data points per unit measurement (e.g., time, space, individual, etc).
    \item Generally, the goal is accuracy \to~precision \to~resolution, but often choice in the matter is not so deliberate.
  \end{itemize}
  
  \subsection{Primer: Probability Distributions}
  \begin{itemize}
    \item The shapes of data distributions are \hyperref[Subsection: Probability Functions]{\dlink{functions of probability theory}}; a more in-depth explanation will be covered later, but for now coverage of common distribution types might be useful.
    \item Overall, there is one major distinction of distribution type based on \hyperref[Subsection: Data Types]{\ulink{data types}} used, either discrete or continuous.
    \item \ddd{Discrete distribution}:
      \begin{itemize}
        \item Deals with events that occur in countable sample spaces; contains finite number of outcomes.
        \item Summation of values can be done to estimate probability of an interval.
        \item Expressed with graphs, piece-wise functions, or tables.
        \item Expected values might not be achievable.
        \item Common examples:
        \begin{multicols}{2}
        \begin{itemize}
          \item \link{https://en.wikipedia.org/wiki/Bernoulli_distribution}{Bernoulli}: a model for the set of possible outcomes of any single binary experiment.
          \item \link{https://en.wikipedia.org/wiki/Binomial_distribution}{Binomial}: a sequence of \(n\) independent Bernoulli experiments; a basis for the binomial test.
          \item \link{https://en.wikipedia.org/wiki/Discrete_uniform_distribution}{Uniform}: a known, finite number of values are equally likely to be observed.
          \item \link{https://en.wikipedia.org/wiki/Poisson_distribution}{Poisson}: a sequence of independent events over a specified interval with a known constant mean rate.
        \end{itemize}
        \end{multicols}
      \end{itemize}
      \item \ddd{Continuous distribution}: 
      \begin{itemize}
        \item Deals with events that occur in a continuous sample space; contains infinitely many consecutive values. 
        \item Summation of values in order to determine probability of interval not possible; integrals used instead.
        \item Expressed with continuous functions or graphs.
        \item Common examples:
        \begin{multicols}{2}
          \begin{itemize}
            \item \link{https://en.wikipedia.org/wiki/Normal_distribution}{Normal (Gaussian)}: used to represent real-valued random variables who are not known.
            \item \link{https://en.wikipedia.org/wiki/Chi-square_distribution}{Chi-Squared}: the sum of squares of \(k\) independent standard normal random variables.
            \item \link{https://en.wikipedia.org/wiki/Log-normal_distribution}{Lognormal}: distribution of a random variable whose logarithm is normally distributed.
            \item \link{https://en.wikipedia.org/wiki/Student\%27s_t-distribution}{Student's t}: estimations of the mean using small sample sizes with unknown standard deviations.
          \end{itemize}
        \end{multicols}
      \end{itemize}
      \item \link{https://en.wikipedia.org/wiki/List_of_probability_distributions}{Wikipedia's list of probability distributions}
    \end{itemize}
\end{itemize}

\section{Descriptive Techniques}
\begin{itemize}
  \item[]

  \subsection{Measures of Central Tendency}
  \begin{itemize}
    \item \ddd{Mean \(\bar{x}\)}: the sum of all measurements \(x_i\) divided by the number \(n\) of observations in the data set \(x\), i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bar{x}= n ^{-1}\sum_{i=1}^{n}x_i
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item Suitable for roughly normally distributed data of continuous data types.
    \end{itemize}
    \item \ddd{Median \(\operatorname{med}(x)\)}: the middle value of the data, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    x_i,\quad i=\frac{n+1}{2}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item Suitable for unimodal distributions of continuous data types.
      \item Odd number of observations with no distinct middle value are usually defined as the mean of the two middle values.
    \end{itemize}
    \item \ddd{Mode}: most common value.
      \begin{itemize}
        \item Suitable for any discrete distribution, usually used for nominal data types.
      \end{itemize}
  \end{itemize}

  \subsection{Measures of Dispersion}
  \begin{itemize}
    \item \ddd{Dispersion}: the measure of how distributed, or deviated, data are around a central value.
    \item \ddd{Variance \(\sigma^2, s^2\)}: the primary measure of dispersion, or more explicitly, the expectation of the squared deviation of a random variable from its mean, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \sigma^2 = \frac{1}{n-1}\sum_{i = 1}^{n}(x_i-\bar{x})^2
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item Suitable for any distribution; better for normally distributed data.
      \item Mean centering, i.e., \((x_i - \bar{x})\), is done to capture the dispersion around the average, but not the magnitude of the values themselves.
      \item The sum of a mean-centered data set would be zero, thus it is squared.
        \begin{itemize}
          \item \ddd{Mean absolute difference (MAD)}: when the absolute value of mean-centered data is taken instead of the square value. 
          \item MAD is more robust to outliers, but further from Euclidean distance and less commonly used. 
        \end{itemize}
      \item Division by \(n-1\) is used for sample variance, as often sample sizes can be small and are considered empirical quantities; \(n ^{-1}\) is used for population variance (a theoretical quantity).
      \item \ddd{Standard Deviation \(\sigma\)}: simply the square root of variance, \(\sqrt{\sigma^2}\)
    \end{itemize}
  \end{itemize}

  \subsection{Statistical Moments}
  \begin{itemize}
    \item \ddd{Moments}: a quantitative measure related to shape of a functions graph; relates to physics and statistics.
      \begin{itemize}
        \item Regarding probability distributions, the general formula can be defined as:
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        m_\emph{k} = n ^{-1} \sum_{i = 1}^{n}(x_i - \bar{x})^\emph{k}
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \item Increments of \(k\) define particular moments, i.e.,
          \begin{itemize}
            \item First moment \(k=1\): expected value, or \hyperref[Subsection: Measures of Central Tendency]{\ulink{mean}}.
            \item Second moment \(k=2\): central moment, or \hyperref[Subsection: Measures of Dispersion]{\ulink{variance}}.
            \item Third moment \(k=3\): dispersion asymmetry, or skewness.
            \item Fourth moment \(k=4\): tail ``thickness,'' or kurtosis.
            \item Further moments are possible, but useful applications are less common.
          \end{itemize}
        \item \ddd{Skewness}: a measure of asymmetry of a probability distribution of a real-valued random variable about its mean.
          \begin{itemize}
            \item Can be positive, zero, negative, or undefined.
            \item \bbb{Negative skew}: an indication that the tail is on the \bbb{left}.
            \item \minor{Zero skew}: an indication that tails \minor{balance out}; can be true for both asymmetric and symmetric distributions depending on kurtosis.
            \item \rrr{Positive skew}: an indication that the tail is on the \rrr{right}. 
          \end{itemize}
        \item \ddd{Kurtosis}: a measure of the thickness/curvature of the tail of a probability distribution is; an indication of deviation/outliers.
          \begin{itemize}
            \item Univariate normal distributions have a kurtosis of 3, leading to a common basis.
            \item \bbb{Platykurtic \(< 3\)}: a term for \bbb{low} kurtosis, indicating that a \bbb{lesser degree} of deviations or \bbb{outliers} is observed.
            \item \rrr{Leptokurtic \(> 3\)}: a term for \rrr{high} kurtosis, indicating that a \rrr{greater degree} of deviations or \rrr{outliers} is observed. 
            \item \ddd{Excess kurtosis}: kurtosis minus 3, often colloquially termed as kurtosis; an indication a greater degree outliers compared to a normal distribution. 
          \end{itemize}
      \end{itemize} 
  \end{itemize}
  
  \subsection{Visualizations Revisited}
  \begin{itemize}
    \item \ddd{Q-Q (quantile-quantile) plot}: a graphical method for comparing two probability distributions by plotting their quantiles against each other. 
      \begin{itemize}
        \item \ddd{Quantile}: cut points dividing the range of probability distributions into continuous intervals with equal probabilities, e.g.,
          \begin{itemize}
            \item Percentiles: 0--100
            \item Quartiles: 0--4
            \item Quantiles: 0--\(x\)
          \end{itemize}
        \item The points of similar distributions will lie approximately on the line \(y=x\); 
        \item However, other linear relations are possible, meaning points may not necessarily lie on the line \(y=x\).
        \item Provides a mean for comparing location, scale, and skewness of similarities of differences in two distributions.
      \end{itemize}
    \item \ddd{Histogram bin number \(k\)}: the is no ``best'' number of bins, different bin sizes can reveal different features of the data, but there are several methods of determining \(k\);
      \begin{itemize}
        \item Determination via suggested bin width \(h\):
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        k = \left\lceil \frac{\max(x) - \min(x)}{h} \right\rceil
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \item Sturges' formula: derived from binomial distribution; assumes approximately normal distribution:
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        k = \left\lceil \log_2(n) \right\rceil
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \item Freedman-Diaconis' rule: method of determining \(h\) using interquartile range (IQR); often method of choice:
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        h = 2\frac{\operatorname{IQR}(x)}{\sqrt[3]{n}}
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \item Arbitrary \(\approx42\): often intuitive guesses are sufficient and yield useable results:
      \end{itemize}
    \item \ddd{Violin plot}: similar to a box plot, but rotated with addition of a kernel density plot on each side. 
      \begin{itemize}
        \item \ddd{Kernel density plot}: essentially a smoothing estimation based on finite data samples.
        \item Statistical and IQR moments can be conveniently shown, sometimes with asymmetric comparisons of similar data sets (rather than a mirrored version).
      \end{itemize}
  \end{itemize}
  
\end{itemize}

\section{Introduction to Normalization}
\begin{itemize}
  \item \ddd{Normalization of ratings (feature scaling)}: adjusting values measured on different scales to notionally common scale, often prior to averaging.
    \begin{itemize}
      \item Often in more complicated cases, the adjustments are meant to bring the entire probability distribution of adjusted values into alignment.
    \end{itemize}
  \item \ddd{Normalized values (normalization)}: creation of shifted and scaled versions of samples with the intention of minimizing the effect of gross \hyperref[Subsection: Outliers]{\dlink{anomalies/outliers}}.
  \item There are many types of normalization techniques in statistics, each with their own respective applications based on data types and distribution shapes; for now, only standard score and min-max scaling will be covered, with others introduced at more appropriate times.

  \subsection{Z-Score Standardization}
  \begin{itemize}
    \item \ddd{Z-score (standard score)}: the number of \hyperref[Subsection: Measures of Dispersion]{\ulink{standard deviations \(\sigma\)}} by which the value of a raw score \(x_i\) is above or below the \hyperref[Subsection: Measures of Central Tendency]{\ulink{mean \(\bar{x}\)}}, i.e., 
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    z_i = \frac{x_i - \bar{x}}{\sigma_x}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item Application of z-normalization is best done on data that is roughly \hyperref[Subsection: Primer: Probability Distributions]{\ulink{Gaussian}}. 
    \item The z-score is dimensionless, as units cancel out, leading to main application wherein data of different scales can be meaningfully compared. 
  \end{itemize}

  \subsection{Min-Max Scaling}
  \begin{itemize}
    \item \ddd{Rescaling (min-max normalization) \(x'\)}: the simplest method of rescaling the range of features, either from \([0,1]\) or \([\text{\textminus}1,1]\); the general formula for \([0,1]\) is given as:
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    x'=\frac{x-\min(x)}{\max(x)-\min(x)}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item Rescaling to any arbitrary range \([\bbb{a},\rrr{b}]\):
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    x'=\bbb{a}+\frac{(x-\min(x))(\rrr{b}-\bbb{a})}{\max(x)-\min(x)}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    
  \end{itemize}
  

\subsection{Outliers}
\begin{itemize}
  \item \ddd{Outlier}: a data point that differs significantly from other observations, potentially due to a variety of reasons either, due to the cause of experimental error in observations, random noise, unexplained/surprising phenomena, or simply by natural variability.
  \item Outliers can cause serious errors in statistical analysis, as many methods square terms, leading to potentially huge errors. 
    \begin{itemize}
      \item Often extremely detrimental impacts on small sample sizes are observed, as significance of the outliers decrease with increasing sample size.
    \end{itemize}
  \item \ddd{Leverage}: a measure of how far away the independent variable values of an observation are from those of other observations.
    \begin{itemize}
      \item Outliers are worse near the ``edges'' of the data, compared to the ``middle,'' as outliers further away increase the leverage.
      \item Lower leverage has less influence on statistical analysis, and in particular, it is a large factor in \hyperref[Chapter: Regression]{\dlink{regression analysis}}.
    \end{itemize}
  \item There are two main strategies for dealing with outliers, either:
    \begin{itemize}
      \item Identify and \fff{remove outliers} prior to analysis; assuming outliers are \fff{noise or invalid}.
      \item \ttt{Keep outliers} in and use robust methods that attenuate the negative impact of outliers; assume outliers are \ttt{unusual but valid}.
        \begin{itemize}
          \item Robust methods of retention will be examined when more appropriate.
        \end{itemize}
    \end{itemize}
  \item Despite strategy chosen, outliers ought to be investigated; sometimes outliers might be an important aspect of the data.
\end{itemize}

  \subsection{Removing Outliers}
  \begin{itemize}
    \item There are many methods of removing outliers, here use of the \hyperref[Subsection: Z-Score Standardization]{\ulink{z-score}} is explained. Again, more in-depth examinations of methods will be examined when appropriate.
    \item First, data must be converted to a \emph{normalized} metric, e.g., the z-score.
    \item Next, a \emph{threshold} must be determined that marks data points for suspect, dealing with them either methods of truncation or winsorization.
      \begin{itemize}
        \item \ddd{Truncation (trimming)}: complete removal, with possible replacement of NaN placeholder to maintain indexing.
        \item \ddd{Winsorization (clipping)}: replacement outlier with the nearest or a less suspect ``alternative'' value.
        \item A variety of methods of determining such threshold can be used, even such methods lead to potentially arbitrary choices; 3 is often a default starting point.
      \end{itemize}
    \item Finally, suspect data are \emph{dealt with iteratively} until no other data pass the given threshold.
    \item Note, the z-score is generally only useful for roughly \hyperref[Subsection: Primer: Probability Distributions]{\ulink{Gaussian distributions}}, however, a modified z-score using the median can be applied for non-normal distributions, i.e., 
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    z_i = \frac{0.6745(x_i-\med(x))}{\med(|x_i-\med(x)|)}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item 0.6745 is a normalization factor equal the standard deviation units of \hyperref[boxplot]{\ulink{\(Q_3\)}} of a Gaussian distribution.
    \end{itemize}
    \item Deletion of data is generally avoided, with only clear indications of measurement error being the reason to do so. 
    \item Multivariate data sets are dealt in similar way, where the only difference is that the mean of the data set is taken by calculating the Euclidean distance between all points in the set, then applying the method(s) described above.
  \end{itemize}

\end{itemize}
