\chapter{Regression}

\section{Regression Fundamentals}
\begin{itemize}
  \item[]
  
  \subsection{Model-Fitting}
  \begin{itemize}
    \item \ddd{Model fitting}: the combination of fixed features and free parameters in such a way that fits experimental data to a mathematical models based on adjustments to the free parameters that attempts to explain the \yyy{dependent variable}.
      \begin{itemize}
        \item \ddd{Fixed features (regressor) \(\xxx{x_n}\)}: \xxx{independent variables} \emph{imposed on the model} based on previous knowledge, understanding, theories, hypotheses, or other evidence; has several other names based on context.
        \item \ddd{Free parameters \(\beta_n\)}: scalar variables that cannot be predicted precisely or constrained by the model; they must be \emph{adjusted or estimated}.
        \item \ddd{Intercept \(\beta_\nil\)}: the average when all other parameters are 0.
        \item \ddd{Residual (error) \(\displaystyle\err\)}: the data \fff{not directly observed} or fit \fff{by the model}.
      \end{itemize}
    \item \ddd{Model interpolation}: the prediction of other experimental results given prior experimental data and a fitted model based on those data.
    \item \ddd{The general outline of model-fitting}:
    \begin{itemize}
      \item \emph{Define the equation(s) underlying the model}; dependent on data availability. % chktex 36
        \begin{itemize}
          \item If \emph{all} the \xxx{fixed features} are \hyperref[Subsection: Data Types]{\ulink{discrete}}, then use \hyperref[Chapter: Analysis of Variance]{\ulink{ANOVA}}.
          \item If at least \emph{some} \xxx{fixed features} are \emph{continuous}, then use regression.
          \item E.g., height \(\yyy{h}\) is governed by numerous complex interactions, but a simplistic model can be made to estimate the importance of particular fixed features;
          \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
          \yyy{h} = \beta_\nil + \beta_1 \xxx{x_1} + \beta_2 \xxx{x_2} + \beta_3  \xxx{x_3} + \err
          \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
          \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
          \xxx{x_1}: \text{sex},\xxx{x_2}: \text{parents' height}, \xxx{x_3}: \text{nutrition}
          \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \end{itemize}
      \item \emph{Map the data to the model equations}, i.e.,  take the real, or simulated, data and map them to the \xxx{fixed features}.
        \begin{itemize}
          \item Yields a system of equations with a series of unknown parameters. 
          \item \ddd{Over determined}: a set of equations with more equations than unknowns.
        \end{itemize}
      \item \emph{Convert the equations into a matrix-vector equation}, i.e., \hyperref[Section: Regression Models]{\dlink{the general linear model}}.
        \begin{itemize}
          \item Sometimes simplified to \(\xxx{\bm{X}}\bm{\beta} = \bm{\y}\) (linear algebra nomenclature: \(\bm{Ax}=\bm{b}\)).
        \end{itemize}
      \item \emph{Computer the parameters}, e.g., using \hyperref[Subsection: Least-Squares]{\dlink{least-squares}}.
      \item \emph{Statistical evaluation of the model}, i.e., the application of inferential statistics.
        \begin{itemize}
          \item See \hyperref[Subsection: Model Significance]{\dlink{model significance}} and \hyperref[Subsection: Standardized Regression Coefficients]{\dlink{coefficients significance}}.
        \end{itemize}
    \end{itemize}
  \end{itemize}

  \subsection{Least-Squares}
  \begin{itemize}
    \item \ddd{Least-squares}: a standard approach in regression analysis to approximate the solution of over determined systems by minimizing the sum of the squares of the residuals, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \| \err \|^2 = \| \bm{\x\beta - \y}\|^2
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item \ddd{Residuals \(\displaystyle \err\)}: the vector that describes the difference{} between the observed value~\(\yyy{\bm{y}}\) and the estimated value \(\bm{\x\beta}~(\bm{\YYY{\hat{y}}})\), i.e.,
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \err = \bm{\YYY{\hat{y}}} - \bm{\y} = \bm{\x\beta - \y}
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \end{itemize}
  \item Since the design matrix \(\bm{\x}\) is an over determined system, then the \bbb{left inverse} can be used to isolate the regression coefficients if \(\bm{\x}\) has \emph{full column rank}, i.e.,
  \begin{align*}
    \bm{\x\beta} &= \bm{\y} \\
    \bbb{(\bm{X}^T \bm{X})^{-1} \bm{X}^T} \bm{\x\beta} &= \bbb{(\bm{X}^T \bm{X})^{-1} \bm{X}^T} \bm{\y} \\
    \bm{\beta} &= \bbb{(\bm{X}^T \bm{X})^{-1} \bm{X}^T} \bm{\y}
  \end{align*}
  \end{itemize}

  \subsection{Model Significance}
  \begin{itemize}
    \item \ddd{Coefficient of determination \(R^2\)}: a measure of how well observed outcomes are replicated by the models; exact definitions vary.
      \begin{itemize}
        \item In \hyperref[Subsection: Simple and Multiple Regression]{\dlink{simple linear regression}} it's referred to as \(r^2\), and if the intercept is included, then it's simply the square of the \hyperref[Subsection: Pearson Correlation]{\ulink{sample correlation coefficient \(r\)}}.
        \item \(R^2\) indicates that there are additional regressors, which means it is equal to the square of the coefficient of multiple correlation. 
        \item Values range from 0 to 1, with 1 indicating a better fit.
      \end{itemize}
    \item \(R^2\) can be found by comparing the \hyperref[Subsection: Sum of Squares]{\ulink{sum of squares}} of the residuals over the total sum of squares, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    R^2 = 1 - \frac{SS_\err}{SS_T} = 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item There is no real cut-off for a ``good fit'', often it is used to compare models.
    \end{itemize}
  \item Further evaluation of significance is similar to the \hyperref[Subsection: F-Test]{\ulink{F-test}} used in ANOVA, whereby determination of at least one \(\beta \neq 0\) is achieved, else all regressors under \(H_\nil = 0 \).
    \begin{itemize}
      \item The F-test here uses a comparison \(SS_{\text{\xxx{M}odel}} = \sum(\hat{y}_i-\bar{y})^2\) over \(SS_\err\), with degrees of freedom determined by number of parameters (+ \(\beta_\nil\)) \(\xxx{k}\), i.e.,
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      F_{(\xxx{k}-1,N-\xxx{k})} = \frac{SS_\xxx{M}/(\xxx{k}-1)}{SS_\err(N-\xxx{k})}
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \item Each individual \(\beta \) coefficient can then be evaluated using a \(t\)-distribution:
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      t_{N-\xxx{k}} = \frac{\beta}{\sigma_\beta} = \frac{\beta}{\sqrt{SS_\err/SS_T}}
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      
    \end{itemize}
  \end{itemize}

  \subsection{Standardized Regression Coefficients}
  \begin{itemize}
    \item \ddd{Standardized (regression) coefficient (weights)}: when the free parameters \(\beta\) of a model have been standardized so that the variances both the dependent and independent variables are equal to 1.
      \begin{itemize}
        \item Unstandardized \(\beta \) weights can change depending on the scale of the independent variables, leading to near impossible comparisons of variables and studies.
        \item There are some cases where unstandardized \(\beta \) weights can reflect important scales of the data, potentially facilitating interpretations.
      \end{itemize}
    \item Standardized \(\beta \) weights are unitless; the measure refers to effect of one-\hyperref[Subsection: Measures of Dispersion]{\ulink{standard deviations \(\sigma \)}} of a \xxx{regressor \(\x\)} on the \yyy{dependent variable \(\y\)}, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \beta^* = \beta_k\frac{\xxx{\sigma_{xk}}}{\yyy{\sigma_y}}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item Note: sometimes, standardization is done with only respect to the regressor.
      \item Another method involves \hyperref[Subsection: Z-Score Standardization]{\ulink{z-normalizing}} all the variables before regression.
    \end{itemize}
    \item Comparisons may be easier with standardized weights, but re-scaling based simply on standard deviations may increase difficulty of analysis due to \hyperref[Section: Primer: Partial Correlation]{\ulink{confounding}}.
      \begin{itemize}
        \item Additionally, non-normal distributions can potentially make the method of standardization much less representative of the truth.
      \end{itemize}
  \end{itemize}
  
\end{itemize}

\section{Regression Models}
\begin{itemize}
  \item \ddd{General linear model}: a compact way of writing several multiple linear regression models using matrix algebra, i.e.,
  \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \bm{\yyy{Y}} = \bm{\beta}\bm{\xxx{X}} + \err
  \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
    \item \(\yyy{\bm{Y}}\): matrix with a series of multivariate measurements, where each column is a set of measurements of one of the \yyy{dependent (response) variables}.
    \item \(\xxx{\bm{X}}\): the \xxx{design matrix}, where each column is a set of observations on \xxx{independent variables (regressors)}.
    \item \(\bm{\beta}\): the matrix of \(\beta \) coefficients (free parameters, scalars) to be estimated.
    \item \(\displaystyle\err\): the matrix of \fff{residuals} associated with the model.
  \end{itemize}
  
  \subsection{Simple and Multiple Regression}
  \begin{itemize}
    \item \ddd{Simple linear regression}: the simplest case using a \xxx{single regressor} (plus the \hyperref[Subsection: Model-Fitting]{\ulink{intercept}}) and a \yyy{single dependent variable} over a number of observations \(i\), i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \y_i = \beta_\nil + \beta_1\x_i + \err_i
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    
    \item \ddd{Multiple linear regression}: the generalization of simple linear regression to the case of \xxx{more than one regressor \(X_n\)}.
      \begin{itemize}
        \item There is a special case (the basic model), where the general linear model is restricted to \yyy{one dependent variable}, i.e.,
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \y_i = \beta_\nil + \beta_1 \x_{i1} + \beta_2 \x_{i2} + \cdots + \beta_n \x_{in} + \err_{i}
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \item As well as the more general multivariate linear regression, where there are \yyy{more than one dependent variables} that share the \xxx{same set of regressors}, i.e.,
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \y_{ij} = \beta_{\nil j} + \beta_{1j} \x_{i1} + \beta_{2j} \x_{i2} + \cdots + \beta_{nj} \x_{in} + \err_{ij}
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \item Where all dependent variables are indexed as \(j = 1, \ldots, n\)
        \item Note: \emph{multivariate analysis} of the general linear model deals with analysis of \emph{all} the \emph{outcomes at once}.
          \begin{itemize}
            \item In contrast, multiple linear regression (\emph{multivariable analysis}) defined here deals with \emph{each outcome independently}, with respect to each dependent variable examined.
          \end{itemize}
        \item Multiple linear regression often has \xxx{additional regressors} generated from \xxx{interactions} between various regressors in the set, as long as independence is maintained; for now, they can be thought of as new fixed features.
      \end{itemize}
  \end{itemize}
  
  
  \subsection{Polynomial Regression}
  \begin{itemize}
    \item \ddd{Polynomial regression}: a special case of multiple linear regression wherein the relationship between the \xxx{fixed features} and the \yyy{dependent variable} is \emph{modeled as an \(n\)th degree polynomial}, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \y = \beta_\nil \xxx{x^0} + \beta_1 \xxx{x^1} + \cdots + \beta_k \xxx{x^k} + \err
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \begin{itemize}
        \item The nonlinear relationship is fit to the values of \(\x\) and the corresponding conditional mean of \(\y\), i.e., \(E(\y~|~\x)\)
        \item The statistical problem is still linear, despite nonlinear fixed features, since the free parameters (\(\beta \) weights) are scalars that keep the model linear. 
      \end{itemize}
      \item \hyperref[Subsection: Nested Models]{\dlink{Under and overfitting}} can easily be done using polynomial regression, with a too low or too high order polynomial leading to each case, respectively. Therefore, a method of model selection is needed.
        \begin{itemize}
          \item \ddd{Bayesian information criterion (BIC)}: a criterion for model selection (often used in \hyperref[Subsection: Primer: Cross-Validation]{\ulink{cross validation}}) among a finite set of models; a lower BIC score is preferred.
          \item BIC defined in the case of model order selection:
          \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
          \operatorname{BIC}_\xxx{k} = n \ln(SS_\err) + \xxx{k}\ln(n) \qquad \xxx{k}\text{: n-parameters}, n\text{: n-data points}
          \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
          \item BIC applied to ordered models with increasing degree can be analyzed to find the lowest scoring value.
            \begin{itemize}
              \item Note: the lowest score is typically found after a large drop, with a slowly increasing score thereafter.
            \end{itemize}
        \end{itemize}
  \end{itemize}
  
  \subsection{Logistic Regression}
  \begin{itemize}
    \item \ddd{Logistic regression}: used to model the \emph{probability} of a \emph{binary} \yyy{dependent variable}.
      \begin{itemize}
        \item Outputs with more than two values are modeled by multinomial logistic regression, or ordinal logistic regression if the categories are ordered.
        \item Does not perform statistical classification, but can be made into a binary classifier by choosing a cutoff probability value.
      \end{itemize}
    \item Logistic regression involves a basic regression model the \ddd{log-odds (logit)}, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \ln\left(\frac{p}{1-p}\right) = \beta_\nil + \beta_1\xxx{x_1} + \cdots + \beta_k\xxx{x_k}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \begin{itemize}
        \item Finding the probability that \(\y = 1\) yields the \ddd{sigmoid function \(S(x)\)}:
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        p = S(x) = \frac{1}{1+e^{-x}} \qquad x = \beta_\nil + \beta_1\xxx{x_1} + \cdots + \beta_k\xxx{x_k}
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \item Using the log-odds increases the dynamic range of the small values, leading to better optimization results when dealing with those smaller probability values.
      \end{itemize}
    \item Nonlinearities in the coefficients prevent the left-inverse from being used to find the regression coefficient.
      \begin{itemize}
        \item Instead, iterative methods such as gradient descent are applied to find the set of parameters that make the probabilities best match the dependent variable
      \end{itemize}
  \end{itemize}

  \subsection{Nested Models}
  \begin{itemize}
    \item Qualitatively, overfitting and underfitting of models occurs when the models correspond to closely or fail to capture the underlying structure, respectively.
    \begin{itemize}
      \item All models exist between these two measures; sometimes certain trade-offs can make one direction more favorable, i.e., \begin{table}[h]
        \centering
        \begin{tabular}{rl}
          \ddd{Overfitting} & \ddd{Underfitting}  \\
          Overly sensitive to noise & Less sensitive to noise \\
          Increased sensitivity to subtle effects & Less likely to detect true effects \\
          Increased estimation complexity & Decreased estimation complexity \\
          Better results with more data & Better results with fewer data
          \end{tabular}
      \end{table}
    \vspace*{-8pt}
    \item Too far in either direction leads to reduced generalizability.
    \item \ddd{Hidden overfitting}: or ``researchers degree of freedom'', the result of choices made in data cleaning, organizing, selection, and other choices of surrounding model us, that occur in order to get a certain model to work. 
      \begin{itemize}
        \item Deciding on analysis pipelines to use in advance helps avoid this problem.
        \item Exploratory phases of the analysis pipelines should only be done on small samples of the data if needed.
      \end{itemize}
    \end{itemize}
    \item Quantitatively, there are methods of detecting the fit of a model via comparisons of ``full'' and ``reduced'' models. 
    \item \ddd{Extended (full) model}: a model that \emph{always} fits the data better than any reduced model; having more parameters (extending the model) is only justified when the model fit is significantly improved.
    \item \ddd{Nested (reduced) model}: a model that contains a subset of identical \xxx{fixed features} to the full model, both of which explain the same \yyy{dependent variable}.
      \begin{itemize}
        \item Many models can be nested under the same full model.
        \item Nested models can differ by more than one parameter, but often don't.
      \end{itemize}
      \item \hyperref[Subsection: Sum of Squares]{\ulink{Sum of squares}} and the \hyperref[Subsection: F-Test]{\ulink{\(F\)-test}} can be used for model comparison, similar to their usage in \hyperref[Subsection: Model Significance]{\ulink{model significance}}.
      \item Namely, the sum of squared errors \(SS_\err\) between the \XXX{full model} and \xxx{reduced model} can be used to quantitatively measure the model fit to the data, i.e.,
      \begin{align*}
        SS_\err = \sum(y_i - \hat{y})^2 \qquad y_i\text{: observed}\quad  \hat{y}\text{: predicted} \\
        F_{(\XXX{p} - \xxx{k}, n - \XXX{p} -1)} = \frac{\left(SS^{\xxx{R}}_\err - SS^{\XXX{F}}_\err\right)/(\XXX{p}-\xxx{k})}{SS^{\XXX{F}}_\err / (\XXX{p} - \xxx{k} - 1)}
      \end{align*}
      \begin{itemize}
        \item \(SS^{\XXX{F}}_\err \): \XXX{full model} \(SS_\err\) with the \XXX{full model's} number of parameters \(\XXX{p}\).
        \item \(SS^{\xxx{R}}_\err \): \xxx{reduced model} \(SS_\err\) with the \xxx{reduced model's} number of parameters \(\xxx{k}\).
      \end{itemize}
      \item The main point of focus is the difference between the full and reduced sum of squares \(\left(SS^{\xxx{R}}_\err - SS^{\XXX{F}}_\err\right)\).
        \begin{itemize}
          \item \ttt{F is statistically significant} when more parameters improves the model, i.e., the \emph{full model is preferred} when \(SS^{\XXX{F}}_\err\) is small, making \ttt{\(F\) large}.
          \item \fff{F is not significant} when reduced model fit is nearly as good as the full model, i.e., the \emph{reduced model is preferred} when the difference is low, making \fff{\(F\) small}.
        \end{itemize}
      \item Formally, the \(H_\nil\) sates that all additional parameters are essentially 0, while the \(H_a\) is true if at least one is not, i.e.,
      \begin{align*}
        \ttt{H_\nil} = \beta_{\xxx{k}+1} = \cdots = \beta_\XXX{p} = 0 \\
        \fff{H_a} = N\left\{\beta_{\xxx{k}+1:\XXX{p}} \neq 0\right\} \geq 1
      \end{align*}
      \item \hyperref[Subsection: Post Hoc Comparisons]{\ulink{Post hoc comparisons}} would need to be performed in case of \(H_a\) being true.
  \end{itemize}
\end{itemize}

\section{Statistical Power and Sample Size}
\begin{itemize}
  \item []
  
  \subsection{Statistical Power}
  \begin{itemize}
    \item 
  \end{itemize}

  \subsection{Sample Size}
  \begin{itemize}
    \item 
  \end{itemize}
  
\end{itemize}
