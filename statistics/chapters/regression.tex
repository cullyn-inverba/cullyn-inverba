\chapter{Regression}

\section{Regression Fundamentals}
\begin{itemize}
  \item[]
  
  \subsection{Model-Fitting}
  \begin{itemize}
    \item \ddd{Model fitting}: the combination of fixed features and free parameters in such a way that fits experimental data to a mathematical models based on adjustments to the free parameters that attempts to explain the \yyy{dependent variable}.
      \begin{itemize}
        \item \ddd{Fixed features (regressor) \(\xxx{x_n}\)}: \xxx{independent variables} \emph{imposed on the model} based on previous knowledge, understanding, theories, hypotheses, or other evidence; has several other names based on context.
        \item \ddd{Free parameters \(\beta_n\)}: scalar variables that cannot be predicted precisely or constrained by the model; they must be \emph{adjusted or estimated}.
        \item \ddd{Intercept \(\beta_\nil\)}: the average when all other parameters are 0.
        \item \ddd{Residual (error) \(\displaystyle\err\)}: the data \fff{not directly observed} or fit \fff{by the model}.
      \end{itemize}
    \item \ddd{Model interpolation}: the prediction of other experimental results given prior experimental data and a fitted model based on those data.
    \item \ddd{The general outline of model-fitting}:
    \begin{itemize}
      \item \emph{Define the equation(s) underlying the model}; dependent on data availability. % chktex 36
        \begin{itemize}
          \item If \emph{all} the \xxx{fixed features} are \hyperref[Subsection: Data Types]{\ulink{discrete}}, then use \hyperref[Chapter: Analysis of Variance]{\ulink{ANOVA}}.
          \item If at least \emph{some} \xxx{fixed features} are \emph{continuous}, then use regression.
          \item E.g., height \(\yyy{h}\) is governed by numerous complex interactions, but a simplistic model can be made to estimate the importance of particular fixed features;
          \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
          \yyy{h} = \beta_\nil + \beta_1 \xxx{x_1} + \beta_2 \xxx{x_2} + \beta_3  \xxx{x_3} + \err
          \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
          \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
          \xxx{x_1}: \text{sex},\xxx{x_2}: \text{parents' height}, \xxx{x_3}: \text{nutrition}
          \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \end{itemize}
      \item \emph{Map the data to the model equations}, i.e.,  take the real, or simulated, data and map them to the \xxx{fixed features}.
        \begin{itemize}
          \item Yields a system of equations with a series of unknown parameters. 
          \item \ddd{Over determined}: a set of equations with more equations than unknowns.
        \end{itemize}
      \item \emph{Convert the equations into a matrix-vector equation}, i.e., \hyperref[Section: Regression Models]{\dlink{the general linear model}}.
        \begin{itemize}
          \item Sometimes simplified to \(\xxx{\bm{X}}\bm{\beta} = \bm{\y}\) (linear algebra nomenclature: \(\bm{Ax}=\bm{b}\)).
        \end{itemize}
      \item \emph{Computer the parameters}, e.g., using \hyperref[Subsection: Least-Squares]{\dlink{least-squares}}.
      \item \emph{Statistical evaluation of the model}, i.e., the application of inferential statistics.
        \begin{itemize}
          \item See \hyperref[Subsection: Model Significance]{\dlink{model significance}} and \hyperref[Subsection: Standardized Regression Coefficients]{\dlink{coefficients significance}}.
        \end{itemize}
    \end{itemize}
  \end{itemize}

  \subsection{Least-Squares}
  \begin{itemize}
    \item \ddd{Least-squares}: a standard approach in regression analysis to approximate the solution of over determined systems by minimizing the sum of the squares of the residuals, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \| \err \|^2 = \| \bm{\x\beta - \y}\|^2
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item \ddd{Residuals \(\displaystyle \err\)}: the vector that describes the difference{} between the observed value~\(\yyy{\bm{y}}\) and the estimated value \(\bm{\x\beta}~(\bm{\YYY{\hat{y}}})\), i.e.,
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \err = \bm{\YYY{\hat{y}}} - \bm{\y} = \bm{\x\beta - \y}
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \end{itemize}
  \item Since the design matrix \(\bm{\x}\) is an over determined system, then the \bbb{left inverse} can be used to isolate the regression coefficients if \(\bm{\x}\) has \emph{full column rank}, i.e.,
  \begin{align*}
    \bm{\x\beta} &= \bm{\y} \\
    \bbb{(\bm{X}^T \bm{X})^{-1} \bm{X}^T} \bm{\x\beta} &= \bbb{(\bm{X}^T \bm{X})^{-1} \bm{X}^T} \bm{\y} \\
    \bm{\beta} &= \bbb{(\bm{X}^T \bm{X})^{-1} \bm{X}^T} \bm{\y}
  \end{align*}
  \end{itemize}

  \subsection{Model Significance}
  \begin{itemize}
    \item \ddd{Coefficient of determination \(R^2\)}: a measure of how well observed outcomes are replicated by the models; exact definitions vary.
      \begin{itemize}
        \item In \hyperref[Subsection: Simple and Multiple Regression]{\dlink{simple linear regression}} it's referred to as \(r^2\), and if the intercept is included, then it's simply the square of the \hyperref[Subsection: Pearson Correlation]{\ulink{sample correlation coefficient \(r\)}}.
        \item \(R^2\) indicates that there are additional regressors, which means it is equal to the square of the coefficient of multiple correlation. 
        \item Values range from 0 to 1, with 1 indicating a better fit.
      \end{itemize}
    \item \(R^2\) can be found by comparing the \hyperref[Subsection: Sum of Squares]{\ulink{sum of squares}} of the residuals over the total sum of squares, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    R^2 = 1 - \frac{SS_\err}{SS_T} = 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item There is no real cut-off for a ``good fit'', often it is used to compare models.
    \end{itemize}
  \item Further evaluation of significance is similar to the \hyperref[Subsection: F-Test]{\ulink{F-test}} used in ANOVA, whereby determination of at least one \(\beta \neq 0\) is achieved, else all regressors under \(H_\nil = 0 \).
    \begin{itemize}
      \item The F-test here uses a comparison \(SS_{\text{\xxx{M}odel}} = \sum(\hat{y}_i-\bar{y})^2\) over \(SS_\err\), with degrees of freedom determined by number of parameters (+ \(\beta_\nil\)) \(\xxx{k}\), i.e.,
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      F_{(\xxx{k}-1,N-\xxx{k})} = \frac{SS_\xxx{M}/(\xxx{k}-1)}{SS_\err(N-\xxx{k})}
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \item Each individual \(\beta \) coefficient can then be evaluated using a \(t\)-distribution:
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      t_{N-\xxx{k}} = \frac{\beta}{\sigma_\beta} = \frac{\beta}{\sqrt{SS_\err/SS_T}}
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      
    \end{itemize}
  \end{itemize}

  \subsection{Standardized Regression Coefficients}
  \begin{itemize}
    \item \ddd{Standardized (regression) coefficient (weights)}: when the free parameters \(\beta\) of a model have been standardized so that the variances both the dependent and independent variables are equal to 1.
      \begin{itemize}
        \item Unstandardized \(\beta \) weights can change depending on the scale of the independent variables, leading to near impossible comparisons of variables and studies.
        \item There are some cases where unstandardized \(\beta \) weights can reflect important scales of the data, potentially facilitating interpretations.
      \end{itemize}
    \item Standardized \(\beta \) weights are unitless; the measure refers to effect of one-\hyperref[Subsection: Measures of Dispersion]{\ulink{standard deviations \(\sigma \)}} of a \xxx{regressor \(\x\)} on the \yyy{dependent variable \(\y\)}, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \beta^* = \beta_k\frac{\xxx{\sigma_{xk}}}{\yyy{\sigma_y}}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item Note: sometimes, standardization is done with only respect to the regressor.
      \item Another method involves \hyperref[Subsection: Z-Score Standardization]{\ulink{z-normalizing}} all the variables before regression.
    \end{itemize}
    \item Comparisons may be easier with standardized weights, but re-scaling based simply on standard deviations may increase difficulty of analysis due to \hyperref[Section: Primer: Partial Correlation]{\ulink{confounding}}.
      \begin{itemize}
        \item Additionally, non-normal distributions can potentially make the method of standardization much less representative of the truth.
      \end{itemize}
  \end{itemize}
  
\end{itemize}

\section{Regression Models}
\begin{itemize}
  \item \ddd{General linear model}: a compact way of writing several multiple linear regression models using matrix algebra, i.e.,
  \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \bm{\y} = \bm{\x\beta} + \err
  \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
    \item \(\yyy{\bm{Y}}\): matrix with a series of multivariate measurements, where each column is a set of measurements of one of the \yyy{dependent (response) variables}.
    \item \(\xxx{\bm{X}}\): the \xxx{design matrix}, where each column is a set of observations on \xxx{independent variables (regressors)}.
    \item \(\bm{\beta}\): the matrix of \(\beta \) coefficients (free parameters, scalars) to be estimated.
    \item \(\displaystyle\err\): the matrix of \fff{residuals} associated with the model.
  \end{itemize}
  
  \subsection{Simple and Multiple Regression}
  \begin{itemize}
    \item \ddd{Simple linear regression}: the simplest case using a \xxx{single regressor} (plus the \hyperref[Subsection: Model-Fitting]{\ulink{intercept}}) and a \yyy{single dependent variable} over a number of observations \(i\), i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \y_i = \beta_\nil + \beta_1\x_i + \err_i
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    
    \item \ddd{Multiple linear regression}: the generalization of simple linear regression to the case of \xxx{more than one regressor \(X_n\)}.
      \begin{itemize}
        \item There is a special case (the basic model), where the general linear model is restricted to \yyy{one dependent variable}, i.e.,
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \y_i = \beta_\nil + \beta_1 \x_{i1} + \beta_2 \x_{i2} + \cdots + \beta_n \x_{in} + \err_{i}
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \item As well as the more general multivariate linear regression, where there are \yyy{more than one dependent variables} that share the \xxx{same set of regressors}, i.e.,
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \y_{ij} = \beta_{\nil j} + \beta_{1j} \x_{i1} + \beta_{2j} \x_{i2} + \cdots + \beta_{nj} \x_{in} + \err_{ij}
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \item Where all dependent variables are indexed as \(j = 1, \ldots, n\)
        \item Note: \emph{multivariate analysis} of the general linear model deals with analysis of \emph{all} the \emph{outcomes at once}.
          \begin{itemize}
            \item In contrast, multiple linear regression (\emph{multivariable analysis}) defined here deals with \emph{each outcome independently}, with respect to each dependent variable examined.
          \end{itemize}
        \item Multiple linear regression often has \xxx{additional regressors} generated from \xxx{interactions} between various regressors in the set, as long as independence is maintained; for now, they can be thought of as new fixed features.
      \end{itemize}
  \end{itemize}
  
  
  \subsection{Polynomial Regression}
  \begin{itemize}
    \item \ddd{Polynomial regression}: a special case of multiple linear regression wherein the relationship between the \xxx{fixed features} and the \yyy{dependent variable} is \emph{modelled as an \(n\)th degree polynomial}, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \y = \beta_\nil \xxx{x^0} + \beta_1 \xxx{x^1} + \cdots + \beta_k \xxx{x^k} + \err
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \begin{itemize}
        \item The nonlinear relationship is fit to the values of \(\x\) and the corresponding conditional mean of \(\y\), i.e., \(E(\y~|~\x)\)
        \item The statistical problem is still linear, despite nonlinear fixed features, since the free parameters (\(\beta \) weights) are scalars that keep the model linear. 
      \end{itemize}
      \item \hyperref[Subsection: Primer: Cross-Validation]{\ulink{Under and overfitting}} can easily be done using polynomial regression, with a too low or too high order polynomial leading to each case, respectively. Therefore, a method of model selection is needed.
        \begin{itemize}
          \item \ddd{Bayesian information criterion (BIC)}: a criterion for model selection (often used in \hyperref[Subsection: Primer: Cross-Validation]{\ulink{cross validation}}) among a finite set of models; a lower BIC score is preferred.
          \item BIC defined in the case of model order selection:
          \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
          \operatorname{BIC}_\xxx{k} = n \ln(SS_\err) + \xxx{k}\ln(n) \qquad \xxx{k}\text{: n-parameters}, n\text{: n-data points}
          \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
          \item BIC applied to various model order parameters can then be analyzed to find the lowest scoring value 
            \begin{itemize}
              \item Note: the lowest score is typically found after a large drop, with a slowly increasing score thereafter.
            \end{itemize}
        \end{itemize}
  \end{itemize}
  
  \subsection{Logistic Regression}
  \begin{itemize}
    \item 
  \end{itemize}

  \subsection{Nested Models}
  \begin{itemize}
    \item 
  \end{itemize}
  
\end{itemize}

\section{Statistical Power and Sample Size}
\begin{itemize}
  \item []
  
  \subsection{Statistical Power}
  \begin{itemize}
    \item 
  \end{itemize}

  \subsection{Sample Size}
  \begin{itemize}
    \item 
  \end{itemize}
  
\end{itemize}
