\chapter{Regression}

\section{Regression Fundamentals}
\begin{itemize}
  \item[]
  
  \subsection{Model-Fitting}
  \begin{itemize}
    \item \ddd{Model fitting}: the combination of fixed features and free parameters in such a way that fits experimental data to a mathematical models based on adjustments to the free parameters that attempts to explain the \yyy{dependent variable}.
      \begin{itemize}
        \item \ddd{Fixed features (regressor) \(\xxx{x_n}\)}: \xxx{independent variables} \emph{imposed on the model} based on previous knowledge, understanding, theories, hypotheses, or other evidence; has several other names based on context.
        \item \ddd{Free parameters \(\beta_n\)}: scalar variables that cannot be predicted precisely or constrained by the model; they must be \emph{adjusted or estimated}.
        \item \ddd{Intercept \(\beta_\nil\)}: the average when all other parameters are 0.
        \item \ddd{Residual (error) \(\displaystyle\fff{\varepsilon}\)}: the data \fff{not directly observed} or fit \fff{by the model}.
      \end{itemize}
    \item \ddd{Model interpolation}: the prediction of other experimental results given prior experimental data and a fitted model based on those data.
    \item \ddd{The general outline of model-fitting}:
    \begin{itemize}
      \item \emph{Define the equation(s) underlying the model}; dependent on data availability. % chktex 36
        \begin{itemize}
          \item If \emph{all} the \xxx{fixed features} are \hyperref[Subsection: Data Types]{\ulink{discrete}}, then use \hyperref[Chapter: Analysis of Variance]{\ulink{ANOVA}}.
          \item If at least \emph{some} \xxx{fixed features} are \emph{continuous}, then use regression.
          \item E.g., height \(\yyy{h}\) is governed by numerous complex interactions, but a simplistic model can be made to estimate the importance of particular fixed features;
          \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
          \yyy{h} = \beta_\nil + \beta_1 \xxx{x_1} + \beta_2 \xxx{x_2} + \beta_3  \xxx{x_3} + \fff{\varepsilon}
          \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
          \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
          \xxx{x_1}: \text{sex},\xxx{x_2}: \text{parents' height}, \xxx{x_3}: \text{nutrition}
          \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \end{itemize}
      \item \emph{Map the data to the model equations}, i.e.,  take the real, or simulated, data and map them to the \xxx{fixed features}.
        \begin{itemize}
          \item Yields a system of equations with a series of unknown parameters. 
          \item \ddd{Over determined}: a set of equations with more equations than unknowns.
        \end{itemize}
      \item \emph{Convert the equations into a matrix-vector equation}, i.e., \hyperref[Subsection: Multiple Regression]{\dlink{the general linear model}}.
        \begin{itemize}
          \item Sometimes simplified to \(\xxx{\bm{X}}\bm{\beta} = \bm{\yyy{y}}\) (linear algebra nomenclature: \(\bm{Ax}=\bm{b}\)).
        \end{itemize}
      \item \emph{Computer the parameters}, e.g., using \hyperref[Subsection: Least-Squares]{\dlink{least-squares}}.
      \item \emph{Statistical evaluation of the model}, i.e., the application of inferential statistics.
        \begin{itemize}
          \item See \hyperref[Subsection: Model Significance]{\dlink{model significance}} and \hyperref[Subsection: Standardized Coefficients]{\dlink{coefficients significance}}.
        \end{itemize}
    \end{itemize}
  \end{itemize}

  \subsection{Least-Squares}
  \begin{itemize}
    \item \ddd{Least-squares}: a standard approach in regression analysis to approximate the solution of over determined systems by minimizing the sum of the squares of the residuals, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \| \fff{\varepsilon} \|^2 = \| \bm{\xxx{X}\beta - \yyy{y}}\|^2
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item \ddd{Residuals \(\displaystyle \fff{\varepsilon}\)}: the vector that describes the difference{} between the observed value~\(\yyy{\bm{y}}\) and the estimated value \(\bm{\xxx{X}\beta}~(\bm{\YYY{\hat{y}}})\), i.e.,
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \fff{\varepsilon} = \bm{\YYY{\hat{y}}} - \bm{\yyy{y}} = \bm{\xxx{X}\beta - \yyy{y}}
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \end{itemize}
  \item Since the design matrix \(\bm{\xxx{X}}\) is an over determined system, then the \bbb{left inverse} can be used to isolate the regression coefficients if \(\bm{\xxx{X}}\) has \emph{full column rank}, i.e.,
  \begin{align*}
    \bm{\xxx{X}\beta} &= \bm{\yyy{y}} \\
    \bbb{(\bm{X}^T \bm{X})^{-1} \bm{X}^T} \bm{\xxx{X}\beta} &= \bbb{(\bm{X}^T \bm{X})^{-1} \bm{X}^T} \bm{\yyy{y}} \\
    \bm{\beta} &= \bbb{(\bm{X}^T \bm{X})^{-1} \bm{X}^T} \bm{\yyy{y}}
  \end{align*}
  \end{itemize}

  \subsection{Model Significance}
  \begin{itemize}
    \item \ddd{Coefficient of determination \(R^2\)}: a measure of how well observed outcomes are replicated by the models; exact definitions vary.
      \begin{itemize}
        \item In \hyperref[Subsection: Simple Regression]{\dlink{simple linear regression}} it's referred to as \(r^2\), and if the intercept is included, then it's simply the square of the \hyperref[Subsection: Pearson Correlation]{\ulink{sample correlation coefficient \(r\)}}.
        \item \(R^2\) indicates that there are additional regressors, which means it is equal to the square of the coefficient of multiple correlation. 
        \item Values range from 0 to 1, with 1 indicating a better fit.
      \end{itemize}
    \item \(R^2\) can be found by comparing the \hyperref[Subsection: Sum of Squares]{\ulink{sum of squares}} of the residuals over the total sum of squares, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    R^2 = 1 - \frac{SS_\fff{\varepsilon}}{SS_T} = 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item There is no real cut-off for a ``good fit'', often it is used to compare models.
    \end{itemize}
  \item Further evaluation of significance is similar to the \hyperref[Subsection: F-Test]{\ulink{F-test}} used in ANOVA, whereby determination of at least one \(\beta \neq 0\) is achieved, else all regressors under \(H_\nil = 0 \).
    \begin{itemize}
      \item The F-test here uses a comparison \(SS_{\text{\xxx{M}odel}} = \sum(\hat{y}_i-\bar{y})^2\) over \(SS_\fff{\varepsilon}\), with degrees of freedom determined by number of parameters (+ \(\beta_\nil\)) \(\xxx{k}\), i.e.,
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      F_{(\xxx{k}-1,N-\xxx{k})} = \frac{SS_\xxx{M}/(\xxx{k}-1)}{SS_\fff{\varepsilon}(N-\xxx{k})}
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \item Each individual \(\beta \) coefficient can then be evaluated using a \(t\)-distribution:
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      t_{N-\xxx{k}} = \frac{\beta}{\sigma_\beta} = \frac{\beta}{\sqrt{SS_\fff{\varepsilon}/SS_T}}
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      
    \end{itemize}
  \end{itemize}

  \subsection{Standardized Coefficients}
  \begin{itemize}
    \item 
  \end{itemize}
  
\end{itemize}

\section{Regression Models}
\begin{itemize}
  \item [] 
  
  \subsection{Simple Regression}
  \begin{itemize}
    \item 
  \end{itemize}
  
  \subsection{Multiple Regression}
  \begin{itemize}
    \item \ddd{General linear model}: a compact way of writing several multiple linear regression models using matrix algebra, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bm{\yyy{Y}} = \bm{\xxx{X}\beta} + \fff{\varepsilon}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item \(\yyy{\bm{Y}}\): matrix with a series of multivariate measurements, where each column is a set of measurements of one of the \yyy{dependent variables}.
      \item \(\xxx{\bm{X}}\): the \xxx{design matrix}, where each column is a set of observations on \xxx{independent variables}.
      \item \(\bm{\beta}\): the matrix of \(\beta \) coefficients to be estimated.
      \item \(\displaystyle\fff{\varepsilon}\): the matrix of \fff{residuals} associated with the model.
    \end{itemize}
  \end{itemize}
  
  \subsection{Polynomial Regression}
  \begin{itemize}
    \item 
  \end{itemize}
  
  \subsection{Logistic Regression}
  \begin{itemize}
    \item 
  \end{itemize}

  \subsection{Nested Models}
  \begin{itemize}
    \item 
  \end{itemize}
  
\end{itemize}

\section{Statistical Power and Sample Size}
\begin{itemize}
  \item []
  
  \subsection{Statistical Power}
  \begin{itemize}
    \item 
  \end{itemize}

  \subsection{Sample Size}
  \begin{itemize}
    \item 
  \end{itemize}
  
\end{itemize}
