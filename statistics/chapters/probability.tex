\chapter{Probability Theory}

\section{Probability Fundamentals}
\begin{itemize}
  \item \ddd{Probability}: a measure of the likelihood that an event will occur; used to quantify attitudes towards propositions whose truth are not certain.
    \begin{itemize}
      \item Quantitatively, probability is a number between 0 and 1, which is often expressed as a percentage.
    \end{itemize}
  \item \ddd{Probability theory}: the axiomatic formalization of probability; widely used in many fields of study from math to philosophy.
  \item \ddd{Probability space \((\Omega, \F, P)\)}: a formal construct consisting of three elements that provides a model for a random process.
    \begin{itemize}
      \item \ddd{Sample space \(\Omega\)}: the set of all possible outcomes.
      \item \ddd{Event space \(\F\)}: all sets of outcomes; all subsets of the sample space.
      \item \ddd{Probability function \(P(E \in \F)\)}: the assignment of a number between 0 and 1 that represents the probability of each event \(E\) in event space.
    \end{itemize}
  \item \ddd{Proportion}: the measure of certainty; a fraction of a whole or the relation between two varying quantities.
    \begin{itemize}
      \item Proportion \textit{could} involve random variables, so depending on how the question is asked, then proportion could be the same as probability, but ultimately they are not interchangeable.
    \end{itemize}
  \item \ddd{Odds}: the ratio of the number of events that produce an outcome to the number of events that do not; essentially probability reframed in potentially more efficient way.

  \subsection{Probability Theory Axioms}
  \begin{itemize}
    \item \ddd{First axiom}: the probability of an event is a \emph{non-negative number real number}, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    P(E) \in \R,\quad P(E)\geq 0 \qqquad \forall E \in \F
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item \ddd{Second axiom}: the assumption of unit measure; the probability that \emph{at least one elementary event} in the entire sample space \emph{will occur} is 1, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    P(\Omega) = 1
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item \ddd{Third axiom}: the assumption of \(\sigma \)-additivity, wherein any \emph{countable} sequence of \hyperref[Subsection: Independent and Mutually Exclusive Events]{\dlink{disjoint sets}} \(E_1,E_2,\ldots\) satisfies
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    P\left(\bigcup_{i=1}^{\infty}E_i\right) = \sum_{i = 1}^{\infty}P(E_i)
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item Thus, only \hyperref[Subsection: Data Types]{\ulink{discrete data}} are valid for probability; continuous data must be converted to discrete forms in order to be valid.
    \end{itemize}
  \end{itemize}

  \subsection{Independent and Mutually Exclusive Events}
  \begin{itemize}
    \item \ddd{Stochastically independent}: when an event does not affect the probability of another, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    P(A~\ttt{\text{and}}~B)=P(A\cap B)=P(A)P(B)
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item Two random variables are independent if the realization of one does not affect the probability distribution of the other.
      \item \ddd{Pairwise independent (weak notion)}: two specific events in a collection that are independent of each other.
      \item \ddd{Mutually independent (strong notion)}: when each event is independent of any combination of other events in the collection.
      \item Often the stronger notion is simply termed independence, as it implies the weaker version, but not the other way around.
    \end{itemize}
    \item \ddd{Mutually exclusive (disjoint)}: two events that cannot occur at the same time, i.e., 
    \begin{itemize}
      \item Probability of \ttt{both}:
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      P(A~\text{\ttt{and}}~B) = P(A\cap B)= \minor{0} 
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \item Probability of \fff{either}: \(\hspace{182pt}\minor{\downarrow}\)
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      P(A~\fff{\text{or}}~B)=P(A\cup B)=  P(A)+P(B) - \minor{P(A\cap B)} = P(A)+P(B) 
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \end{itemize}
    \item \ddd{Collectively exhaustive (jointly)}: when at least one event must occur while exhausting all other possibilities at a given time, or that their union must coverall the events within the entire sample space, i.e., 
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    A\cup B = \Omega
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    
  \end{itemize}
  
  \subsection{Primer: Conditional Probability}
  \begin{itemize}
    \item \ddd{Conditional probability}: the probability of some event \(A\), given \(|\) the occurrence of some other event \(B\), i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    P(A~|\iipt B)=\frac{P(A\cap B)}{P(B)}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item Note, \(P(A\iipt | \ipt B)\) typically differs from \(P(B \iipt | \ipt A)\), falsely equating the two often results in errors, termed the base rate fallacy.
    \end{itemize}
    \item \ddd{Bayes' theorem}: probability of an event based on prior knowledge of conditions that might be related to the event; inference using conditional probability, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    P(A~|\iipt B)=\frac{P(B\,|\iipt A)P(A)}{P(B)}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item More on Bayesian statistics may or not be explored in greater depth in this course.
  \end{itemize}
  
  
  \subsection{Probability Functions}
  \begin{itemize}
    \item Again, a \hyperref[Section: Probability Fundamentals]{\ulink{probability function}} is the assignment of a number of \hyperref[Section: Probability Fundamentals]{\ulink{probability space}} (sometimes denoted \(X,\mathcal{A},P\), respectively). 
    \item \ddd{Probability distributions}: the product of  variations of the probability function based on given event space and properties of data types.
    \begin{itemize}
      \item As mentioned in the \hyperref[Subsection: Primer: Probability Distributions]{\ulink{primer}} to this topic, probability distributions are generally divided into two classes based on data, i.e., either \emph{discrete or continuous}.
    \end{itemize}
    
    \item \ddd{Probability mass function (PMF)}: a function that gives the probability that a \emph{discrete} random variable  is exactly equal to some value. 
    \begin{itemize}
      \item The function \(p:\R \rightarrow [0,1]\) is defined formally as:
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      p(x_i)=P(X=x_i) \qquad -\infty<x<\infty
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \item The associated probability values must follow the \hyperref[Subsection: Probability Theory Axioms]{\ulink{Kolmogorov axioms}}, which means all possible values must be positive and sum up to 1, implying all other probabilities must be 0, i.e., 
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      p(x_i) > 0,\qquad
      \sum p(x_i)=1,\qquad
      p(x) = 0 \quad \forall x \,\neg ~x_i
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \item Thinking of probability as mass helps avoid mistakes since physical mass is conserved, as is total probability for all hypothetical outcomes of \(x\).
      \item Major associated distributions include the  \hyperref[Subsection: Primer: Probability Distributions]{\ulink{previously mentioned}} Bernoulli and Binomial distributions, but geometric distributions deserve a mention as well. 
      \item \link{https://en.wikipedia.org/wiki/Geometric_distribution}{Geometric distribution}: a description of the number of trials/failures needed to get to one/first success.
    \end{itemize}
    \item \ddd{Probability density function (PDF)}: a function that describes \emph{relative} probabilities for a set of \hyperref[Subsection: Independent and Mutually Exclusive Events]{\ulink{exclusive}} \emph{continuous} events, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    P(a\leq X \leq b) = \int_a^b f_X(x)dx
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item \ddd{Cumulative density function (CDF)}: the PDF can also be described as the cumulative sum of continuous probabilities up to a particular point, i.e., 
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    F_X(x) = \int_{-\infty}^x f_X(u)du \quad \text{or (in practice)}\quad
    C(x_a) = \sum_{i = 1}^{a} p(x_i)
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item Note: every CDF is non-decreasing and right-continuous 
      \item Note: the sum of CDF is \(> 1\).
    \end{itemize}
  \end{itemize}
\end{itemize}

\section{Sample Distributions}
\begin{itemize}
  \item[]
  
  \subsection{Random, Representative Sampling}
  \begin{itemize}
    \item 
  \end{itemize}

  \subsection{Monte Carlo Methods}
  \begin{itemize}
    \item 
  \end{itemize}
  
  \subsection{Sample Variability}
  \begin{itemize}
    \item 
  \end{itemize}
  
\end{itemize}

\section{Convergence of Random Variables}
\begin{itemize}
  \item[]
  
  \subsection{Law of Large Numbers}
  \begin{itemize}
    \item 
  \end{itemize}

  \subsection{Central Limit Theorem}
  \begin{itemize}
    \item 
  \end{itemize}
  
\end{itemize}
