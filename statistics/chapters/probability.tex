\chapter{Probability Theory}

\section{Probability Fundamentals}
\begin{itemize}
  \item \ddd{Probability}: a measure of the likelihood that an event will occur; used to quantify attitudes towards propositions whose truth are not certain.
    \begin{itemize}
      \item Quantitatively, probability is a number between 0 and 1, which is often expressed as a percentage.
    \end{itemize}
  \item \ddd{Probability theory}: the axiomatic formalization of probability; widely used in many fields of study from math to philosophy.
  \item \ddd{Probability space \((\Omega, \F, P)\)}: a formal construct consisting of three elements that provides a model for a random process.
    \begin{itemize}
      \item \ddd{Sample space \(\Omega\)}: the set of all possible outcomes.
      \item \ddd{Event space \(\F\)}: all sets of outcomes; all subsets of the sample space.
      \item \ddd{Probability function \(P(E \in \F)\)}: the assignment of a number between 0 and 1 that represents the probability of each event \(E\) in event space.
    \end{itemize}
  \item \ddd{Proportion}: the measure of certainty; a fraction of a whole or the relation between two varying quantities.
    \begin{itemize}
      \item Proportion \textit{could} involve random variables, so depending on how the question is asked, then proportion could be the same as probability, but ultimately they are not interchangeable.
    \end{itemize}
  \item \ddd{Odds}: the ratio of the number of events that produce an outcome to the number of events that do not; essentially probability reframed in potentially more efficient way.

  \subsection{Probability Theory Axioms}
  \begin{itemize}
    \item \ddd{First axiom}: the probability of an event is a \emph{non-negative number real number}, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    P(E) \in \R,\quad P(E)\geq 0 \qqquad \forall E \in \F
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item \ddd{Second axiom}: the assumption of unit measure; the probability that \emph{at least one elementary event} in the entire sample space \emph{will occur} is 1, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    P(\Omega) = 1
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item \ddd{Third axiom}: the assumption of \(\sigma \)-additivity, wherein any \emph{countable} sequence of \hyperref[Subsection: Independent and Mutually Exclusive Events]{\dlink{disjoint sets}} \(E_1,E_2,\ldots\) satisfies
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    P\left(\bigcup_{i=1}^{\infty}E_i\right) = \sum_{i = 1}^{\infty}P(E_i)
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item Thus, only \hyperref[Subsection: Data Types]{\ulink{discrete data}} are valid for probability; continuous data must be converted to discrete forms in order to be valid.
    \end{itemize}
  \end{itemize}

  \subsection{Independent and Mutually Exclusive Events}
  \begin{itemize}
    \item \ddd{Stochastically independent}: when an event does not affect the probability of another, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    P(A~\ttt{\text{and}}~B)=P(A\cap B)=P(A)P(B)
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item Two random variables are independent if the realization of one does not affect the probability distribution of the other.
      \item \ddd{Pairwise independent (weak notion)}: two specific events in a collection that are independent of each other.
      \item \ddd{Mutually independent (strong notion)}: when each event is independent of any combination of other events in the collection.
      \item Often the stronger notion is simply termed independence, as it implies the weaker version, but not the other way around.
    \end{itemize}
    \item \ddd{Mutually exclusive (disjoint)}: two events that cannot occur at the same time, i.e., 
    \begin{itemize}
      \item Probability of \ttt{both}:
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      P(A~\text{\ttt{and}}~B) = P(A\cap B)= \minor{0} 
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \item Probability of \fff{either}: \(\hspace{182pt}\minor{\downarrow}\)
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      P(A~\fff{\text{or}}~B)=P(A\cup B)=  P(A)+P(B) - \minor{P(A\cap B)} = P(A)+P(B) 
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \end{itemize}
    \item \ddd{Collectively exhaustive (jointly)}: when at least one event must occur while exhausting all other possibilities at a given time, or that their union must coverall the events within the entire sample space, i.e., 
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    A\cup B = \Omega
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    
  \end{itemize}
  
  \subsection{Primer: Conditional Probability}
  \begin{itemize}
    \item \ddd{Conditional probability}: the probability of some event \(A\), given \(|\) the occurrence of some other event \(B\), i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    P(A~|\iipt B)=\frac{P(A\cap B)}{P(B)}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item Note, \(P(A\iipt | \ipt B)\) typically differs from \(P(B \iipt | \ipt A)\), falsely equating the two often results in errors, termed the base rate fallacy.
    \end{itemize}
    \item \ddd{Bayes' theorem}: probability of an event based on prior knowledge of conditions that might be related to the event; inference using conditional probability, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    P(A~|\iipt B)=\frac{P(B\,|\iipt A)P(A)}{P(B)}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item More on Bayesian statistics may or not be explored in greater depth in this course.
  \end{itemize}
  
  
  \subsection{Probability Functions}
  \begin{itemize}
    \item Again, a \hyperref[Section: Probability Fundamentals]{\ulink{probability function}} is the assignment of a number of \hyperref[Section: Probability Fundamentals]{\ulink{probability space}} (sometimes denoted \(X,\mathcal{A},P\), respectively). 
    \item \ddd{Probability distributions}: the product of  variations of the probability function based on given event space and properties of data types.
    \begin{itemize}
      \item As mentioned in the \hyperref[Subsection: Primer: Probability Distributions]{\ulink{primer}} to this topic, probability distributions are generally divided into two classes based on data, i.e., either \emph{discrete or continuous}.
    \end{itemize}
    
    \item \ddd{Probability mass function (PMF)}: a function that gives the probability that a \emph{discrete} random variable  is exactly equal to some value. 
    \begin{itemize}
      \item The function \(p:\R \rightarrow [0,1]\) is defined formally as:
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      p(x_i)=P(X=x_i) \qquad -\infty<x<\infty
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \item The associated probability values must follow the \hyperref[Subsection: Probability Theory Axioms]{\ulink{Kolmogorov axioms}}, which means all possible values must be positive and sum up to 1, implying all other probabilities must be 0, i.e., 
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      p(x_i) > 0,\qquad
      \sum p(x_i)=1,\qquad
      p(x) = 0 \quad \forall x \,\neg ~x_i
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \item Thinking of probability as mass helps avoid mistakes since physical mass is conserved, as is total probability for all hypothetical outcomes of \(x\).
      \item Major associated distributions include the  \hyperref[Subsection: Primer: Probability Distributions]{\ulink{previously mentioned}} Bernoulli and Binomial distributions, but geometric distributions deserve a mention as well. 
      \item \link{https://en.wikipedia.org/wiki/Geometric_distribution}{Geometric distribution}: a description of the number of trials/failures needed to get to one/first success.
    \end{itemize}
    \item \ddd{Probability density function (PDF)}: a function that describes \emph{relative} probabilities for a set of \hyperref[Subsection: Independent and Mutually Exclusive Events]{\ulink{exclusive}} \emph{continuous} events, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    P(a\leq X \leq b) = \int_a^b f_X(x)dx
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item \ddd{Cumulative density function (CDF)}: the PDF can also be described as the cumulative sum of continuous probabilities up to a particular point, i.e., 
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    F_X(x) = \int_{-\infty}^x f_X(u)du \quad \text{or (in practice)}\quad
    C(x_a) = \sum_{i = 1}^{a} p(x_i)
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item Note: every CDF is non-decreasing and right-continuous 
      \item Note: the sum of CDF is \(> 1\).
    \end{itemize}
  \end{itemize}
\end{itemize}

\section{Sampling}
\begin{itemize}
  \item \ddd{Sampling distribution}: the probability distribution of a given random variable when derived from a random sample size \(n\).
    \begin{itemize}
      \item Useful to be considered as the statistic for all possible samples from the same population of a given sample size; is dependent on the underlying distribution of the population.
    \end{itemize}
  \item Sampling a subset is often an easier and faster way to estimate an entire population, providing a potentially major simplification to statistical inference.
  \item \ddd{Expected (mean) value \(\mu, \bar{X}\)}: the expected mean of the \hyperref[Section: Data Fundamentals]{\ulink{population}}, or in case of random sampling, the expected mean of numerous samples, i.e.,
  \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \bar{X} = \sum_{i = 1}^{k}x_i p_i
  \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
    \item Where \(X\) is a random variable with a finite number outcomes \(x_i\) occurring with respective probabilities \(p_i\).
    \item Thus, the expected value is the weighted sum, with the probabilities as weights.
  \end{itemize}
  \item \ddd{Sampling variability}: different samples from the same population can have different values of the same measurement.
  \begin{itemize}
      \item A single measurement may be an unreliable estimate of a population parameter.
      \item Potential to randomly select outliers are the main source of sampling variability, but cannot be avoided.
      \item Thus, natural variation, measurement noise, and failing to under complexity of phenomena are all sources of variability.
  \end{itemize}
  \item \ddd{Sampling frame}: the source material, data, or device from which a sample of drawn; ideal frame qualities include:
  \begin{itemize}
      \item The units have logical, numerical identifiers.
      \item The units can be found again, or resampled.
      \item The frame is organized, systematically.
      \item The frame has additional information about the units and for the potential use of more advanced sampling frames.
      \item Every element of the population of interest is present.
      \item Every element of the population is only present once.
      \item No elements outside the population of interest are present.
      \item The data is keep up to date, accepting new information.
  \end{itemize}
  
  \subsection{Sampling Methods}
  \begin{itemize}
    \item \ddd{Probability sample}: a sample wherein every unit in the population has a chance \((P > 0)\) of being selected in the sample, and the probability can be accurately determined.
    \item There are numerous methods of sampling, not all of which will be covered, but the various ways have the following two things in common: 
      \begin{itemize}
        \item Every element has a known \emph{nonzero probability} of being sampled.
        \item Involves \emph{random selection} at some point.
      \end{itemize}
    \item Factors that contribute to choice between methods:
      \begin{itemize}
        \item Nature, quality, and availability of auxiliary information of the data.
        \item Accuracy requirements, and need to measure accuracy.
        \item Degree of expected analysis, cost, and operational concerns.
      \end{itemize}
    \item \ddd{Simple random sampling}: all subsets of a sampling frame have an equal probability of being selected.
      \begin{itemize}
        \item Minimizes bias, simplifies analysis.
        \item Variance between individual results within the sample is a good indicator of variance of overall population, leading to easy estimations of accuracy.
        \item Subject to sampling error, and implicit bias can go unnoticed due to data collection methods.
      \end{itemize}
    \item \ddd{Systematic (interval) sampling}: method of arranging the study population according to an ordering scheme, then selecting the starting element randomly and progressing at a specified interval.
      \begin{itemize}
        \item Useful if the arrangement of the data correlated with the variable of interest.
        \item Some arrangements can introduce periodic biases, potentially leading to samples unrepresentative of the overall population.
        \item Can be hard to quantify the accuracy, even if it can be more accurate and efficient than simple random sampling.
      \end{itemize}
    \item \ddd{Stratified sampling}: organization of data into discrete categories, or ``strata'' where each stratum is treated like an independent population and randomly sampled from.
      \begin{itemize}
        \item Helps avoid errors due to methods of data collection that may lead to subpopulations being overrepresented, causing to inaccurate generalizations if combined into one population. 
        \item Can be expensive, hard to select for relevant stratification variables, and is not useful when no homogenous subgroups.
      \end{itemize}
     \item Other (less common?) methods of sampling include: probability proportional to size sampling, cluster sampling, multistage sampling, quota sampling, voluntary sampling, snowball sampling, accidental sampling, and panel sampling.
     \item \ddd{Monte Carlo methods}: a broad class of computational algorithms that rely on repeated random sampling.
      \begin{itemize}
        \item Relies on properties of randomness to solves difficult problems that are deterministic in principle but not necessarily in practice.
        \item Used in optimization functions, numerical integration, and generation draws from a probability distribution. 
        \item Might be covered later, and probably will be included under Bayesian statistics, if that is covered in-depth.
      \end{itemize}
  \end{itemize}
  
  \subsection{Law of Large Numbers and Central Limit Theorem}
  \begin{itemize}
    \item \ddd{Law of large numbers (LLN)}: describes the result of performing the same experiment many times, wherein the \hyperref[Subsection: Measures of Central Tendency]{\ulink{average}} approaches the \hyperref[Section: Sampling]{\ulink{expected value}}.
      \begin{itemize}
        \item There is a weak strong law, that essentially state the same thing, but with slight difference; the strong law contains a more elaborate, but not covered proof.
      \end{itemize}
    \item \ddd{Weak law of large numbers}: with a sufficiently large sample size, then for any nonzero margin specified there will be a high probability that the average observations fall within the margin, i.e.,
    \[%%%%%%%%%%%%%#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \lim_{n\to \infty} P\left(|\bar{x}_n-\bar{X}| > \varepsilon\right) = 0 \qqquad \varepsilon = {x\in\R~|~x > 0}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item \ddd{Strong law of large numbers}: as \(n \to \infty\), then the probability that the average converges to the expected value is 1, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    P\left(\lim_{n\to\infty}\bar{x}_n=\bar{X}\right)=1
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item Essentially, this implies that the mean of sample means is often a useful estimate of a population mean. This helps lead to the central limit theorem, which is a critical bridge between classical and modern probability theory. 
    \item \ddd{Central limit theorem (CLT)}: when independence random variables are added, then their properly normalized sum tends to converge towards a normal distribution even if the original variables are not normally distributed.
      \begin{itemize}
        \item \ddd{``All roads lead to Gauss:''} another way of stating the CLT, i.e., the distribution of sample means approaches a Gaussian distribution, regardless of the shape of the population distribution.
      \end{itemize}
    \end{itemize}
\end{itemize}
