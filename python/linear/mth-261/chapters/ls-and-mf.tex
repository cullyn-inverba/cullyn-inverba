\chapter{Least-Squares and Model-Fitting}\label{Least-Squares and Model-Fitting}

\section{Model-Fitting}\label{Outline of Model-Fitting}
\begin{itemize}
  \item \jjj{Model fitting}: the combination of fixed features and free parameters in such a way that fits experimental data to a mathematical model based on adjustments to free parameters. 
    \begin{itemize}
      \item \jjj{Fixed features}: components on the imposed on the model based on previous knowledge, understanding, theories, hypotheses, or other evidence. 
      \item \jjj{Free parameters}: variables that cannot be predicted precisely or constrained by the model; they must be adjusted or estimated, which is the main goal of model fitting.
    \end{itemize}
  \item \jjj{Model interpolation}: the prediction of other experimental results given prior experimental data and a fitted model based on those data.
  
  \subsection{Five Steps to Model-Fitting}\label{Five Steps to Model-Fitting}
  \begin{itemize}
    \item[1.] \emph{Define the equation(s) underlying the model.} % chktex 36
      \begin{itemize}
        \item This step has less to do with linear algebra and more to do with data availability and theories around the fixed features a system. 
        \item For example, height is governed by a complex interaction between genetics and the environment, but a simplistic model can be made to estimate the importance of important features; 
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        h = \beta_1 s + \beta_2 p + \beta_3 n 
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \(h\) = height, \(s\) = sex, \(p\) = parents' height, \(n\) = nutrition, and \(\beta \) = free parameters.
      \end{itemize}
    \item[2.] \emph{Map the data to the model equations.}
      \begin{itemize}
        \item This step is where one takes real, or simulated, data and maps them to each of the fixed features. 
        \item The end result produces a system of equations with a series of unknown parameters. 
        \item One other component---the \jjj{intercept, \({\epsilon} \)}---is often included to capture the expected value of the data when all the predictors are zero and account for global offsets. 
      \end{itemize}
    \item[3.] \emph{Convert the equations into a matrix-vector equation.}
      \begin{itemize}
        \item This process was previously covered when describing \hyperref[Elementary Operations]{\ulink{elementary operations}} and involves converting the system of equations into a form of matrix-vector multiplication, i.e., \(\bm{Ax}=\bm{b}\)
      \end{itemize}
    \item[4.] \emph{Computer the parameters.}
      \begin{itemize}
        \item There are numerous ways to compute the parameters (the focus of this chapter), but in general this step involves the actual fitting of the model via estimation of the parameters.
      \end{itemize}
    \item[5.] \emph{Statistical evaluation of the model.}
      \begin{itemize}
        \item This step uses inferential statistics, which is outside the scope of linear algebra, but is deeply connected. Thus, this step won't be discussed; most of the following information is to understand linear algebra's role in the process.
        \item Statistics does have a different nomenclature, so it is useful to briefly cover the differences, i.e.,
        \begin{itemize}
          \item \(\bm{y} = \bm{X\beta} + {\epsilon} \): the general linear model, sometimes with no \({\epsilon} \).
          \item \(\bm{X}\): the design matrix, where columns = independent variables/predictors/regressors.
          \item \(\bm{\beta}\): the vector of regression coefficients/beta parameters.
          \item \(\bm{y}\): vector of the dependent variables/outcome measure.
          \item Essentially, \(\bm{X\beta} = \bm{y}\) is equivalent to \(\bm{Ax}=\bm{b}\)
        \end{itemize}
      \end{itemize}
  \end{itemize}
  
\end{itemize}

\section{Least-Squares}\label{Least-Squares}
\begin{itemize}
  \item \jjj{Least-squares}: a standard approach in regression analysis to approximate the solution of overdetermined systems (systems of questions with more equations than unknowns) by minimizing the sum of the squares of the residuals, i.e.,
  \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \| \epsilon \|^2 = \| \bm{X\beta - y}\|^2
  \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item \jjj{Residuals \(\epsilon\)}: the vector that describes the difference between the observed value \(\bm{y}\) and the estimated value \(\bm{X\beta}; \bm{\hat{y}}\) provided by a model, i.e.,
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \epsilon = \bm{\hat{y}} - \bm{y} = \bm{X\beta - y}
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      Here, \(\epsilon\) is the equivalent to \(\bm{z}\)---the orthogonal sum---described in \hyperref[Finding Projections]{\ulink{projections}}.
      \item The goal is to minimize this difference, and the magnitude of such difference is heavily used in machine learning and statistics. 
      \item The only variables that can be changed in order to minimize the residuals are the \(\beta \) parameters, which can be done in multiple ways. Here we focus on tools and methods from linear algebra that allow us to perform the least-squares approach.
    \end{itemize}

  \subsection{Least-Squares via Left Inverse and Orthogonal Projections}\label{Least-Squares via Left Inverse}
  \begin{itemize}
    \item Since the design matrix \(\bm{X}\) is an overdetermined system, then the \hyperref[Inverse Basics]{\ulink{\bbb{left} inverse}} can be used to isolate the regression coefficients if \(\bm{X}\) has \hyperref[Maximum Rank]{\ulink{full column rank}}, i.e.,
    \begin{align*}
      \bm{Ax = b} &\to \bm{X\beta = y} \\
      \bbb{(\bm{X}^T \bm{X})^{-1} \bm{X}^T} \bm{X\beta} &= \bbb{(\bm{X}^T \bm{X})^{-1} \bm{X}^T} \bm{y} \\
      \bm{\beta} &= \bbb{(\bm{X}^T \bm{X})^{-1} \bm{X}^T} \bm{y}
    \end{align*}

    \item Finding \hyperref[Projections]{\ulink{orthogonal projections}} in arbitrary dimensions, \(\mathbb{R}^N\), yields the following: 
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bm{x} = \bbb{(\bm{A}^T \bm{A})^{-1} \bm{A}^T} \bm{b}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item Which is exactly the same as the solution to the regression coefficients,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bm{\beta} = \bbb{(\bm{X}^T \bm{X})^{-1} \bm{X}^T} \bm{y}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item Where the key takeaway here is that \(\bm{X\beta}\), after the application of least-squares, produces an estimated value, \(\bm{\hat{y}}\). However, we start with the observed values \(\bm{y}\), and this difference between \(\bm{\hat{y}}\) and \(\bm{y}\) is \(\epsilon\). 
    \item This gives us the general form, since most models do not perfectly describe the observed data.
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bm{X\beta} = \bm{y} + \epsilon
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item Taking the squared magnitude values allows for an efficient regression (due to methods in calculus) towards to minimal values acceptable in \(\beta \), yielding the original definition:
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \| \epsilon \|^2 = \| \bm{X\beta - y}\|^2
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    
  \end{itemize}
  
\end{itemize}
