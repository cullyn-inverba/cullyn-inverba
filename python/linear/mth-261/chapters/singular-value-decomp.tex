\chapter{Singular Value Decomposition}\label{Singular Value Decomposition}

\section{Singular Value Decomposition Fundamentals}\label{Singular Value Decomposition Fundamentals}
\begin{itemize}
  \item \dd{Singular value decomposition (SVD)}: an extension of \hyperref[Eigendecomposition]{\ulink{eigendecomposition}} to any \(\xxx{m} \times \yyy{n} \) matrix via polar decomposition.
    \begin{itemize}
      \item \dd{Polar decomposition}: the factorization of a matrix \tbm{A} in the form \(\bm{A}=\bm{U}{\bm
      {P}}\), where \tbm{U} is a unitary matrix and \(\bm{{P}}\) is a positive-semidefinite \hyperref[Diagonal and Trace]{\ulink{Hermitian matrix}}, both square and the same size.
        \begin{itemize}
          \item \dd{Unitary}: when a complex square matrix has a \hyperref[Conjugate Transpose]{\ulink{conjugate transpose}} \(\bm{A}^* \) that is also it's inverse, i.e.,
          \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
          \bm{U}^*\bm{U} = \bm{U}\bm{U}^* = \bm{I}
          \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
          \item The real analogue of a unitary matrix is an \hyperref[Orthogonal Matrices]{\ulink{orthogonal matrix}}.
        \end{itemize}
      \item The rest of the details regarding polar decomposition will not be covered, as it detracts from understanding of single value decomposition; for now we will only deal with real matrices.
      \item Other important notes before continuing:
      \begin{itemize}
        \item Recall how \hyperref[Hadamard Multiplication]{\ulink{creating symmetric matrices}} is done, i.e.,
        \begin{itemize}
          \item \(\bm{\bbb{A^T}A}=\bm{\yyy{S_{n\times n}}}\) \quad and \quad \(\bm{A\rrr{A^T}} = \xxx{\bm{S}_{m \times m}}\)
        \end{itemize}
        \item And that the \hyperref[Column Space]{\ulink{column and row space}} of matrices multiplied with their transpose are the same the original matrix column/row space, i.e., 
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \yyy{C}(\bm{A\rrr{A^T}}) = \yyy{C}(\bm{\xxx{S}}) = \yyy{C}(\bm{A}), \quad \xxx{R}(\bm{\bbb{A^T} A}) = \xxx{R}(\yyy{\bm{S}}) = \xxx{R}(\bm{A})
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \end{itemize}
      \item Finally, the singular value decomposition in more detail:
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \underbrace{\bm{A}}_{\xxx{m} \times \yyy{n}} = \bbb{\underbrace{\U}_{\xxx{m} \times \xxx{m}}} \underbrace{\S}_{\xxx{m} \times \yyy{n}} \rrr{\underbrace{\Vt}_{\yyy{n} \times \yyy{n}}}
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \begin{itemize}
        \item \(\U\)~--- the orthogonal basis for \yyy{column space} of \bm{A}.
          \begin{itemize}
            \item Termed the \bbb{\textbf{left singular vectors}} as \(\bbb{\bm{U}^T}\bm{A = \S \Vt}\)
            \item In the form of eigenvalue equation \(\bbb{\bm{u}^T}\bm{A}=\ttt{\sigma} \xxx{\bm{v}^T}\).
          \end{itemize}
        \item \(\S\)~--- a diagonal matrix consisting of the \ttt{\textbf{singular values} \(\bm{\sigma}\)} of \bm{A}. 
          \begin{itemize}
            \item The number of non-zero singular values is equal to the \hyperref[Matrix Rank]{\ulink{rank}} of \tbm{A}.
          \end{itemize}
        \item \(\Vt\)~--- the orthogonal basis for \xxx{row space} of \bm{A}.
          \begin{itemize}
            \item Termed the \rrr{\textbf{right singular vectors}} as \(\bm{A}\rrr{\bm{V}} = \U \S\)
            \item In the form of eigenvalue equation \(\bm{A}\rrr{\bm{v}}= \yyy{\bm{u}} \ttt{\sigma}\)
          \end{itemize}
      \end{itemize}
    \end{itemize}
  
  \newpage
  \subsection{Computing SVD}\label{Computing SVD}
  \begin{itemize}
    \item Assuming \tbm{A} is an \(\xxx{m} \times \yyy{n} \) matrix, then multiplying by the transpose yields two results that are the similar to eigendecomposition and dependent on the order of multiplication:
    \begin{align*}
      \bm{A} &= \U \S \Vt &&& \bm{A} &= \U \S \Vt \\
      \bm{\bbb{A^T}A} &= (\U\S\Vt)^T \U\S\Vt &&& \bm{A\rrr{A^T}} &=  \U\S\Vt (\U\S\Vt)^T \\
      \bm{\yyy{S}} &= \Vt^T\S^T\yyy{\bm{U}^T} \U\S\Vt &&& \bm{\xxx{S}} &=  \U\S\Vt \Vt^T\S^T\yyy{\bm{U}^T} \\
      \bm{\yyy{S}} &= \bm{\xxx{V}}\S^T\bm{I}\S\Vt &&& \bm{\xxx{S}} &=  \U\S\bm{I}\S^T\yyy{\bm{U}^T} \\
      \bm{\yyy{S}} &= \bm{\xxx{V}}\S^2\Vt &&& \bm{\xxx{S}} &=  \U\S^2\yyy{\bm{U}^T} \\
      &\downarrow &&& &\downarrow \\
      \bm{\yyy{S}} &= \str{\bm{Q}}\eigL \str{\bm{Q}^T} &&& \bm{\xxx{S}} &= \str{\bm{Q}}\eigL \str{\bm{Q}^T} \\
      \xxx{R}(\bm{\yyy{S}}) &= \str{\bm{Q}}\eigL \str{\bm{Q}^T} &&& \yyy{C}(\bm{\xxx{S}}) &= \str{\bm{Q}}\eigL \str{\bm{Q}^T} \\
      \xxx{R}(\bm{A}) &= \str{\bm{Q}}\eigL \str{\bm{Q}^T} &&& \yyy{C}(\bm{A}) &= \str{\bm{Q}}\eigL \str{\bm{Q}^T} \\
      &\downarrow &&& &\downarrow \\ 
      \xxx{R}(\bm{A}) &= \bm{\xxx{V}}\S^2\Vt &&& \yyy{C}(\bm{A}) &=  \U\S^2\yyy{\bm{U}^T} 
    \end{align*}
    \item Thus, in the case of a real square matrix, \(\S^2 = \eigL\) when \tbm{A} is transposed with itself and \(\Vt\)/~\(\U\) act as the orthogonal basis for the \xxx{row}/\yyy{column} spaces, respectively.
    \item Reviewing the singular value equation:
    \begin{align*}
      \bm{A} = &~\U \S \Vt \\ 
      \swarrow &\hspace{14pt} \searrow \\
      \bbb{\bm{U^T}}\bm{A} = \S \Vt \hspace{20pt}&\hspace{35pt}\bm{A}\rrr{\bm{V}} = \U \S  \\
      \xxx{\bm{u}^T}\bm{A}=\ttt{\sigma} \xxx{\bm{v}^T} \hspace{24pt}&\hspace{37pt} \bm{A}\yyy{\bm{v}}= \yyy{\bm{u}} \ttt{\sigma}  
    \end{align*}
    \item I.e.,  the \bbb{left singular matrix \(\bm{U}^T\)} is a matrix whose \xxx{rows} describe a transformation of \(\bm{A} \to\) an \xxx{orthogonal basis \(\bm{V^T}\)} wherein each column scaled by its respective \ttt{singular value \(\sigma \)} describes the \xxx{row space} of \tbm{A}.
    \item Likewise, the \rrr{right singular matrix \(\bm{V}\)} is a matrix whose \yyy{columns} describe a transformation of \(\bm{A} \to\) an \yyy{orthogonal basis \(\bm{U}\)} wherein each column scaled by its respective \ttt{singular value \(\sigma \)} describes the \yyy{column space} of \tbm{A}.   
  \end{itemize}

  \subsection{Singular Values vs. Eigenvalues}\label{Singular Values vs. Eigenvalues}
  \begin{itemize}
    \item \(\operatorname{eig}(\bm{A}^T \bm{A}) = \operatorname{svd}(\bm{A})^2 \)
    \item \(\operatorname{eig}(\bm{A}^T \bm{A}) = \operatorname{svd}(\bm{A}^T \bm{A}) \), as \(\operatorname{svd}(\bm{A}^T \bm{A})\) essentially squares the singular values.
    \item \(\operatorname{eig}(\bm{A})~\text{and}~\operatorname{svd}(\bm{A}) \) are usually not related at all, unless \tbm{A} consists all real entries.
    \begin{itemize}
      \item Singular values are always real, whereas eigenvalues are likely to be complex.
    \end{itemize}
  \end{itemize}
  

  \subsection{Relation to Matrix Subspaces}\label{SVD Relation to Subspaces}
  \begin{itemize}
    \item Since the SVD provides orthogonal basis for the row and column spaces of a matrix, then it also tells provides the \hyperref[Null Space]{\ulink{kernel and cokernel}} of the matrix.
    \item Again, revisiting the singular value equation and taking note of dimensions of each of the matrices yields plenty of useful information:
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \underbrace{\bm{A}}_{\xxx{m} \times \yyy{n}} = \underbrace{\U}_{\xxx{m} \times \xxx{m}} \underbrace{\S}_{\xxx{m} \times \yyy{n}} \underbrace{\Vt}_{\yyy{n} \times \yyy{n}}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item Again, the \emph{rank \(r\)} is equal to the sum of singular values greater than 0, i.e.,
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \rank{\bm{A}=\Sigma~(\s > 0)}
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \item Where the orthogonal basis \U~spanning all of \(R^{\xxx{m}}\) with:
        \begin{itemize}
          \item a \yyy{column space} of \yyy{\(1:\emph{r}\)}
          \item and a \xxx{cokernel} of \xxx{\(\emph{r}+1:\xxx{m}\)}
        \end{itemize}
      \item Likewise, the orthogonal basis \(\Vt\) spanning all of \(R^\yyy{n}\) has:
      \begin{itemize}
        \item a \xxx{row space} of \xxx{\(1:\emph{r}\)}
        \item and a \yyy{kernel} of \yyy{\(\emph{r}+1:n\)}
      \end{itemize}
    \end{itemize}
  \end{itemize}
\end{itemize}

\newpage 

\section{Applications of SVD}\label{Applications of SVD}
\begin{itemize}
  \item []
  
  \subsection{Low-Rank Approximations}\label{Low Rank Approximations}
  \begin{itemize}
    \item \dd{Low-rank approximation}: a minimization problem, in which the cost function measures the fit between a given matrix (the data) and the approximation matrix (optimization variable), subject to a constraint that the approximation matrix has \hyperref[Matrix Rank]{\ulink{reduced rank}}.
    \item The goal is to \emph{reduce data} used in the system, while \emph{retaining as much information} as necessary; the balance between the trade-off is often application dependent.
    \item This is done by taking a subset of matrices within the SVD, up to range \(\rrr{k}\), which can be determined with \hyperref[Percent Variance]{\dlink{percent variance}}.
    \begin{itemize}
      \item The relative importance of singular values and corresponding basis vectors is normally sorted in the output of the algorithm, so typically values beyond \(\rrr{k}\) are noise and offer little data, leading to minimization opportunities.
    \end{itemize} 
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \underbrace{\bm{A}}_{\xxx{m} \times \yyy{n}} = \underbrace{\U}_{\xxx{m} \times \rrr{k}} \underbrace{\S}_{\rrr{k} \times \rrr{k}} \underbrace{\Vt}_{\rrr{k} \times \yyy{n}}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    
  \end{itemize}

  \subsection{Percent Variance}\label{Percent Variance}
  \begin{itemize}
    \item Comparing or interpreting singular values at directly is often a challenge, and for many applications one might want to know the relative importance of each singular value, which is done through analysis of the variance of the singular values.
    \item Singular values are scale-dependent, i.e., any scaling \((\amp{\Phi})\) of the matrix \tbm{A} will scale the singular values by the same amount:
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \amp{\Phi}\bm{A} = \U(\amp{\Phi}\S)\Vt
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item Any matrix can be formed by combining rank 1 matrices, and in context of SVD, those matrices are derived from the \hyperref[Outer Product]{\ulink{outer product}} of columns of the \(\U\) and corresponding row of \(\Vt\) (provides direction) and scaled by the corresponding singular value (provides magnitude), i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bm{A} = 
    \yyy{\bm{u_1}}\,\ttt{\bm{\sigma_1}}\,\xxx{\bm{v_1}^T} +
    \yyy{\bm{u_2}}\,\ttt{\bm{\sigma_2}}\,\xxx{\bm{v_2}^T} +
    \cdots +
    \yyy{\bm{u_n}}\,\ttt{\bm{\sigma_n}}\,\xxx{\bm{v_n}^T}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item The outer product of \(\yyy{\bm{u_n}}\xxx{\bm{v_n}^T}\) involves singular vectors from the orthogonal bases, meaning they have unit length \(|1|\) and only encodes the direction.  
      \item The corresponding singular value encodes the importance, or magnitude, of the direction given by the vectors.
      \item The sum of the singular values tells you the total importance (variance) of all the directions in the matrix.
      \item Thus, the proportion of each singular value of total variance can easily be found, yielding the percent variance:
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \ttt{\bm{\sigma_i}} = 100\:\frac{\ttt{\bm{\sigma_i}}}{\Sigma\,\ttt{\bm{\sigma}}}
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \item This normalization allows for easy identification of the complexity of the matrix, and where data compression (i.e., range \(\rrr{k}\) of relevant values) might be beneficial.
    \end{itemize}
  \end{itemize}
  
  \subsection{Pseudoinverse}\label{Pseudoinverse}
  \begin{itemize}
    \item \dd{Pseudoinverse\(^{\bm{\dagger}}\) (Moore-Penrose inverse)}: a widely known generalization of the inverse matrix, used for computing the inverse of \hyperref[Inverse Basics]{\ulink{singular matrices}}.
      \begin{itemize}
        \item There are multiple generalizations of the inverse, or different definitions of the pseudoinverse, as well as ways to compute each. However, the Moore-Penrose is the most common, and SVD is generally used to compute the inverse.
      \end{itemize}
    \item Finding the regular inverse of a matrix using SVD is useful to understand how the pseudoinverse is computed:
    \begin{align*}
      \bm{A} &= \U\S\Vt \\
      \bm{A}^{-1} &= (\U\S\Vt)^{-1} \\
      \bm{A}^{-1} &= \bm{\xxx{V}}^{-T}\S^{-1}\U^{-1} \\
      \bm{A}^{-1} &= \bm{\xxx{V}}\S^{-1}\U^T
    \end{align*}
    \item \(\S ^{-1}\) is found simply by taking the inverse of each element along the diagonal; this clearly shows why singular matrices cannot be used, as \(0 ^{-1}\) is undefined.
    \item Thus, a pseudoinverse\(^\dagger\) is needed, i.e.,
    \begin{align*}
      \bm{A} &= \U\S\Vt \\
      \bm{A}^{\dagger} &= (\U\S\Vt)^{\dagger} \\
      \bm{A}^{\dagger} &= \bm{\xxx{V}}^{-T}\S^{\dagger}\U^{-1} \\
      \bm{A}^{\dagger} &= \bm{\xxx{V}}\S^{\dagger}\U^T
    \end{align*}
    \item The difference is that \(\S^\dagger\) only inverts the non-zero singular values.
  \end{itemize}

  \subsection{Condition Number}\label{Condition Number}
  \begin{itemize}
    \item \dd{Condition number \((\kappa)\)}: the measure of how much the output of a function changes based in the input, i.e., it measures how \emph{sensitive} a function is to changes or errors in the input.
      \begin{itemize}
        \item In regard to SVD, the condition number defines the spread of the information in a matrix by comparing the \emph{ratio of the maximum singular value to the smallest}, i.e.,
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \frac{\ttt{\bm{\sigma_{\rrr{\max}}}}}{\ttt{\bm{\sigma_{\bbb{\min}}}}}
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \end{itemize}
    \item The condition number for any singular matrix is undefined, as the minimum singular value would be zero. Additionally, the closer minimum value gets to zero, the less conditioned a matrix is (larger spread), i.e.,
    \begin{itemize}
      \item \bbb{Small} min value \to~\rrr{large} spread \to~\rrr{large} condition number \to~\textbf{\rrr{ill}-conditioned}.
      \item \rrr{Large} min value \to~\bbb{small} spread \to~\bbb{small} condition number \to~\textbf{\bbb{well}-conditioned}.
    \end{itemize}
    \item There is no specified threshold between ill- and well-conditioned matrix---it is often dependent on the dataset.
    \item Having an ill-condition matrix isn't necessarily bad, instead, the condition number is often used as an indicator of large-scale structure within the matrix.
  \end{itemize}
  
\end{itemize}



