% chktex-file 3
\chapter{Matrix Multiplication}\label{Matrix Multiplication}
\section{Standard Matrix Multiplication}\label{Standard Matrix Multiplication}
\begin{itemize}
  \item \jjj{Standard matrix multiplication \(\bm{AB}\)}: a binary operation that produces a matrix from two matrices whose \emph{inner dimensions match}.
    \begin{itemize}
      \item Multiplication of matrices can be thought of as going from right to left (\(\bm{A}\leftarrow\bm{B}\)), or rather, \bbb{\tbm{A} pre-multiplies \tbm{B}}, or \rrr{\tbm{B} post-multiplies \tbm{A}}.
      \item The number of \yyy{columns (\(n\)) in the first matrix} must be \ttt{equal to} the number of \\ \xxx{rows (\(m\)) in the second matrix}.
      \item \jjj{Matrix product}: the product, whose size is equal to \xxx{rows of the first matrix} and the number of \yyy{columns of the second matrix}.
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      (\xxx{m_1} \times \ttt{n_1})(\ttt{m_2} \times \yyy{n_2}) = \xxx{m_1} \times \yyy{n_2}
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \end{itemize}
  \item \hyperref[Transposition]{\ulink{Transposing}} a matrix switches the dimensions, so it can enable computation in some cases, e.g,
  \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \underbrace{\bm{A}}_{\xxx{5}\times \yyy{7}} = \underbrace{\bm{A}^T}_{\xxx{7}\times \yyy{5}} 
  \qquad
  \underbrace{\bm{A}^T}_{\xxx{7}\times \ttt{5}} 
  \underbrace{\bm{B}}_{\ttt{5}\times \yyy{2}} = \underbrace{\bm{C}}_{\xxx{7}\times\yyy{2}} 
  \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \item Revisiting the \hyperref[The Dot Product]{\ulink{dot (inner) product}} and the \hyperref[Outer Product]{\ulink{outer product}} of vectors:
  \begin{itemize}
    \item The dot product must have equal-length vectors since the transpose of left vector makes inner dimensions much match, e.g.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \underbrace{\bm{v}}_{5\times1}\underbrace{\bm{w}}_{5\times1}
    \to
    \underbrace{\bm{v}^T}_{\xxx{1}\times \ttt{5}}\underbrace{\bm{w}}_{\ttt{5}\times\yyy{1}}
    = 1\times 1~\text{(scalar)}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item However, the transpose of the right vector makes the \(1 \times 1\) dimensions match, making the outer dimensions irrelevant to the validity of the computation, e.g., 
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \underbrace{\bm{v}}_{6\times1}\underbrace{\bm{w}}_{9\times1}
    \to
    \underbrace{\bm{v}}_{\xxx{6}\times \ttt{1}}\underbrace{\bm{w}^T}_{\ttt{1}\times\yyy{9}}
    = 6\times 9~\text{(matrix)}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \end{itemize}

  \subsection{Standard Matrix Multiplication Perspectives}\label{Standard Matrix Multiplication Perspectives}
  \begin{itemize}
    \item There are four ways to compute and conceptualize the process of multiplication of two matrices, all of which give the same result.
    \item I.e, if \tbm{A} is an \(\xxx{m} \times \ttt{n}\) matrix and \tbm{B} is an \(\ttt{n} \times \yyy{p}\) matrix, then their product \tbm{C} =
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \hspace{-25pt}
    {\begin{bmatrix}
      a_{11}b_{11} + \cdots + a_{1\ttt{n}}b_{\ttt{n}1}&a_{11}b_{12} + \cdots + a_{1\ttt{n}}b_{\ttt{n}2} &\cdots &a_{11}b_{1\yyy{p}} + \cdots + a_{1\ttt{n}}b_{\ttt{n}\yyy{p}}
      \\ a_{21}b_{11} + \cdots + a_{2\ttt{n}}b_{\ttt{n}1}&a_{21}b_{12} + \cdots + a_{2\ttt{n}}b_{\ttt{n}2}&\cdots &a_{21}b_{1\yyy{p}} + \cdots + a_{2\ttt{n}}b_{\ttt{n}\yyy{p}}
      \\ \vdots &\vdots &\ddots &\vdots 
      \\ a_{\xxx{m}1}b_{11} + \cdots + a_{\xxx{m}\ttt{n}}b_{\ttt{n}1}&a_{\xxx{m}1}b_{12} + \cdots + a_{\xxx{m}\ttt{n}}b_{\ttt{n}2}&\cdots &a_{\xxx{m}1}b_{1\yyy{p}} + \cdots + a_{\xxx{m}\ttt{n}}b_{\ttt{n}\yyy{p}}
    \end{bmatrix}}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item \jjj{The element perspective}: building the product matrix directly, one element at a time, via the computation of the \hyperref[The Dot Product]{\ulink{dot product}} between the \xxx{rows of the left matrix} and the \yyy{columns of the right matrix}.
    \begin{align*}
    \begin{bmatrix} \xxx{1} & \xxx{2} \\ 3 & 4 \end{bmatrix}
    \begin{bmatrix} \yyy{a} & b \\ \yyy{c} & d \end{bmatrix}
    &=
    \begin{bmatrix}
    \xxx{1}\yyy{a} + \xxx{2}\yyy{c} & \cdots \\ \cdots & \cdots \end{bmatrix}
    \\%
    \begin{bmatrix} \xxx{1} & \xxx{2} \\ 3 & 4 \end{bmatrix}
    \begin{bmatrix} a & \yyy{b} \\ c & \yyy{d} \end{bmatrix}
    &=
    \begin{bmatrix}
    1a + 2c & \xxx{1}\yyy{b}+\xxx{2}\yyy{d} \\ \cdots & \cdots 
    \end{bmatrix}
    \\%
    \begin{bmatrix} 1 & 2 \\ \xxx{3} & \xxx{4} \end{bmatrix}
    \begin{bmatrix}  \yyy{a} & b \\ \yyy{c} & d \end{bmatrix}
    &=
    \begin{bmatrix}
    1a + 2c & 1b+2d \\ \xxx{3}\yyy{a}+\xxx{4}\yyy{c} & \cdots 
    \end{bmatrix}
    \\%
    \begin{bmatrix} 1 & 2 \\ \xxx{3} & \xxx{4} \end{bmatrix}
    \begin{bmatrix} a & \yyy{b} \\ c & \yyy{d} \end{bmatrix}
    &=
    \begin{bmatrix}
    1a + 2c & 1b+2d \\ 3a+4c & \xxx{3}\yyy{b}+\xxx{4}\yyy{d} 
    \end{bmatrix}
    \end{align*}
    \item This is generally the most common perspective as the others methods have all have two ``pseudo steps'' to reach the final product, rather than through one direct method.
    \item \jjj{The layer perspective}: the building of the product matrix one layer at a time, followed by a ``flattening'' to make the final product.
      \begin{itemize}
        \item Each layer is the same size \((\xxx{m_1} \times \yyy{n_2})\) as the product, but is only a \hyperref[Matrix Rank]{\dlink{rank~1~matrix}}, or essentially the representation of only one column's worth of information.
        \item Can be thought of as a \yyy{left matrix of columns} and a \xxx{right matrix of rows}, resulting in the computation of the \hyperref[Outer Product]{\ulink{outer product}}.
      \end{itemize}
    \begin{align*}
    \begin{bmatrix} \yyy{1} & 2 \\ \yyy{3} & 4 \end{bmatrix}
    \begin{bmatrix} \xxx{a} & \xxx{b} \\ c & d \end{bmatrix}
    &=
    \begin{bmatrix}
    \yyy{1}\xxx{a} & \yyy{1}\xxx{b} \\
    \yyy{3}\xxx{a} & \yyy{3}\xxx{b} 
    \end{bmatrix}
    \\
    \begin{bmatrix} 1 & \yyy{2} \\ 3 & \yyy{4} \end{bmatrix}
    \begin{bmatrix} a & b \\ \xxx{c} & \xxx{d} \end{bmatrix}
    &=
    \begin{bmatrix}
    \yyy{2}\xxx{c} & \yyy{2}\xxx{d} \\
    \yyy{4}\xxx{c} & \yyy{4}\xxx{d} 
    \end{bmatrix}
    \\
    \begin{bmatrix}
      \yyy{1}\xxx{a} & \yyy{1}\xxx{b} \\
      \yyy{3}\xxx{a} & \yyy{3}\xxx{b} 
    \end{bmatrix}
    \Downarrow
    \begin{bmatrix}
      \yyy{2}\xxx{c} & \yyy{2}\xxx{d} \\
      \yyy{4}\xxx{c} & \yyy{4}\xxx{d} 
    \end{bmatrix}
    &=
    \begin{bmatrix}
      \yyy{1}\xxx{a} + \yyy{2}\xxx{c} & \yyy{1}\xxx{b}+\yyy{2}\xxx{d} \\ \yyy{3}\xxx{a}+\yyy{4}\xxx{c} & \yyy{3}\xxx{b}+\yyy{4}\xxx{d}  
    \end{bmatrix}
    \end{align*}
    \item \jjj{The column perspective}: the building of the product matrix by columns, where the first column is the sum of the two columns of the left matrix weighted (scaled) by the elements of the first column of the right matrix. 
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \begin{bmatrix} \bbb{1} & \chap{2} \\ \bbb{3} & \chap{4} \end{bmatrix}
      \begin{bmatrix} \rrr{a} & \emph{b} \\ \rrr{c} & \emph{d} \end{bmatrix}
      =
      \begin{pmatrix}
      \rrr{a} \begin{bmatrix} \bbb{1} \\ \bbb{3} \end{bmatrix} + \rrr{c} \begin{bmatrix} \chap{2} \\ \chap{4} \end{bmatrix} &
      \emph{b} \begin{bmatrix} \bbb{1} \\ \bbb{3} \end{bmatrix} + \emph{d} \begin{bmatrix} \chap{2} \\ \chap{4} \end{bmatrix}
      \end{pmatrix}
      =
      \begin{bmatrix}
        \bbb{1}\rrr{a} + \chap{2}\rrr{c} & \bbb{1}\emph{b}+\chap{2}\emph{d} \\ \bbb{3}\rrr{a}+  \chap{4}\rrr{c} & \bbb{3}\emph{b}+\chap{4}\emph{d} 
      \end{bmatrix}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item \jjj{The row perspective}: similar to the column perspective, but building up by row. 
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{bmatrix} \bbb{1} & \bbb{2} \\ \chap{3} & \chap{4} \end{bmatrix}
    \begin{bmatrix} \rrr{a} & \rrr{b} \\ \emph{c} & \emph{d} \end{bmatrix}
    =
    \begin{pmatrix}
    \bbb{1} \begin{bmatrix} \rrr{a} & \rrr{b} \end{bmatrix} + 
    \bbb{2} \begin{bmatrix} \emph{c} & \emph{d} \end{bmatrix} \\
    \chap{3} \begin{bmatrix} \rrr{a} & \rrr{b} \end{bmatrix} + 
    \chap{4} \begin{bmatrix} \emph{c} & \emph{d} \end{bmatrix}
    \end{pmatrix}
    =
    \begin{bmatrix}
      \bbb{1}\rrr{a} + \bbb{2}\emph{c} & \bbb{1}\rrr{b}+\bbb{2}\emph{d} \\ \chap{3}\rrr{a}+\chap{4}\emph{c} & \chap{3}\rrr{b}+\chap{4}\emph{d} 
    \end{bmatrix}
  \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \end{itemize}
\end{itemize}

\section{Properties of Matrix Multiplication}\label{Properties of Matrix Multiplication}
\begin{itemize}
  \item[] 
  \subsection{Diagonal Matrix Multiplication}\label{Diagonal Matrix Multiplication}
  \begin{itemize}
    \item Square \hyperref[Diagonal and Triagnular Matrices]{\ulink{diagonal matrices}} are often used to scale another matrix by the elements along such diagonal.
    \item You can scale the columns or rows depending on the placement of the diagonal matrix \(\bm{D}\) relative to original matrix \(\bm{A}\). 
      \begin{itemize}
        \item \bbb{Pre-multiplication} of the diagonal results in the scaling by \xxx{rows}, i.e., \(\xxx{\bm{D}}\bm{A}\):
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \begin{bmatrix}
        1 & 2 & 3 \\
        4 & 5 & 6 \\
        7 & 8 & 9 
        \end{bmatrix}
        \begin{bmatrix}
        \xxx{a} & 0 & 0 \\
        0 & \xxx{b} & 0 \\
        0 & 0 & \xxx{c}
        \end{bmatrix}
        =
        \begin{bmatrix}
          \xxx{a}1 & \xxx{a}2 & \xxx{a}3 \\
          \xxx{b}4 & \xxx{b}5 & \xxx{b}6 \\
          \xxx{c}7 & \xxx{c}8 & \xxx{c}9 
          \end{bmatrix}
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \item \rrr{Post-multiplication} of the diagonal results in the scaling by \yyy{columns}, i.e., \(\bm{A}\yyy{\bm{D}}\):
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \begin{bmatrix}
        1 & 2 & 3 \\
        4 & 5 & 6 \\
        7 & 8 & 9 
        \end{bmatrix}
        \begin{bmatrix}
        \yyy{a} & 0 & 0 \\
        0 & \yyy{b} & 0 \\
        0 & 0 & \yyy{c}
        \end{bmatrix}
        =
        \begin{bmatrix}
          1\yyy{a} & 2\yyy{b}  & 3\yyy{c} \\
          4\yyy{a} & 5\yyy{b} & 6\yyy{c} \\
          7\yyy{a} & 8\yyy{b} & 9\yyy{c} 
          \end{bmatrix}
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \end{itemize}
    \item Again, when all the elements along the diagonal are the same, then it is simply a scaled version of the \hyperref[Identity and Zero Matrices]{\ulink{identity matrix}}, or sometimes referred to as a \hyperref[Diagonal and Triagnular Matrices]{\ulink{scaled matrix}}.
  \end{itemize}

  \subsection{Order of Operations}\label{Order of Operations}
  \begin{itemize}
    \item \minor{``And love is evol, spell it backwards, I'll show ya''}
    \item \jjj{\(\bm{(L\,O\,VE)}^T = \bm{E}^T\bm{V}^T\bm{O}^T\bm{L}^T\)}: reversing the order of multiplication on a set of matrices is valid if the same operation (e.g., \hyperref[Transposition]{\ulink{transpose}} or \hyperref[Matrix Inverse]{\dlink{inverse}}) can be applied to each matrix, e.g, (\emph{diagonal} highlighted for easier time seeing transpose)
    \begin{align*}
    \begin{pmatrix}
      \begin{bmatrix} \emph{1} & 2 \\ 3 & \emph{4} \end{bmatrix}
      \begin{bmatrix} \emph{a} & b \\ c & \emph{d} \end{bmatrix}
    \end{pmatrix}^T
    &=
    \begin{bmatrix}
      \emph{1a+2c} & 1b+2d \\ 3a+4c & \emph{3b+4d} 
    \end{bmatrix}^T
    &= 
    \begin{bmatrix}
      \emph{1a+2c} & 3a+4c \\ 1b+2d & \emph{3b+4d} 
    \end{bmatrix}
    \\
    \begin{bmatrix} \emph{a} & b \\ c & \emph{d} \end{bmatrix}^T
    \begin{bmatrix} \emph{1} & 2 \\ 3 & \emph{4} \end{bmatrix}^T
    &=
    \begin{bmatrix} \emph{a} & c \\ b & \emph{d} \end{bmatrix}
    \begin{bmatrix} \emph{1} & 3 \\ 2 & \emph{4} \end{bmatrix}
    &=
    \begin{bmatrix}
      \emph{1a+2c} & 1b+2d \\ 3a+4c & \emph{3b+4d} 
    \end{bmatrix}
    \end{align*}
  \end{itemize}
  
  \subsection{Matrix Vector Multiplication}\label{Matrix Vector Multiplication}
  \begin{itemize}
    \item Multiplying a matrix by a vector always results in a vector, regardless order. 
    \item The order of multiplication does impact the orientation and size (dimensionality) of the product vector, i.e.,
    \\ \bbb{pre-multiplication} \to~\xxx{row vector}
    (weighted combinations of the rows of \tbm{A})
    \\ \rrr{post-multiplication} \to~\yyy{column vector} (weighted combinations of the columns of \tbm{A})
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \underbrace{\bbb{\bm{w}^T}}_{\xxx{1}\times\ttt{n}}\underbrace{\bm{A}}_{\ttt{m}\times\yyy{n}}=\xxx{\underbrace{\bm{v}}_{\xxx{1}\times\yyy{n}}}
    \qquad 
    \underbrace{\bm{A}}_{\xxx{m}\times\ttt{n}}\underbrace{\rrr{\bm{w}}}_{\ttt{m}\times\yyy{1}}=\yyy{\underbrace{\bm{v}}_{\xxx{m}\times\yyy{1}}}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item The order of multiplication does not matter when multiplying a vector with a \hyperref[Symmetric and Skew-Symmetric Matrices]{\ulink{symmetric matrix}}:
    \begin{align*}
      \ttt{\bm{S}^T} &= \ttt{\bm{S}} & \text{symmetric matrix}\\
      \bm{S}\bm{w} &= \yyy{\bm{v}} & \text{column vector}\\ 
      (\bm{S}\bm{w})^T &= \xxx{\bm{v}^T} & \text{apply transpose \rightarrow~row vector}\\ % chktex 3
      \bm{w}^T \bm{S}^T &= \bm{v}^T & \text{\hyperref[Order of Operations]{\ulink{love \leftrightarrow~evol}}}\\
      \bm{w}^T \ttt{\bm{S}} &= \bm{v}^T & \text{symmetric matrix \leftrightarrow~transpose}\\
      \xxx{\bm{v}^T} &= \yyy{\bm{v}} & \text{row vector \leftrightarrow~column vector}
    \end{align*}
    \item If \(\bm{S^T}\neq\bm{S}\), then order does matter, resulting in different product vectors.
    \item Multiplication of vectors and matrices form the basis of linear transformations.
    \item When multiplication between a matrix and vector is equal to the multiplication between a scalar and the same vector, then the scalar is the \hyperref[Eigendecomposition]{\dlink{eigenvalue}} and the vector is the \hyperref[Eigendecomposition]{\dlink{eigenvector}}. 
  \end{itemize}
  
  \subsection{Additive and Multiplicative Matrices}\label{Additive and Multiplicative Matrices}
  \begin{itemize}
    \item \jjj{Multiplicative identity matrix}: commonly referred to as the \hyperref[Identity and Zero Matrices]{\ulink{identity matrix}}, where both \bbb{pre} and \rrr{post} multiplication are equal and both produce the original matrix.
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bbb{\bm{I}} \bm{A} = \bm{A}\rrr{\bm{I}} = \bm{A}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item However, addition of the multiplicative identity matrix does not yield the same product, i.e.,
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \bm{A} + \bm{I} \neq \bm{A}
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \end{itemize}
  \item \jjj{Additive identity matrix}: the compliment to the multiplicative matrix that uses the zero matrix (matrix of all zeros), hence why it is commonly referred to as simply the zero matrix.
    \begin{itemize}
      \item Multiplication by the zero matrix of course does not yield the original matrix (unless the original matrix was a zero matrix), but the addition of zero will yield the original product, i.e.,
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \bm{A0} = \bm{0A}\neq A ,\qquad  \bm{A} + \bm{0} = \bm{A} 
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \end{itemize}
  \end{itemize}
  
  \subsection{Creating Symmetric Matrices}\label{Creating Symmetric Matrices}
  \begin{itemize}
    \item \ddd{Additive method}: using a square matrix \tbm{A} added to its own transpose will create a \hyperref[Symmetric and Skew-Symmetric Matrices]{\ulink{symmetric matrix}}, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bm{S} = \bm{A}+\bm{A}^T
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item Non-square matrices result in invalid \hyperref[Matrix Addition and Subtraction]{\ulink{matrix addition}} due to unequal dimensions.
      \item Optionally, a scaling factor of \(\frac{1}{2}\) can be used to make the symmetric matrix more closely resemble the original matrix since all values are doubled during addition.
    \end{itemize}
    
    \item \ddd{Multiplicative method}: multiplying a matrix \tbm{A} of any dimension with the transpose of itself to create a symmetric matrix \tbm{S}.
      \begin{itemize}
        \item The order of matrix multiplication matters if the matrix is a non-square matrix, i.e.,
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \underbrace{\bbb{\bm{A}^T}}_{\yyy{n} \times \ttt{m}} 
          \underbrace{\bm{A}}_{\ttt{m} \times \yyy{n}} = \underbrace{\bm{S}}_{\yyy{n \times n}}
          \qquad
          \underbrace{\bm{A}}_{\xxx{m} \times \ttt{n}} 
          \underbrace{\rrr{\bm{A}^T}}_{\ttt{n} \times \xxx{m}} = \underbrace{\bm{S}}_{\xxx{m \times m}}
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \item Note, again the order of multiplication does impact the dimensionality of the symmetric matrix produced, i.e.,\\
        \bbb{pre-multiplication} \to~\yyy{size \(n \times n\)} = number of \yyy{columns} in the original matrix
        \\
        \rrr{post-multiplication} \to~\xxx{size \(m \times m\)} = number of \xxx{rows} in the original matrix
      \end{itemize}
      \item Proving multiplicative matrices using \hyperref[Order of Operations]{\ulink{love \leftrightarrow~evol rule}}:
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \bm{A}^T \bm{A} = (\bm{A}^T \bm{A})^T = \bm{A}^T \bm{A}^{TT} = \bm{A}^T\bm{A}  % chktex 3
        \qquad
        \bm{A}\bm{A}^T = (\bm{A}\bm{A}^T)^T = \bm{A}^{TT}\bm{A}^T = \bm{A}\bm{A}^T % chktex 3
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \item Example of producing a symmetric matrix:
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \begin{bmatrix}
      a & b & c \\
      d & e & f \\
      \end{bmatrix}
      \begin{bmatrix}
      a & d \\
      b & e \\
      c & f   
      \end{bmatrix}
      =
      \begin{bmatrix}
      a^2+b^2+c^2 & ad+be+cf & \\
      ad+be+cf & d^2+e^2+f^2 &  \\
      \end{bmatrix}
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \item Notice that the diagonal of the symmetric matrix is the columns of the original (or rows of transposed matrix) squared while the off diagonal elements are the cross terms (or rows of the original and columns of transpose).
      \item The multiplicative method is widely used signal processing and statistics; the product is often called a covariance matrix, i.e., a matrix where the diagonal elements are the variances and the off diagonal elements are the covariances.
  \end{itemize}
  
  \subsection{Hadamard Multiplication}\label{Hadamard Multiplication}
  \begin{itemize}
    \item Covered briefly in while introducing other \hyperref[Properties of Vectors]{\ulink{properties of vectors}}; it is often denoted with either \( \circ, \text{or}~\odot \).
    \item To reiterate, now with more context, the Hadamard product takes two matrices with equal dimensions and multiplies each element with the corresponding element in the other matrix, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{bmatrix}
    a_{1,1}  & \cdots & a_{1,n} \\
    \vdots & \ddots & \vdots \\
    a_{m,1}  & \cdots & a_{m,n}
    \end{bmatrix}
    \odot
    \begin{bmatrix}
      b_{1,1}  & \cdots & b_{1,n} \\
      \vdots & \ddots & \vdots \\
      b_{m,1}  & \cdots & b_{m,n}
    \end{bmatrix}
    =
    \begin{bmatrix}
      a_{1,1}b_{1,1}  & \cdots & a_{1,n}b_{1,n} \\
      \vdots & \ddots & \vdots \\
      a_{m,1} b_{m,1}  & \cdots & a_{m,n}b_{m,n}
    \end{bmatrix}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item \ttt{\cmark~associative}, \ttt{\cmark~distributive}, and \ttt{\cmark~commutative}, unlike standard matrix multiplication which is not commutative.
  \end{itemize}
\end{itemize}