\chapter{Vectors}\label{Vectors}
\section{Interpretations of Vectors}\label{Interpretations of Vectors}
\begin{itemize}
  \item \jjj{Algebraic vectors \jx{\left(\bm{v},~\vv{v}\right)}}: an ordered list of numbers.
  \begin{itemize}
    \item E.g., \(\bm{v}= \begin{bmatrix} 1 & 2 & 3 \end{bmatrix}\)
    \item Vectors can be written as rows (seen above) or columns (seen below), but  differ only at the level of notation and convention.
    \item The order of elements in a vector matters:
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{bmatrix}
      1 \\
      2 \\
      3 \
    \end{bmatrix}
    \neq
    \begin{bmatrix}
      2 \\
      1 \\
      3 \
    \end{bmatrix}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \end{itemize}
  \item \jjj{Dimensionality}: the number of elements in a vector, where each new element provides new information, or geometrically, a new direction.
  
  \item \ddd{Euclidean (geometric, spatial) vectors}: a line in geometric space that indicates the \emph{magnitude} and \emph{direction} from a starting point (tail) to an end point (head).
  \begin{itemize}
    \item Geometric vectors can start at any point in space, but often represented as starting from the \emph{origin}---such vectors are in \emph{standard position}.
    \item Coordinates are not the same as vectors, but they do indicate where the head of a vector will land if it is in standard position. 
  \end{itemize}
  
  \subsection{Vector Addition and Subtraction}\label{Vector Addition and Subtraction}
  \begin{itemize}
    \item Algebraically, \emph{dimensionality} of vectors \emph{must be equal}. When they are, then addition or subtraction vectors is done on the corresponding elements of each vector, e.g.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{bmatrix}
      1 \\
      0 \\
      4 \\
      5 
    \end{bmatrix}
    +
    \begin{bmatrix}
      2 \\
      3 \\
      -6 \\
      11
    \end{bmatrix}
    =
    \begin{bmatrix}
      3 \\
      3 \\
      -2 \\
      16
    \end{bmatrix}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item Geometrically, addition can be thought of translating the tail of one vector to the head of the other---resulting in a new vector. 
    \item Geometric interpretations of subtraction can be thought of in two ways:
    \begin{itemize}
      \item[1.] Multiplying one vector by -1, then applying vector addition method above.
      \item[2.] Placing both vectors in standard position, with the resulting vector between the two heads being the answer.
    \end{itemize}
  \end{itemize}
  
  \subsection{Vector-Scalar Multiplication}\label{Vector-Scalar Multiplication}
  \begin{itemize}
    \item \ddd{Scalar}: typically denoted with lower case Greek letters (e.g., \(\alpha,~\lambda \)) indicating an element of a field (typically real numbers) used in scalar-multiplication of vectors. 
    \item Algebraically, scalar-multiplication is the multiplication of each element of a vector by a particular scalar, e.g.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \lambda \bm{v} \rightarrow 7 \begin{bmatrix}
      -1 \\
      0 \\
      1 \
    \end{bmatrix} = \begin{bmatrix}
      -7 \\
      0 \\
      7 \
    \end{bmatrix}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item Geometrically, scalar-multiplication can be thought of as the \rrr{extension (\(\lambda > 1\))} or \xxx{compression (\(\lambda\in(0,1)\))} of a vector.
    \begin{itemize}
      \item When \bbb{\(\lambda \) < 0}, then it can be thought of inverting its direction with respect to the origin.  
    \end{itemize}
  \end{itemize}  
\end{itemize}
  

\section{The Dot Product}\label{The Dot Product}
\begin{itemize}
  \item \ddd{Dot (scalar, inner) product}: an algebraic operation that takes two \emph{equal-length} sequences of numbers (usually coordinate vectors), and returns a \emph{single number}.
  \begin{itemize}
    \item The result of a dot product is a \hyperref[Scalar]{\ulink{scalar}}, so often it is represented as such.
      \begin{itemize}
        \item It can also be represented as multiplication between two vectors (\(\bm{a\cdot b}\)). 
        \item However, it is commonly represented as \(\bm{a}^T\bm{b}\) --- \hyperref[tbd]{\dlink{transpose}} will be explained in more detail when dealing with \hyperref[tbd]{\dlink{matrix products}}.
      \end{itemize}
    \item Algebraically: \(\sum_{i = 1}^{n} \bm{a}_i \bm{b}_i  \) --- where \(\Sigma \) denotes summation and \(n\) is the dimension of the vector space, e.g.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \rrr{\begin{bmatrix} 1 & 3 & 5 \end{bmatrix}}\bbb{\begin{bmatrix}
      4 \\
      -2 \\
      1 \
    \end{bmatrix}} = (\rrr{1}\cdot\bbb{4}) + (\rrr{3}\cdot \bbb{-2}) + (\rrr{-5} \cdot \bbb{-1}) = 3
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \end{itemize}
  
  \subsection{Properties of the Dot Product}\label{Properties of the Dot Product}
  \begin{itemize}
    \item Note: the following properties hold as long as \(\bm{a}\), \(\bm{b}\), and \(\bm{c}\) are real vectors.
    \item \ttt{\cmark~Distributive}: \(\bm{a}^T(\bm{b}+\bm{c})=~\bm{a}^T\bm{b}+\bm{a}^T\bm{c}\) ---  vector multiplication distributes over vector addition. 
    \item \fff{\xmark~Associative}: \(\bm{a}^T(\bm{b}^T\bm{c})\neq(\bm{a}^T\bm{b})\bm{c}\) --- in general the associative property does not hold, as the dot product would most likely produce different scalars.
    \begin{itemize}
      \item Additionally, \bm{a} could have a different dimensionality than \bm{b} and \bm{c}. I.e., even if \bm{b} and \bm{c} had the same dimensionality (\(\bm{a}^T(\bm{b}^T\bm{c})\) would be valid vector-scalar multiplication) then \(\bm{a}^T \bm{b}\) would be invalid.
    \end{itemize}
    \item \ttt{\cmark~Commutative}: \(\bm{a}^T\bm{b} = \bm{b}^T\bm{a}\) --- the order of the vectors does not matter. 
  \end{itemize}

  \subsection{Vector Length}\label{Vector Length}
  \begin{itemize}
    \item \jjj{Vector norm (magnitude, length)}: denoted with double vertical bars \( \| \bm{v} \| \), indicating length of a vector in euclidean space. Not to be confused with absolute value \(|x|\) of a scalar's ``norm.'' However, sometimes the notation \(| \bm{v} |\) is used.
    \item Calculating \( \| \bm{v} \| \) is done using the Euclidean norm:
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \| \bm{v} \| = \sqrt{v_1^2+v_2^2+v_3^2}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item This is a consequence of the Pythagorean theorem, since the \hyperref[Basis]{\dlink{basis vectors}}  \\ \(\bm{e}_1\), \(\bm{e}_2\), and \(\bm{e}_3\) are \hyperref[Orthogonality]{\dlink{orthogonal}} \hyperref[Unit Vectors]{\dlink{unit vectors}}.
    \end{itemize}
    \item Thus, the \emph{norm} can easily be found by taking the square root of the dot product of the vector with itself:
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \emph{\| \bm{v} \| = \sqrt{\bm{v}^T v}}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \end{itemize}

  \subsection{Geometric Interpretation of the Dot Product}\label{Geometric Interpretation of the Dot Product}
  \begin{itemize}
    \item The dot product of two \hyperref[Euclidean (geometric, spatial) vectors]{\ulink{Euclidean vectors}} \tbm{a} and \tbm{b} is defined by:
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \emph{\lambda = \bm{a}^T \bm{b} = \| \bm{a} \| ~\| \bm{b} \| \cos(\theta)}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    where \(\theta \) is the angle between \tbm{a} and \tbm{b}.
    \item \jjj{Features based on \tbm{\theta}}:
    \begin{itemize}
      \item When \(\rrr{\cos{\theta} > 0}\) \(\left(\theta < \ang{90}~\text{--- \rrr{acute}}\right)\) then \(\rrr{\lambda > 0~(+)}\)
      \item When \(\bbb{\cos{\theta} < 0}\) \(\left(\theta > \ang{90}~\text{--- \bbb{obtuse}}\right)\) then \(\bbb{\lambda < 0~(-)}\)
      \item When \(\minor{\cos{\theta} = 0}\)  (\(\theta = \ang{90}~\text{--- \minor{perpendicular}}\)) then \(\minor{\lambda = 0}\)
      \begin{itemize}
        \item This represents a special case where the vectors are said to be \emph{orthogonal}. 
        \item \ddd{Orthogonality}: the generalization of the notion of \emph{perpendicularity} to the linear algebra of bilinear forms.
      \end{itemize}
      \item When \(\emph{\cos{\theta} = 1}\) then the vectors are \emph{codirectional}:
      \[\bm{a}^T\bm{b} = \| \bm{a} \| ~\| \bm{b} \| \]\vspace{-25pt}
      \begin{itemize}
        \item Thus, the dot product with a vector \tbm{v} with itself is
        \[\bm{v}^T\bm{v} = \| \bm{v} \|^2\]\vspace{-25pt}
        \item Which gives us the \hyperref[Vector Length]{\ulink{norm}} as defined above, i.e., \( \| \bm{v} \| = \sqrt{\bm{v}^T v}\)
        \item If \(\cos{\theta} = -1\), then really vectors are still codirectional, but point in opposite directions with respect to the origin. 
      \end{itemize}
    \end{itemize}
  \end{itemize}
\end{itemize}

\newpage
\section{Other Properties of Vectors}\label{Other Properties of Vectors}
\begin{itemize}
  \item \hyperref[tbd]{\dlink{Matrices}} are properly defined later. However, vectors are technically single row or column matrices, so many of the following operations also work on vectors, thus use of matrices often appears in this section.
  
  \subsection{Hadamard Multiplication}\label{Vector Hadamard Multiplication}
  \begin{itemize}
    \item \jjj{Hadamard (element-wise) product}: a binary operation (only takes two operands) that matrices of the same dimensions and produces another matrix of the same dimension as the operands, e.g., vector Hadamard multiplication:
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{bmatrix}
      1 \\
      0 \\
      4 \\
      5 
    \end{bmatrix}
    +
    \begin{bmatrix}
      2 \\
      3 \\
      -6 \\
      11
    \end{bmatrix}
    =
    \begin{bmatrix}
      2 \\
      0 \\
      -24 \\
      55
    \end{bmatrix}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \end{itemize}

  \subsection{Outer Product}\label{Outer Product}
  \begin{itemize}
    \item Recall that the \hyperref[Dot (scalar, inner) product]{\ulink{dot (scalar, inner) product}} produces a \(1\times1 \) matrix, or rather a scalar, hence ``scalar'' product.
      \begin{itemize}
        \item Note: the typical notation used for dot products is \(\bm{v}^T \bm{w}\) --- part of reasoning for the ``inner'' product. 
      \end{itemize}
    \item \jjj{Outer product}: an \(N\times M\) matrix that results from the product of two vectors with dimensions \(n\) and \(m\).
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bm{v}\bm{w}^T=N\times M
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item The subtle change in notation matters in contrast to the dot product, as both represent distinct operations (assuming they are column vectors).
    \item The outer product allows for the multiplication of vectors with different dimensionality
    \item Can be thought of in two different ways:
    \begin{itemize}
      \item The \xxx{row} perspective:
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \begin{bmatrix}
        1 \\
        0 \\
        4 \\
        2
      \end{bmatrix} \begin{bmatrix} \xxx{a} & \xxx{b} & \xxx{c} \end{bmatrix}
      =
      \begin{bmatrix}
        1\xxx{a} & 1\xxx{b} & 1\xxx{c} \\
        0\xxx{a} & 0\xxx{b} & 0\xxx{c} \\
        4\xxx{a} & 4\xxx{b} & 4\xxx{c} \\
        2\xxx{a} & 2\xxx{b} & 2\xxx{c}  
      \end{bmatrix}
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \item The \yyy{column} perspective:
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \begin{bmatrix}
        \yyy{1} \\
        \yyy{0} \\
        \yyy{4} \\
        \yyy{2}
      \end{bmatrix} \begin{bmatrix} a & b & c \end{bmatrix}
      =
      \begin{bmatrix}
        \yyy{1}a & \yyy{1}b & \yyy{1}c \\
        \yyy{0}a & \yyy{0}b & \yyy{0}c \\
        \yyy{4}a & \yyy{4}b & \yyy{4}c \\
        \yyy{2}a & \yyy{2}b & \yyy{2}c  
      \end{bmatrix}
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \end{itemize}
  \end{itemize}
  
  \subsection{Cross Product}\label{Cross Product}
  \begin{itemize}
    \item \jjj{Cross (vector, directed area) product}: denoted by the symbol \jjj{\(\bm{\times} \)}, indicating a binary operation on two vectors in \emph{three-dimensional space \(\mathbb{R}^3 \)}.
      \begin{itemize}
        \item Given two \hyperref[Linear Independence]{\dlink{linearly independent}} vectors \tbm{a} and \tbm{b}, then the cross product \(\bm{a}\times\bm{b}\) produces a new vector that is \emph{orthogonal} to both \tbm{a} and \tbm{b}, or \emph{normal} to the plane containing them.
        \item The \emph{direction} of the vector is given by the \emph{right-hand rule} (\tbm{a} = pointer, \tbm{b} = index, thumb = direction). 
        \item The \emph{magnitude} of the vector represents the \emph{area of the parallelogram} that the vectors span.
      \end{itemize}
    \item The cross product can be defined by the formula: 
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bm{a}\times \bm{b} = \| \bm{a} \|~\| \bm{b} \|\sin(\theta)\,\bm{v}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item Notice this is similar to the \hyperref[Geometric Interpretation of the Dot Product]{\ulink{geometric interpretations of the dot product}} --- the contrast between the two leads to an intuitive interpretation:
      \begin{itemize}
        \item \(\cos(\theta)\) in the \emph{dot product} is used to measure how ``parallel'' the two vectors are, i.e., they are \emph{codirectional} when \(\theta = 1\), allowing for calculation of the \hyperref[Vector Length]{\ulink{norm}}. 
        \item \(\sin(\theta)\) in the \emph{cross product} is used to measure how ``perpendicular'' two vectors are, i.e., they are \emph{orthogonal} when \(\theta = 1\). There are multiple directions of the orthogonal vector, so calculation of the signed area returns vector \tbm{v} that describes both magnitude and direction as described above.
        \item The intuition described here will be more clear when the \hyperref[tbd]{\dlink{determinant}} is discussed in more detail.
      \end{itemize}
    \end{itemize}
    \item An algebraic example of vector e.g.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{bmatrix}
    1 \\
    2 \\
    3 
    \end{bmatrix}
    \times
    \begin{bmatrix}
    a \\
    b \\
    c 
    \end{bmatrix}
    =
    \begin{bmatrix}
    2c & - & 3b \\
    3a & - & 1c \\
    1b & - & 2a 
    \end{bmatrix}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \end{itemize}
  
  \subsection{Primer on Complex Numbers}\label{Primer on Complex Numbers}
  \begin{itemize}
    \item \ddd{Complex number}: a number that can be expressed in the form \(\xxx{a} + \yyy{bi}\), where \(\xxx{a}\) and \(\xxx{b}\) are \xxx{real numbers}, and \yyy{\(i~(\: j\,)\)} is a symbol called the \yyy{imaginary unit} that satisfies the equation \yyy{\(i = \sqrt{-1}\)}.
    \item This imaginary unit allows for the \yyy{imaginary set \((a + bi \in \mathbb{C})\)} to be described. 
    \item The combination of a real part and an imaginary part (\(\xxx{a} + \yyy{bi}\)) gives imaginary numbers both a direction and magnitude on the 2-D plane created between the two parts---thus they can be described as 2-D vectors in this space.
    \item Multiplication of complex numbers can be done by factoring the imaginary unit, e.g.,
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{align*}
      z &= a + b\yyy{j}  \\
      w &= c + d\yyy{j}  \\
      \\
      zw &= (a+b\yyy{j})(c+d\yyy{j}) &  \\
      &= ac + ad\yyy{j} + cb\yyy{j} + bd\yyy{j^2} \\
      &= ac + ad\yyy{j} + cb\yyy{j} - bd & (\yyy{j^2} = -1)
    \end{align*}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item Computing the dot product with complex vectors is the same, just including the factoring mentioned above when necessary, e.g.,
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{align*}
      \begin{bmatrix}
      1+3j \\
      -2j \\
      4 \\
      5 
      \end{bmatrix}^T
      \begin{bmatrix}
      6+2j \\
      8 \\
      3j \\
      -5 
      \end{bmatrix}\\
    \end{align*}
    \vspace{-45pt}
    \begin{align*}
      &= \rrr{(1+3j)(6+2j)} + -16j + 12j + 25  \\
      &= \rrr{6 + 2j + 18j - 6} - 16j + 12j + 25 \\
      &= 25 + 16j
    \end{align*}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \end{itemize}
  
  \subsection{Conjugate Transpose}\label{Conjugate Transpose}
  \begin{itemize}
    \item \jjj{Conjugate (Hermitian) transpose \(M^H,~M^*\)}: the \(n\text{-by-}m\) matrix obtained by taking the \hyperref[tbd]{\dlink{transpose}} and then taking the complex conjugate of each entry.
    \item \jjj{Complex conjugate}: the number with both equal real and imaginary parts equal in magnitude but \emph{opposite in sign.}
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      a+bi = a - bi~~\longleftrightarrow~~a - bi = a + bi
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \vspace{-24pt}
    \item Multiplying a vector by the conjugate transpose allows us to calculate the \emph{magnitude} of the vector containing imaginary numbers by using the complex conjugate to ``canceling out'' all the imaginary units and give a \emph{real number answer} (or rather, \(a + 0i\)).
    \item Matrices will return a matrix of real numbers---this is part of what makes using imaginary numbers useful for future computations. 
  \end{itemize} 
  \vspace{12pt}
  \subsection{Unit Vectors}\label{Unit Vectors}
  \begin{itemize}
    \item \jjj{Unit vectors \(\ih , \jh, \kh \)}: a vector scaled such that (\,:\,) the length of the vector \emph{equals 1} in a normed vector space.
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \ih = \lambda \bm{v} : \| \lambda\bm{v} \| = \frac{1}{\| \bm{v} \|} \| \bm{v} \| = 1
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item \ddd{Normed vector space}: a vector space, over the real or complex numbers, on which a \hyperref[Vector Length]{\ulink{norm}} is defined.
  \end{itemize}
\end{itemize}

\section{Linear Combinations}\label{Linear Combinations}
\begin{itemize}
  \item \ddd{Linear combination}: an expression constructed from a set of terms by multiplying each term by a scalar and adding the results, i.e.,
  \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  a_1\bm{v}_1 +  a_2\bm{v}_2 +  a_3\bm{v}_3 +  a_n\bm{v}_n   
  \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \item For example, in a 3-D vector space \(\mathbb{R}^3\), then \emph{any vector} in the space is can be made by a linear combination of the following three vectors \(\bm{e}_1,~\bm{e}_2,~\bm{e}_3\) (unit vectors) multiplied by some scalar:
  \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \lambda\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} +
  \lambda\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix} + 
  \lambda\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}
  \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsection{Subspace}\label{Subspace}
  \begin{itemize}
    \item \jjj{Linear (vector) subspace}: a vector space that is a subset of some larger vector space.
    \item \emph{Algebraically}, a subspace is the set of all vectors that can be created by taking a linear combination of some vector or a set of vectors.
    \item A vector subspace must be \emph{closed under} \emph{addition} and \emph{scalar multiplication} while also \emph{containing the zero vector}, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \forall\bm{v}, \bm{w} \in L;\quad \forall\lambda,\alpha \in \mathbb{R};\quad \lambda \bm{v} + \alpha \bm{w} \in L
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item For all (\(\forall \)) vectors \(\bm{v},\bm{w}\) in (\(\in \)) the linear subspace (\(V\)), and for all scalars \(\lambda,\alpha \) in the set of real numbers (\(\mathbb{R}\)), then any linear combination of the two are in the same subspace.
      \item The above equation also implies the inclusion of zero vector, which is a trivial subspace of the vector space.
    \end{itemize}
    \item The \emph{geometric} interpretation is best described through some examples:
    \begin{itemize}
      \item A \(\mathbb{R}^1\) vector and all the scaled possibilities added together describes a line stretches infinity in both directions, while a \(\mathbb{R}^2\) vector would create a 2-D plane. 
      \item Both of the previous subspaces exist in the higher dimensional vector spaces, i.e., a 2-D plane and a 1-D line both exist in the 3-D vector space.
      \item All subspaces also pass through the origin, including the O-D subspace, which is just the origin.
    \end{itemize}
  \end{itemize}

  \subsection{Subsets}\label{Subsets}
  \begin{itemize}
    \item \jjj{Subset \(\subseteq \)}: a set \(A\) is a subset of a set \(B\) if all elements of \(A\) are also elements of \(B\); \(B\) is then b a \jjj{superset \(\supseteq  \)} of \(A\).
    \item Not all subsets of vector spaces are subsets; subsets don't need to include the origin, don't need to be closed, or could have boundaries.
  \end{itemize}
  
  \subsection{Span}\label{Span}
  \begin{itemize}
    \item \jjj{Linear span (hull) \(\operatorname{span}(S)\)}: a set \(S\) of vectors (from a vector space) that is the smallest linear subspace that contains the set.
    \item The span is typically infinite, but it also can be defined as the set of all finite \hyperref[Linear combination]{\ulink{linear~combinations}} of vectors of \(S\) given an arbitrary field \(K\):
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \operatorname{span}(S)=\left\lbrace\left. \sum_{i = i}^{k} \lambda_i\bm{v}_i ~ \right| k \in \mathbb{N}, \bm{v}_i \in S, \lambda_i \in K \right\rbrace
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item A frequent question is asked whether a vector is in a span or not, e.g., are vectors \(v, w \in \operatorname{span}(S)\): 
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bm{v}=\begin{bmatrix} 1 \\ 2 \\ 0 \end{bmatrix}
    w=\begin{bmatrix} 3 \\ 2 \\ 1 \end{bmatrix}
    S= \left\lbrace \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} \begin{bmatrix} 1 \\ 7 \\ 0 \end{bmatrix} \right\rbrace 
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item \(\bm{v}\in \operatorname{span}(S) \) since both vectors in \(S\) can be scaled then combined in some way to form \tbm{v}, i.e., 
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bm{v}=\begin{bmatrix} 1 \\ 2 \\ 0 \end{bmatrix} =
    \frac{5}{6}\begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} + 
    \frac{1}{6}\begin{bmatrix} 1 \\ 7 \\ 0 \end{bmatrix} 
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item Determining the weights will be discussed later using \hyperref[tbd]{\dlink{matrix computations}}.
    \end{itemize}
    \item However, it's clear that \(\bm{w}\notin \operatorname{span}(S) \) since 0 cannot be scaled to equal 1.
    \item Geometrically, a useful intuitive example is a 2-D span in 3-D space; the 2-D plane can be moved anywhere in the 3-D space as long as the two vectors describing the 2-D plane are \hyperref[Linear Independence]{\dlink{linearly independent}}.
  \end{itemize}
  
  \subsection{Linear Independence}\label{Linear Independence}
  \begin{itemize}
    \item \jjj{Linearly dependent}: a set of \(V\) vectors where at least one vector in the set can be defined as a \hyperref[Linear combination]{\ulink{linear combination}} of the others.
    \item \jjj{Linearly independent}: when no vector in the set can be written in the above way. 
    \item Algebraically, a sequence of vectors \(\bm{v}_1,\bm{v}_2,\dots,\bm{v}_k\) that a vector space \(V\) are linearly independent if there exist scalars \(\lambda_1,\lambda_2,\dots,\lambda_k\) (not all zero) such that they form the zero vector \(\bm{0}\), i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \lambda_1\bm{v}_1,\lambda_2\bm{v}_2,\dots,\lambda_k\bm{v}_k = \bm{0} \qquad \lambda \in \mathbb{R}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item Geometrically, a set of \(V\) vectors in independent if each vectors points in a geometric dimension not reachable using other vectors in the set.
    \begin{itemize}
      \item E.g., if two vectors are lie along the same line, then they are really just the same vector; same concept with a 2-D plane containing three vectors in \(\mathbb{R}^3\).
    \end{itemize}
    \end{itemize}
  
  \subsection{Basis}\label{Basis}
  \begin{itemize}
    \item \jjj{Basis}: the combination of \hyperref[Span]{\ulink{span}} and \hyperref[Linear Independence]{\ulink{linear independence}}, i.e., the set \(B\) of vectors in vector space \(V\) if every element of \(V\) may be written as a \emph{unique} finite \hyperref[Linear combination]{\ulink{linear~combination}} of elements of \(B\).
      \begin{itemize}
        \item More concisely, a basis is simply a \emph{linearly independent spanning set}.
        \item \jjj{Components (coordinates)}: the coefficients of the linear combination and are referred to as of the vector with respect to \(B\), or \jjj{basis vectors}.
      \end{itemize}
    \item Examples of standard basis vectors:
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \mathbb{R}^2 ~ 
    \left\lbrace 
    \begin{bmatrix} 1 \\ 0 \end{bmatrix} 
    \begin{bmatrix} 0 \\ 1 \end{bmatrix}
    \right\rbrace
    \qquad
    \mathbb{R}^3 ~ 
    \left\lbrace 
    \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}
    \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}
    \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}
     \right\rbrace
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \end{itemize}
  \item The standard basis vectors of \(\mathbb{R}^3\) was used in the example of \hyperref[Linear Combinations]{\ulink{linear combinations}} to demonstrate how these unit vectors could all be scaled so that they could describe any vector in the space.
  \item \hyperref[tbd]{\dlink{Basis vectors can be changed}}, so it's better to think of them as the ``rulers'' used to describe any other vector in the space.
\end{itemize}





