\chapter{Matrices}\label{Matrices}  
\section{Matrix Terminology}\label{Matrix Terminology}
\begin{itemize}
  \item \jjj{Matrix \(\bm{M}_{\xxx{r},\yyy{c}}\)}: a \emph{rectangular array} of elements arranged in \xxx{rows \(\leftrightarrow \)} and \yyy{columns \(\updownarrow \)}, e.g.,
  \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  M = \begin{bmatrix}
  1 & 0 & 3 \\
  5 & 4 & 2 \\
  7 & 6 & 9 
  \end{bmatrix}
  \qquad M_{\xxx{3},\yyy{2}} = 6
  \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \item \ddd{Block (partitioned) matrix}: a matrix that is interpreted as having been broken into sections called blocks or submatrices, e.g.,
  \begin{align*}
    \bm{M} = 
    \begin{bmatrix}
      \bbb{\bm{D}} & \fff{\bm{N}} \\
      \ttt{\bm{Y}} & \bbb{\bm{D}} 
    \end{bmatrix} 
    =
    \begin{bmatrix}
    \bbb{4} & \bbb{2} & \fff{0} & \fff{0} \\
    \bbb{6} & \bbb{9} & \fff{0} & \fff{0} \\
    \ttt{1} & \ttt{1} & \bbb{4} & \bbb{2} \\
    \ttt{1} & \ttt{1} & \bbb{6} & \bbb{9}
    \end{bmatrix}
    \\
    \bm{D} = 
    \bbb{\begin{bmatrix}
      4 & 2 \\
      6 & 9 
    \end{bmatrix}}\quad
    \bm{N} =
    \fff{\begin{bmatrix}
      0 & 0 \\
      0 & 0 
    \end{bmatrix}}\quad
    \bm{Y} = 
    \ttt{\begin{bmatrix}
      1 & 1 \\
      1 & 1 
    \end{bmatrix}}
  \end{align*}
  \begin{itemize}
    \item Can be used for large matrices with high level structure, offering convenient notation, and sometimes providing computational benefits.
  \end{itemize}

  \item \ddd{Diagonal}: the elements of matrix starting from the \emph{top~left~\(\searrow \, \)~lower~right}.
    \begin{itemize}
      \item \jjj{Off-diagonal}: elements not along the diagonal (0s and 1s in example below)
      \item Works for both \hyperref[Square and Rectangular Matrices]{\dlink{square and rectangular matrices}}, e.g.,
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \begin{bmatrix}
        \emph{4} & 1 & 0 & 1\\
        0 & \emph{2} & 0 & 1\\
        1 & 0 & \emph{6} & 0\\
        1 & 1 & 0 & \emph{9} 
      \end{bmatrix}
      \qquad
      \begin{bmatrix}
        \emph{4} & 1 & 0 & 1 & 1 & 0 & 0 \\
        0 & \emph{2} & 0 & 1 & 0 & 1 & 1 \\
        1 & 0 & \emph{6} & 0 & 1 & 0 & 1 \\
        1 & 1 & 0 & \emph{9} & 1 & 0 & 1
      \end{bmatrix}
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \end{itemize}
  \item \ddd{Matrix size}: a matrix with \xxx{\(m\) rows} and \yyy{\(n\) columns} is called an \(\xxx{m} \times \yyy{n}\) matrix, or \(\xxx{m}\)-by-\(\yyy{n}\) matrix, while \(\xxx{m}\) and \(\yyy{n}\) are the dimensions.
    \begin{itemize}
      \item Order matters---the convention is rows then columns, i.e., \(\xxx{m} \times \yyy{n} \neq \yyy{n} \times \xxx{m} \).
      \item \xxx{\textbf{MR}}. \yyy{\textbf{N}}i\yyy{\textbf{C}}e guy: a useful mnemonic to remember typical conventions.
    \end{itemize}
  \item \ddd{Matrix dimensionality}: 
    \begin{itemize}
      \item \(\mathbb{R}^{mn}\): describes the total number of elements, here the multiplication of the dimensions is commutative (order doesn't matter). 
      \item \(\mathbb{R}^{m \times n}\): the specific matrix size using rows and columns as described above.
      \item \(C(M) \in \mathbb{R}^m\): a collection of column vectors, i.e., a matrix spanned by set column vectors with \(m\) elements.
      \item \(R(M) \in \mathbb{R}^n\): a collection of row vectors, the inverse of above.
    \end{itemize}

  \subsection{Square and Rectangular Matrices}\label{Square and Rectangular Matrices}
  \begin{itemize}
    \item \ddd{Square matrix}: a matrix with the same number of rows and columns. 
      \begin{itemize}
        \item An \(n\times n\) matrix is known as a square matrix of order \(n\).
        \item Any two square matrices of the same order can be added and multiplied.
      \end{itemize}
    \item \ddd{Rectangular matrix}: a matrix with an unequal number of rows and columns, i.e., \(m \neq n\). 
    \item Both square and rectangular matrices have a \hyperref[Diagonal]{\ulink{diagonal}}, as described above.
  \end{itemize}

  \subsection{Symmetric and Skew-Symmetric Matrices}\label{Symmetric and Skew-Symmetric Matrices}
  \begin{itemize}
    \item \ddd{Symmetric matrix}: a square matrix that can be mirrored across the diagonal, e.g.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{bmatrix}
    4 & \ttt{-6} & \ttt{-1} \\
    \ttt{-6} & 2 & \ttt{9} \\
    \ttt{-1} & \ttt{9} &  0
    \end{bmatrix}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \begin{itemize}
        \item Algebraically, it's a square matrix \(\bm{A}\) that is equal to its \hyperref[Transposition]{\dlink{transpose}}, i.e., \\ \(\bm{A} = \bm{A}^T\).
        \item It does not matter what is on the diagonal, as any number is equal to itself. 
      \end{itemize}
    \item \ddd{Skew-symmetric matrix}: a square matrix that is still symmetric, but all elements mirrored across the diagonal and inverted, e.g.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{bmatrix}
    0 & \fff{+6} & \fff{+1} \\
    \ttt{-6} & 0 & \fff{-9} \\
    \ttt{-1} & \ttt{9} &  0
    \end{bmatrix}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item Algebraically, it's a square matrix \(\bm{A}\) that is equal to its negative transpose, i.e., \(\bm{A} = -\bm{A}^T\)
      \item Here all elements on the diagonal must be zero, as zero is the only number that can be equal its inverse. 
    \end{itemize}
  \end{itemize}
  
  \subsection{Identity and Zero Matrices}\label{Identity and Zero Matrices}
  \begin{itemize}
    \item \jjj{Identity matrix \(\bm{I}_n\)}: a matrix with size \(n \times n\) with \emph{all elements along the diagonal = 1} and \bbb{all other elements = 0}, e.g, \(\bm{I}_3\) and \(\bm{I_n}\) (\(~\cdots~\)indicate continuation of pattern):
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{bmatrix}
    \emph{1} & \bbb{0} & \bbb{0} \\
    \bbb{0} & \emph{1} & \bbb{0} \\
    \bbb{0} & \bbb{0} & \emph{1} 
    \end{bmatrix}
    \qquad
    \begin{bmatrix}
    1 & 0 & \cdots & 0 \\
    0 & 1 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & 1
    \end{bmatrix}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item Essentially, the identity matrix is the equivalent of the number 1 in linear algebra. 
    \end{itemize}
    \item \jjj{Zero matrix 0}: a matrix of \emph{all zeros}.
  \end{itemize}
  
  \subsection{Diagonal and Triangular Matrices}\label{Diagonal and Triagnular Matrices}
  \begin{itemize}
    \item \ddd{Diagonal matrix}: when all elements \emph{outside the main diagonal} are zero, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{bmatrix}
    e_{1,1} & 0 & \cdots & 0 \\
    0 & e_{2,2} & 0 & \vdots \\
    \vdots & 0 & \ddots & 0  \\
    0 & \cdots & 0 & e_{i,i}  \\
    \end{bmatrix}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item Elements along the diagonal don't have to be the same, and they can be zero (meaning rectangular matrices still can be diagonal, technically).
      \item When all the elements along the main diagonal are the same, then it's a scaled version of the \hyperref[Identity and Zero Matrices]{\ulink{identity matrix}}, i.e., a \ddd{scaled matrix}\jjj{\(\lambda\bm{I}\)}.
    \end{itemize}
    \item \ddd{Triangular matrices}: when all elements above or below the diagonal are zero, but not on both sides.
      \begin{itemize}
        \item \rrr{Upper triangular matrix}: when all the elements \bbb{below} the diagonal are zero. 
        \item \bbb{Lower triangular matrix}: when all the elements \rrr{above} the diagonal are zero. 
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \rrr{\text{Upper}=\begin{bmatrix}
          e_{1,1} & e_{1,2} & e_{1,3} \\
          \bbb{0} & e_{2,2} & e_{2,3} \\
          \bbb{0} &  \bbb{0} & e_{3,3}  \\
        \end{bmatrix}}
        \qquad
        \bbb{\text{Lower}=\begin{bmatrix}
          e_{1,1} & \rrr{0} & \rrr{0} \\
          e_{2,1} & e_{2,2} & \rrr{0} \\
          e_{3,1} &  e_{3,2}& e_{3,3}  \\
        \end{bmatrix}}
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        
      \end{itemize}
  \end{itemize}
  
  \subsection{Augmented and Complex Matrices}\label{Augmented and Complex Matrices}
  \begin{itemize}
    \item \jjj{Augmented (concatenated) matrix \(\bm{A}~\,\vline~\bm{B}\)}: a matrix obtained by appending the columns of two given matrices, e.g.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{bmatrix}
    4 & 2 & 0 \\
    3 & 7 & 6 \\
    1 & 6 & 9 
    \end{bmatrix}
    ~\vline~
    \begin{bmatrix} 4 \\ 2 \\ 0 \end{bmatrix}
    =
    \begin{bmatrix}[ccc|c]
      4 & 2 & 0 & 4 \\
      3 & 7 & 6 & 2 \\
      1 & 6 & 9 & 0 
    \end{bmatrix}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item Used typically for the purpose of performing the same elementary row operations on each of the given matrices. 
      \item Matrices must have the same number of rows (or columns for vertical augmentation) for the concatenation to be applied.
    \end{itemize}
    \item \ddd{Complex matrix}: A matrix whose elements may contain complex numbers.
    \item The \hyperref[Conjugate Transpose]{\ulink{conjugate transpose}} discussed previously in vectors can is used here as well.
    \item \hyperref[Transposition]{\dlink{Transposition}} will be discussed shortly, but for now, the complex conjugate still behaves the same (just imaginary numbers change sign), e.g.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{bmatrix}
    1 & -1\yyy{+5i} & 0 \\
    1 & -2 & -4 \\
   \yyy{ 6i} & -4 & 5\yyy{-2i}  
    \end{bmatrix}^H
    =
    \begin{bmatrix}
    1 & 1 & \yyy{-6i} \\
    -1\yyy{-5i} & -2 & -4 \\
    0 & -4 & 5\yyy{+2i} 
    \end{bmatrix}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \end{itemize}  
\end{itemize}

\section{Basic Matrix Operations}\label{Basic Matrix Operations}
\begin{itemize}
  \item []
  
  \subsection{Matrix Addition and Subtraction}\label{Matrix Addition and Subtraction}
  \begin{itemize}
    \item \ddd{Matrix addition (subtraction)}: the operation of adding (subtracting) two matrices of \emph{equal dimensions} \(m \times n\) by adding (subtracting) the corresponding elements together, e.g.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{bmatrix}
    1 & 2 & 5 \\
    0 & 6 & 8 \\
    9 & 6 & 4 
    \end{bmatrix} +
    \begin{bmatrix}
      0 & 3 & 5 \\
      1 & -6 & 9 \\
      -5 & -4 & 0 
    \end{bmatrix} 
    = 
    \begin{bmatrix}
    1 & 5 & 10 \\
    1 & 0 & 17 \\
    4 & 2 & 4 
    \end{bmatrix}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item Note: there are other operations which could also be considered addition for matrices, such as the direct sum and the Kronecker sum (not discussed as of now).
    \item \ttt{\cmark~Commutative}: \(\bm{A}+\bm{B} = \bm{B} + \bm{A}\)
    \item \ttt{\cmark~Associative}: \(\bm{A} + (\bm{B}+\bm{C}) = (\bm{A}+\bm{B}+ \bm{C})\)
  \end{itemize}

  \subsection{Matrix Scalar Multiplication}\label{Matrix Scalar Multiplication}
  \begin{itemize}
    \item \jjj{Matrix scalar multiplication}: the same as \hyperref[Vector Scalar Multiplication]{\ulink{vector scalar multiplication}} or simply, scalar multiplication, as vectors are \(m \times 1~(\text{or}~1 \times n)\) matrices.
    \item Scalar multiplication is true when both \bbb{left scalar} and \rrr{right scalar} are equal, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bbb{\lambda}(\bm{M})_{ij} = (\bbb{\lambda} \bm{M})_{ij} = ( \bm{M}\rrr{\lambda})_{ij}= (\bm{M})_{ij}\rrr{\lambda} % chktex 3
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item More explicitly: 
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{bmatrix}
    \bbb{\lambda} e & \bbb{\lambda} e & \cdots & \bbb{\lambda} e \\
    \bbb{\lambda} e & \bbb{\lambda} e & \cdots & \bbb{\lambda} e \\
    \vdots & \vdots & \ddots & \vdots \\
    \bbb{\lambda} e & \bbb{\lambda} e & \cdots & \bbb{\lambda} e
    \end{bmatrix}
    =
    \left(
      \bbb{\lambda}~\text{or}
    \begin{bmatrix}
    e & e & \cdots & e \\
    e & e & \cdots & e \\
    \vdots & \vdots & \ddots & \vdots \\
    e & e & \cdots & e
    \end{bmatrix}
      \text{or}~\rrr{\lambda}
    \right)
    =
    \begin{bmatrix}
    e\rrr{\lambda} & e\rrr{\lambda} & \cdots & e\rrr{\lambda} \\
    e\rrr{\lambda} & e\rrr{\lambda} & \cdots & e\rrr{\lambda} \\
    \vdots & \vdots & \ddots & \vdots \\
    e\rrr{\lambda} & e\rrr{\lambda} & \cdots & e\rrr{\lambda}
    \end{bmatrix}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item The above is \ttt{true only} where the underlying ring (algebraic structure that generalize fields\dots I need to learn more about this\dots) \ttt{is commutative}. This fact is essential for later proofs.
  \end{itemize}
  
  \subsection{Transposition}\label{Transposition}
  \begin{itemize}
    \item \jjj{Transpose \(^T\)}: an operation where a matrix is flipped over its \hyperref[Diagonal]{\ulink{diagonal}}, i.e., \\ it switches the row and column indices of the matrix, e.g., 
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{bmatrix}
    \rrr{1} & \ttt{5} & \bbb{9} \\
    \rrr{2} & \ttt{6} & \bbb{0} \\
    \rrr{3} & \ttt{7} & \bbb{1} \\
    \rrr{4} & \ttt{8} & \bbb{2}
    \end{bmatrix}^T
    =
    \begin{bmatrix}
    \rrr{1} & \rrr{2} & \rrr{3} & \rrr{4} \\
    \ttt{5} & \ttt{6} & \ttt{7} & \ttt{8} \\
    \bbb{9} & \bbb{0} & \bbb{1} & \bbb{2}
    \end{bmatrix}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item Formally, the element of the \xxx{\(i\)-th row}, \yyy{\(j\)-th column} of matrix \(\bm{M}\) when transposed becomes the element of the \yyy{\(j\)-th row}, \xxx{\(i\)-th column} of matrix \(\bm{M}^T\), i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bm{M}_{\xxx{i},\yyy{j}} = \bm{M}_{\yyy{j},\xxx{i}}^T
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item Alternatively, with regard to dimensionality, if \(\bm{M}\) is an \(\xxx{m} \times \yyy{n}\) matrix, then \(\bm{M}^T\) is an \(\yyy{n} \times \xxx{m}\) matrix. 
      \begin{itemize}
        \item Thus, a transposed matrix that is transposed again will produce the original matrix, i.e.,
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \left(\bm{M}_{\yyy{j},\xxx{i}}^T\right)^T = \bm{M}_{\xxx{i},\yyy{j}} % chktex 3
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \end{itemize}
    \item Revisiting \hyperref[Augmented and Complex Matrices]{\ulink{complex matrices}}:
    \begin{itemize}
      \item \ddd{Hermitian matrix}: a \emph{square} complex matrix whose transpose is equal to every entry being replaced with its \hyperref[Complex conjugate]{\ulink{complex conjugate}}. 
        \begin{itemize}
          \item Denoted: \(\bm{M}^T={\overline{\bm{M}\,}}\)
        \end{itemize}
      \item \ddd{Skew-Hermitian matrix}: a Hermitian matrix whose transpose is equal to the \emph{negation} of its complex conjugate.
      \begin{itemize}
        \item Denoted: \(\bm{M}^T=-{\overline{\bm{M}\,}}\)
      \end{itemize}
    \end{itemize}
  \end{itemize}

  \subsection{Diagonal and Trace}\label{Diagonal and Trace}
  \begin{itemize}
    \item The \hyperref[Diagonal]{\ulink{main diagonal}} of a matrix can be extracted and turned into a vector.
      \begin{itemize}
        \item Not to be confused with \hyperref[tbd]{\dlink{diagonalization}} of a matrix, which is a result of \hyperref[tbd]{\dlink{matrix~decomposition}} resulting from \hyperref[tbd]{\dlink{eigendecomposition}}.
      \end{itemize}
    \item \jjj{Trace \(\operatorname{tr}(\bm{M})\)}: the sum of all diagonal elements, defined only for square matrices.
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \operatorname{tr}(\bm{M}) = \sum_{i = 1}^{n} e_{i,i} = e_{1,1} + e_{2,2} + \cdots + e_{n,n}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item The trace is a \emph{linear mapping} (two vector spaces that preserves the operations of vector addition and scalar multiplication.), i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \operatorname{tr}(\bm{A + B})  =  \operatorname{tr}(\bm{A}) + \operatorname{tr}(\bm{B})  \qquad
      \operatorname{tr}(\lambda\bm{A}) = \lambda\operatorname{tr}(\bm{A}) 
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item Additionally, a matrix and its transpose have the same trace, as elements along the main diagonal are not affected, i.e., \( \operatorname{tr}(\bm{M}) = \operatorname{tr}(\bm{M}^T)\) 
  \end{itemize}

  \subsection{Broadcasting}\label{Broadcasting}
  \begin{itemize}
    \item \jjj{Broadcasting}: duplication of a vector so that the dimensionality matches a larger matrix, allowing for simplification of element wise addition or multiplication. 
      \begin{itemize}
        \item Technically not a valid operation in linear algebra at face value, but is used commonly in applied linear algebra and machine learning.
      \end{itemize}
  \end{itemize}
  
  
\end{itemize}

\section{Matrix Multiplication}\label{Matrix Multiplication}
\begin{itemize}
  \item \jjj{Standard matrix multiplication \(\bm{AB}\)}: a binary operation that produces a matrix from two matrices whose \emph{inner dimensions match}.
    \begin{itemize}
      \item Multiplication of matrices can be thought of as going from right to left (\(\bm{A}\leftarrow\bm{B}\)), or rather, \tbm{B} post-multiplies \tbm{A}.
      \item The number of \yyy{columns (\(n\)) in the first matrix} must be \ttt{equal to} the number of \\ \xxx{rows (\(m\)) in the second matrix}.
      \item \jjj{Matrix product}: the product, whose size is equal to \xxx{rows of the first matrix} and the number of \yyy{columns of the second matrix}.
      \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      (\xxx{m_1} \times \ttt{n_1})(\ttt{m_2} \times \yyy{n_2}) = \xxx{m_1} \times \yyy{n_2}
      \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \end{itemize}
  \item \hyperref[Transposition]{\ulink{Transposing}} a matrix switches the dimensions, so it can enable computation in some cases, e.g,
  \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \underbrace{\bm{A}}_{\xxx{5}\times \yyy{7}} = \underbrace{\bm{A}^T}_{\xxx{7}\times \yyy{5}} 
  \qquad
  \underbrace{\bm{A}^T}_{\xxx{7}\times \ttt{5}} 
  \underbrace{\bm{B}}_{\ttt{5}\times \yyy{2}} = \underbrace{\bm{C}}_{\xxx{7}\times\yyy{2}} 
  \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \item Revisiting the \hyperref[The Dot Product]{\ulink{dot (inner) product}} and the \hyperref[Outer Product]{\ulink{outer product}} of vectors:
  \begin{itemize}
    \item The dot product must have equal-length vectors since the transpose of left vector makes inner dimensions much match, e.g.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \underbrace{\bm{v}}_{5\times1}\underbrace{\bm{w}}_{5\times1}
    \to
    \underbrace{\bm{v}^T}_{\xxx{1}\times \ttt{5}}\underbrace{\bm{w}}_{\ttt{5}\times\yyy{1}}
    = 1\times 1~\text{(scalar)}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item However, the transpose of the right vector makes the \(1 \times 1\) dimensions match, making the outer dimensions irrelevant to the validity of the computation, e.g., 
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \underbrace{\bm{v}}_{6\times1}\underbrace{\bm{w}}_{9\times1}
    \to
    \underbrace{\bm{v}}_{\xxx{6}\times \ttt{1}}\underbrace{\bm{w}^T}_{\ttt{1}\times\yyy{9}}
    = 6\times 9~\text{(matrix)}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \end{itemize}

  \subsection{Standard Matrix Multiplication Perspectives}\label{Standard Matrix Multiplication Perspectives}
  \begin{itemize}
    \item There are four ways to compute and conceptualize the process of multiplication of two matrices, all of which give the same result.
    \item I.e, if \tbm{A} is an \(\xxx{m} \times \ttt{n}\) matrix and \tbm{B} is an \(\ttt{n} \times \yyy{p}\) matrix, then their product \tbm{C} =
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \hspace{-25pt}
    {\begin{bmatrix}
      a_{11}b_{11} + \cdots + a_{1\ttt{n}}b_{\ttt{n}1}&a_{11}b_{12} + \cdots + a_{1\ttt{n}}b_{\ttt{n}2} &\cdots &a_{11}b_{1\yyy{p}} + \cdots + a_{1\ttt{n}}b_{\ttt{n}\yyy{p}}
      \\ a_{21}b_{11} + \cdots + a_{2\ttt{n}}b_{\ttt{n}1}&a_{21}b_{12} + \cdots + a_{2\ttt{n}}b_{\ttt{n}2}&\cdots &a_{21}b_{1\yyy{p}} + \cdots + a_{2\ttt{n}}b_{\ttt{n}\yyy{p}}
      \\ \vdots &\vdots &\ddots &\vdots 
      \\ a_{\xxx{m}1}b_{11} + \cdots + a_{\xxx{m}\ttt{n}}b_{\ttt{n}1}&a_{\xxx{m}1}b_{12} + \cdots + a_{\xxx{m}\ttt{n}}b_{\ttt{n}2}&\cdots &a_{\xxx{m}1}b_{1\yyy{p}} + \cdots + a_{\xxx{m}\ttt{n}}b_{\ttt{n}\yyy{p}}
    \end{bmatrix}}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item \jjj{The element perspective}: building the product matrix directly, one element at a time, via the computation of the \hyperref[The Dot Product]{\ulink{dot product}} between the \xxx{rows of the left matrix} and the \yyy{columns of the right matrix}.
    \begin{align*}
    \begin{bmatrix} \xxx{1} & \xxx{2} \\ 3 & 4 \end{bmatrix}
    \begin{bmatrix} \yyy{a} & b \\ \yyy{c} & d \end{bmatrix}
    &=
    \begin{bmatrix}
    \xxx{1}\yyy{a} + \xxx{2}\yyy{c} & \cdots \\ \cdots & \cdots \end{bmatrix}
    \\%
    \begin{bmatrix} \xxx{1} & \xxx{2} \\ 3 & 4 \end{bmatrix}
    \begin{bmatrix} a & \yyy{b} \\ c & \yyy{d} \end{bmatrix}
    &=
    \begin{bmatrix}
    1a + 2c & \xxx{1}\yyy{b}+\xxx{2}\yyy{d} \\ \cdots & \cdots 
    \end{bmatrix}
    \\%
    \begin{bmatrix} 1 & 2 \\ \xxx{3} & \xxx{4} \end{bmatrix}
    \begin{bmatrix}  \yyy{a} & b \\ \yyy{c} & d \end{bmatrix}
    &=
    \begin{bmatrix}
    1a + 2c & 1b+2d \\ \xxx{3}\yyy{a}+\xxx{4}\yyy{c} & \cdots 
    \end{bmatrix}
    \\%
    \begin{bmatrix} 1 & 2 \\ \xxx{3} & \xxx{4} \end{bmatrix}
    \begin{bmatrix} a & \yyy{b} \\ c & \yyy{d} \end{bmatrix}
    &=
    \begin{bmatrix}
    1a + 2c & 1b+2d \\ 3a+4c & \xxx{3}\yyy{b}+\xxx{4}\yyy{d} 
    \end{bmatrix}
    \end{align*}
    \item This is generally the most common perspective as the others methods have all have two ``pseudo steps'' to reach the final product, rather than through one direct method.
    \item \jjj{The layer perspective}: the building of the product matrix one layer at a time, followed by a ``flattening'' to make the final product.
      \begin{itemize}
        \item Each layer is the same size \((\xxx{m_1} \times \yyy{n_2})\) as the product, but is only a \hyperref[Matrix Rank]{\dlink{rank~1~matrix}}, or essentially the representation of only one column's worth of information.
        \item Can be thought of as a \yyy{left matrix of columns} and a \xxx{right matrix of rows}, resulting in the computation of the \hyperref[Outer Product]{\ulink{outer product}}.
      \end{itemize}
    \begin{align*}
    \begin{bmatrix} \yyy{1} & 2 \\ \yyy{3} & 4 \end{bmatrix}
    \begin{bmatrix} \xxx{a} & \xxx{b} \\ c & d \end{bmatrix}
    &=
    \begin{bmatrix}
    \yyy{1}\xxx{a} & \yyy{1}\xxx{b} \\
    \yyy{3}\xxx{a} & \yyy{3}\xxx{b} 
    \end{bmatrix}
    \\
    \begin{bmatrix} 1 & \yyy{2} \\ 3 & \yyy{4} \end{bmatrix}
    \begin{bmatrix} a & b \\ \xxx{c} & \xxx{d} \end{bmatrix}
    &=
    \begin{bmatrix}
    \yyy{2}\xxx{c} & \yyy{2}\xxx{d} \\
    \yyy{4}\xxx{c} & \yyy{4}\xxx{d} 
    \end{bmatrix}
    \\
    \begin{bmatrix}
      \yyy{1}\xxx{a} & \yyy{1}\xxx{b} \\
      \yyy{3}\xxx{a} & \yyy{3}\xxx{b} 
    \end{bmatrix}
    \Downarrow
    \begin{bmatrix}
      \yyy{2}\xxx{c} & \yyy{2}\xxx{d} \\
      \yyy{4}\xxx{c} & \yyy{4}\xxx{d} 
    \end{bmatrix}
    &=
    \begin{bmatrix}
      \yyy{1}\xxx{a} + \yyy{2}\xxx{c} & \yyy{1}\xxx{b}+\yyy{2}\xxx{d} \\ \yyy{3}\xxx{a}+\yyy{4}\xxx{c} & \yyy{3}\xxx{b}+\yyy{4}\xxx{d}  
    \end{bmatrix}
    \end{align*}
    \item \jjj{The column perspective}: the building of the product matrix by columns, where the first column is the sum of the two columns of the left matrix weighted (scaled) by the elements of the first column of the right matrix. 
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \begin{bmatrix} \bbb{1} & \chap{2} \\ \bbb{3} & \chap{4} \end{bmatrix}
      \begin{bmatrix} \rrr{a} & \emph{b} \\ \rrr{c} & \emph{d} \end{bmatrix}
      =
      \begin{pmatrix}
      \rrr{a} \begin{bmatrix} \bbb{1} \\ \bbb{3} \end{bmatrix} + \rrr{c} \begin{bmatrix} \chap{2} \\ \chap{4} \end{bmatrix} &
      \emph{b} \begin{bmatrix} \bbb{1} \\ \bbb{3} \end{bmatrix} + \emph{d} \begin{bmatrix} \chap{2} \\ \chap{4} \end{bmatrix}
      \end{pmatrix}
      =
      \begin{bmatrix}
        \bbb{1}\rrr{a} + \chap{2}\rrr{c} & \bbb{1}\emph{b}+\chap{2}\emph{d} \\ \bbb{3}\rrr{a}+  \chap{4}\rrr{c} & \bbb{3}\emph{b}+\chap{4}\emph{d} 
      \end{bmatrix}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item \jjj{The row perspective}: similar to the column perspective, but building up by row. 
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{bmatrix} \bbb{1} & \bbb{2} \\ \chap{3} & \chap{4} \end{bmatrix}
    \begin{bmatrix} \rrr{a} & \rrr{b} \\ \emph{c} & \emph{d} \end{bmatrix}
    =
    \begin{pmatrix}
    \bbb{1} \begin{bmatrix} \rrr{a} & \rrr{b} \end{bmatrix} + 
    \bbb{2} \begin{bmatrix} \emph{c} & \emph{d} \end{bmatrix} \\
    \chap{3} \begin{bmatrix} \rrr{a} & \rrr{b} \end{bmatrix} + 
    \chap{4} \begin{bmatrix} \emph{c} & \emph{d} \end{bmatrix}
    \end{pmatrix}
    =
    \begin{bmatrix}
      \bbb{1}\rrr{a} + \bbb{2}\emph{c} & \bbb{1}\rrr{b}+\bbb{2}\emph{d} \\ \chap{3}\rrr{a}+\chap{4}\emph{c} & \chap{3}\rrr{b}+\chap{4}\emph{d} 
    \end{bmatrix}
  \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \end{itemize}
    
  \subsection{Diagonal Matrix Multiplication}\label{Diagonal Matrix Multiplication}
  \begin{itemize}
    \item Square \hyperref[Diagonal and Triagnular Matrices]{\ulink{diagonal matrices}} are often used to scale another matrix by the elements along such diagonal.
    \item You can scale the columns or rows depending on the placement of the diagonal matrix \(\bm{D}\) relative to original matrix \(\bm{M}\). 
      \begin{itemize}
        \item Post-multiplying by the diagonal results in the scaling by \yyy{columns}, i.e., \(\bm{M}\yyy{\bm{D}}\):
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \begin{bmatrix}
        1 & 2 & 3 \\
        4 & 5 & 6 \\
        7 & 8 & 9 
        \end{bmatrix}
        \begin{bmatrix}
        \yyy{a} & 0 & 0 \\
        0 & \yyy{b} & 0 \\
        0 & 0 & \yyy{c}
        \end{bmatrix}
        =
        \begin{bmatrix}
          \yyy{a}1 & \yyy{b}2 & \yyy{c}3 \\
          \yyy{a}4 & \yyy{b}5 & \yyy{c}6 \\
          \yyy{a}7 & \yyy{b}8 & \yyy{c}9 
          \end{bmatrix}
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \item Post-multiplying by the original results in the scaling by \xxx{rows}, i.e., \(\xxx{\bm{D}}\bm{M}\):
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \begin{bmatrix}
        1 & 2 & 3 \\
        4 & 5 & 6 \\
        7 & 8 & 9 
        \end{bmatrix}
        \begin{bmatrix}
        \xxx{a} & 0 & 0 \\
        0 & \xxx{b} & 0 \\
        0 & 0 & \xxx{c}
        \end{bmatrix}
        =
        \begin{bmatrix}
          \xxx{a}1 & \xxx{a}2 & \xxx{a}3 \\
          \xxx{b}4 & \xxx{b}5 & \xxx{b}6 \\
          \xxx{c}7 & \xxx{c}8 & \xxx{c}9 
          \end{bmatrix}
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \end{itemize}
    \item Again, when all the elements along the diagonal are the same, then it is simply a scaled version of the \hyperref[Identity and Zero Matrices]{\ulink{identity matrix}}, or sometimes referred to as a \hyperref[scaled matrix]{\ulink{scaling matrix}}.
  \end{itemize}

  \subsection{Order of Operations}\label{Order of Operations}
  \begin{itemize}
    \item \minor{``And love is evol, spell it backwards, I'll show ya''}
    \item \jjj{\(\bm{(L\,O\,VE)}^T = \bm{E}^T\bm{V}^T\bm{O}^T\bm{L}^T\)}: reversing the order of multiplication on a set of matrices is valid if the same operation (e.g., \hyperref[Transposition]{\ulink{transpose}} or \hyperref[Matrix Inverse]{\dlink{inverse}}) can be applied to each matrix, e.g, (\emph{diagonal} highlighted for easier time seeing transpose)
    \begin{align*}
    \begin{pmatrix}
      \begin{bmatrix} \emph{1} & 2 \\ 3 & \emph{4} \end{bmatrix}
      \begin{bmatrix} \emph{a} & b \\ c & \emph{d} \end{bmatrix}
    \end{pmatrix}^T
    &=
    \begin{bmatrix}
      \emph{1a+2c} & 1b+2d \\ 3a+4c & \emph{3b+4d} 
    \end{bmatrix}^T
    &= 
    \begin{bmatrix}
      \emph{1a+2c} & 3a+4c \\ 1b+2d & \emph{3b+4d} 
    \end{bmatrix}
    \\
    \begin{bmatrix} \emph{a} & b \\ c & \emph{d} \end{bmatrix}^T
    \begin{bmatrix} \emph{1} & 2 \\ 3 & \emph{4} \end{bmatrix}^T
    &=
    \begin{bmatrix} \emph{a} & c \\ b & \emph{d} \end{bmatrix}
    \begin{bmatrix} \emph{1} & 3 \\ 2 & \emph{4} \end{bmatrix}
    &=
    \begin{bmatrix}
      \emph{1a+2c} & 1b+2d \\ 3a+4c & \emph{3b+4d} 
    \end{bmatrix}
    \end{align*}
  \end{itemize}
  
  \subsection{Matrix Vector Multiplication}\label{Matrix Vector Multiplication}
  \begin{itemize}
    \item 
  \end{itemize}

  \subsection{Transformation Matrix}\label{Tansformation Matrix}
  \begin{itemize}
    \item 
  \end{itemize}
  
  \subsection{Additive and Multiplicative Matrices}\label{Additive and Multiplicative Matrices}
  \begin{itemize}
    \item 
  \end{itemize}
  
  \subsection{Hadamard Multiplication}\label{Hadamard Multiplication}
  \begin{itemize}
    \item 
  \end{itemize}
  
  \subsection{Multiplication of Symmetric Matrices}\label{Multiplication of Symmetric Matrices}
  \begin{itemize}
    \item 
  \end{itemize}

  \subsection{Frobenius Dot Product}\label{Frobenius Dot Product}
  \begin{itemize}
    \item 
  \end{itemize}
  
  \subsection{Matrix Norms}\label{Other Matrix Norms}
  \begin{itemize}
    \item 
  \end{itemize}
\end{itemize}

\section{Matrix Rank}\label{Matrix Rank}
\begin{itemize}
  \item 
\end{itemize}


\section{Matrix Spaces}\label{Matrix Spaces}
\begin{itemize}
  \item 
\end{itemize}

\section{The Determinant}\label{The Determinant}
\begin{itemize}
  \item 
\end{itemize}

\section{Matrix Inverse}\label{Matrix Inverse}
\begin{itemize}
  \item 
\end{itemize}

\chapter{To Be Defined}\label{tbd}
\begin{itemize}
  \item Diagonalization
  \item Matrix decomposition
  \item Eigendecomposition
\end{itemize}