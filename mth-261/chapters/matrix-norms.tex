\chapter{Matrix Norms}\label{Matrix Norms}
% chktex-file 3
\section{Matrix Norms Basics}\label{Matrix Norms Basics}
\begin{itemize}
  \item \jjj{Matrix norm}: a \hyperref[Vector Length]{\ulink{vector norm}} in a vector space whose elements are matrices.
    \begin{itemize}
      \item More specifically, given a field \(K\) (of either real or complex numbers) and the vector space \(K^{m\times n}\) of matrices of the size \(m\times n\) with entries in the field \(K\), then a matrix norm is a norm in the vector space \(K^{m\times n}\). 
      \item Thus, a matrix norm is a function \( \| \bm{A} \| : K^{m \times n} \to \mathbb{R}\) that satisfies the following properties for all scalars \(\lambda \in K\) and all matrices \(\bm{A},\bm{B} \in K^{m\times n}\): 
      \begin{itemize}
        \item \( \| \lambda\bm{A} \|= |\lambda|\| \bm{A} \| \) (absolutely homogenous)
        \item \( \| \bm{A}+\bm{B} \| \leq \| \bm{A} \| + \| \bm{B} \| \) (sub-additive or satisfying the triangle inequality)
        \item \( \| \bm{A} \| \geq 0\) (positive)
        \item \( \| \bm{A} \| = 0 \leftrightarrow \bm{A}= 0_{m,n} \) (definite)
      \end{itemize}
      \item \jjj{Submultiplicative norm}: special cases of square matrices that also follow the following condition:
        \begin{itemize}
          \item \( \| \bm{AB} \| \leq \| \bm{A} \| \| \bm{B} \| \) for all matrices \tbm{A} and \tbm{B} in \(K^{n\times n}\)
        \end{itemize}
    \end{itemize}
  \item There are many types of norms, and special cases for each category of norms, but for now just the Frobenius norm, the induced 2-norm, and the Schatten p-norm will be discussed.
  
  \subsection{Frobenius Norm}\label{Frobenius Norm}
  \begin{itemize}
    \item \ddd{Frobenius inner (dot) product} \jx{\langle\bm{A}, \bm{B} \rangle_F}: binary operation that takes two matrices of equal dimensions and computes a component-wise \hyperref[The Dot Product]{\ulink{inner product}} of two matrices as if they were vectors. There are several ways of thinking about how the Frobenius inner product is computed, either:
      \begin{itemize}
        \item Element-wise multiplication \to~summation of all elements;
        \item Vectorization of both matrices \to~vector dot product;
          \begin{itemize}
            \item \jjj{Vectorization \(\operatorname{vec}(\bm{A})\)}: a linear transformation that converts a matrix into a \yyy{column vector}, i.e., a \(\xxx{m}\yyy{n} \times 1\) column vector is obtained by stacking the columns of the matrix A on top of one another: 
            \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
            \operatorname{vec}(A)=[a_{1,1},\ldots ,a_{m,1},a_{1,2},\ldots ,a_{m,2},\ldots ,a_{1,n},\ldots ,a_{m,n}]^T
            \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
          \end{itemize}
        \item Or the must efficient way, simply taking the \hyperref[Diagonal and Trace]{\ulink{trace}} of \(\bm{A}^T \bm{B}\)
        \item Thus, the Frobenius inner product can be defined as:
        \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \langle\bm{A}, \bm{B} \rangle_F =
        \sum_{i,j}\bm{A}_{i,j}\bm{B}_{i,j} = 
        \operatorname{vec}(\bm{A})^T\operatorname{vec}(\bm{B})=
        \operatorname{tr}(\bm{A}^T\bm{B})
        \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \end{itemize}
    \item \jjj{Frobenius (Euclidean) norm}: the square root of the Frobenius inner product of a matrix with itself, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \operatorname{norm}(\bm{A}) = \sqrt{\langle\bm{A}, \bm{A} \rangle_F} = \sqrt{\operatorname{tr}(\bm{A}^T \bm{A})}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
      \item This works since the use of the \hyperref[Hadamard Multiplication]{\ulink{multiplicative method}} create a symmetric matrix whose diagonal can be traced, resulting in a vector consisting of each element being squared then summed, thus the \hyperref[Vector Length]{\ulink{norm of that vector}} can easily be taken, yielding the norm of the matrix.
      \item This is often the most commonly used matrix norm and is often simply called denoted as the \(\operatorname{norm}(\bm{A})\) of a matrix.
    \end{itemize}
  \end{itemize}

  \subsection{Induced p-Norm}\label{Induced 2-norm}
  \begin{itemize}
    \item \jjj{Induced p-norm}\jx{\| \bm{A} \|_p}: Essentially measures the effect of a matrix on a particular vector norm, particularly if its longer or shorter, hence \(\sup()\) in following definition.
    \item If \(p\) (often 2, making it commonly referred to as the 2-norm) is used for both \(K^n\) and \(K^m\), then the induced p-norm can be defined as:
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \| \bm{A} \|_p = \sup_{x\neq 0} \frac{\| \bm{Ax} \|_p}{\| \bm{x} \|_p} \qquad
    1 \leq p \leq \infty
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item The notation does not differ from the Schatten p-norm or other entry-wise p-norms. 
  \end{itemize}
  
  \subsection{Schatten p-Norm}\label{Schatten Norms}
  \begin{itemize}
    \item \jjj{Schatten p-norm \( \| \bm{A} \|_p \)}: arises when applying the p-norm to the vector of \hyperref[Singular Value Decomposition]{\dlink{singular~values of a matrix}} (for now, a special set of scalars with a certain matrix).
    \item If the singular values of a matrix are denoted by \(\sigma \), then the Schatten p-norm is defined the sum of all singular values of matrix, i.e.,
    \[%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \| \bm{A} \|_p = \left(\sum_{i = 1}^{r}\sigma_i^p\right)^{1/p}
    \]%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \item All Schatten norms are submultiplicative.
    \item Often used cases are when \(p=1,2,\text{and}~\infty \);
      \begin{itemize}
        \item \jx{p=2}: yields the Frobenius norm.
        \item \jx{p=\infty}: yields the induced vector 2-norm.
        \item \jx{p=1}: yields the nuclear norm, which is the sum of all the singular values of the matrix.
      \end{itemize}
  \end{itemize}
\end{itemize}